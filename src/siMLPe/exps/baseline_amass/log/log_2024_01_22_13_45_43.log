[2024-01-22 13:46:25,073] INFO: {
    "abs_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass",
    "amass_anno_dir": "/home/aiRobots/src/siMLPe/data/amass/",
    "batch_size": 256,
    "cos_lr_max": 0.0003,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 115000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "link_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/log_last.log",
    "link_val_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/val_last.log",
    "log_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log",
    "log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/log_2024_01_22_13_45_43.log",
    "model_pth": null,
    "motion": {
        "amass_input_length": 50,
        "amass_input_length_dct": 50,
        "amass_target_length": 25,
        "amass_target_length_eval": 25,
        "amass_target_length_train": 25,
        "dim": 54,
        "pw3d_input_length": 50,
        "pw3d_target_length_eval": 25,
        "pw3d_target_length_train": 25
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": false,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": true,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 54,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 8,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "pw3d_anno_dir": "/home/aiRobots/src/siMLPe/data/3dpw/sequenceFiles/",
    "repo_name": "siMLPe",
    "root_dir": "/home/aiRobots/src/siMLPe",
    "save_every": 5000,
    "seed": 888,
    "shift_step": 5,
    "snapshot_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/snapshot",
    "this_dir": "baseline_amass",
    "use_relative_loss": true,
    "val_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/val_2024_01_22_13_45_43.log",
    "weight_decay": 0.0001
}
[2024-01-22 13:46:28,073] INFO: Iter 100 Summary: 
[2024-01-22 13:46:28,074] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.1056568468362093
[2024-01-22 13:46:30,956] INFO: Iter 200 Summary: 
[2024-01-22 13:46:30,956] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0914660657197237
[2024-01-22 13:46:33,500] INFO: Iter 300 Summary: 
[2024-01-22 13:46:33,500] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07815426148474217
[2024-01-22 13:46:36,289] INFO: Iter 400 Summary: 
[2024-01-22 13:46:36,290] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07310734100639821
[2024-01-22 13:46:38,966] INFO: Iter 500 Summary: 
[2024-01-22 13:46:38,966] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07124167330563068
[2024-01-22 13:46:41,548] INFO: Iter 600 Summary: 
[2024-01-22 13:46:41,548] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0707253334671259
[2024-01-22 13:46:43,910] INFO: Iter 700 Summary: 
[2024-01-22 13:46:43,910] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06928369481116534
[2024-01-22 13:46:46,543] INFO: Iter 800 Summary: 
[2024-01-22 13:46:46,543] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06837234765291214
[2024-01-22 13:46:49,129] INFO: Iter 900 Summary: 
[2024-01-22 13:46:49,130] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06728430718183517
[2024-01-22 13:46:51,797] INFO: Iter 1000 Summary: 
[2024-01-22 13:46:51,797] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06674013230949641
[2024-01-22 13:46:54,375] INFO: Iter 1100 Summary: 
[2024-01-22 13:46:54,375] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06569624613970518
[2024-01-22 13:46:57,098] INFO: Iter 1200 Summary: 
[2024-01-22 13:46:57,098] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06480834230780602
[2024-01-22 13:47:00,014] INFO: Iter 1300 Summary: 
[2024-01-22 13:47:00,014] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06419035002589225
[2024-01-22 13:47:02,477] INFO: Iter 1400 Summary: 
[2024-01-22 13:47:02,477] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06383298553526401
[2024-01-22 13:47:05,166] INFO: Iter 1500 Summary: 
[2024-01-22 13:47:05,166] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06325074795633555
[2024-01-22 13:47:07,828] INFO: Iter 1600 Summary: 
[2024-01-22 13:47:07,829] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06286655358970165
[2024-01-22 13:47:10,335] INFO: Iter 1700 Summary: 
[2024-01-22 13:47:10,335] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06262153595685958
[2024-01-22 13:47:13,132] INFO: Iter 1800 Summary: 
[2024-01-22 13:47:13,132] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06206351742148399
[2024-01-22 13:47:15,962] INFO: Iter 1900 Summary: 
[2024-01-22 13:47:15,963] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.061828465424478056
[2024-01-22 13:47:18,544] INFO: Iter 2000 Summary: 
[2024-01-22 13:47:18,544] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.061529595777392386
[2024-01-22 13:47:20,957] INFO: Iter 2100 Summary: 
[2024-01-22 13:47:20,957] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06119382824748754
[2024-01-22 13:47:23,700] INFO: Iter 2200 Summary: 
[2024-01-22 13:47:23,700] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.060998836755752565
[2024-01-22 13:47:26,369] INFO: Iter 2300 Summary: 
[2024-01-22 13:47:26,369] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06065137386322021
[2024-01-22 13:47:29,078] INFO: Iter 2400 Summary: 
[2024-01-22 13:47:29,079] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06007492922246456
[2024-01-22 13:47:31,762] INFO: Iter 2500 Summary: 
[2024-01-22 13:47:31,763] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06046014003455639
[2024-01-22 13:47:34,566] INFO: Iter 2600 Summary: 
[2024-01-22 13:47:34,567] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05983458552509546
[2024-01-22 13:47:37,388] INFO: Iter 2700 Summary: 
[2024-01-22 13:47:37,388] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05923790723085404
[2024-01-22 13:47:40,130] INFO: Iter 2800 Summary: 
[2024-01-22 13:47:40,130] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05931091785430908
[2024-01-22 13:47:42,959] INFO: Iter 2900 Summary: 
[2024-01-22 13:47:42,959] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.059196186661720274
[2024-01-22 13:47:45,693] INFO: Iter 3000 Summary: 
[2024-01-22 13:47:45,694] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05923462975770235
[2024-01-22 13:47:48,369] INFO: Iter 3100 Summary: 
[2024-01-22 13:47:48,370] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05891428831964731
[2024-01-22 13:47:51,045] INFO: Iter 3200 Summary: 
[2024-01-22 13:47:51,045] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.058673470988869665
[2024-01-22 13:47:53,578] INFO: Iter 3300 Summary: 
[2024-01-22 13:47:53,578] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05841201413422823
[2024-01-22 13:47:56,222] INFO: Iter 3400 Summary: 
[2024-01-22 13:47:56,222] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05848508562892676
[2024-01-22 13:47:59,069] INFO: Iter 3500 Summary: 
[2024-01-22 13:47:59,069] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05847132228314877
[2024-01-22 13:48:01,881] INFO: Iter 3600 Summary: 
[2024-01-22 13:48:01,882] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05823783375322819
[2024-01-22 13:48:04,456] INFO: Iter 3700 Summary: 
[2024-01-22 13:48:04,456] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.057458160780370235
[2024-01-22 13:48:06,991] INFO: Iter 3800 Summary: 
[2024-01-22 13:48:06,991] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05756119608879089
[2024-01-22 13:48:09,723] INFO: Iter 3900 Summary: 
[2024-01-22 13:48:09,723] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.057678433619439605
[2024-01-22 13:48:12,323] INFO: Iter 4000 Summary: 
[2024-01-22 13:48:12,323] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05727056533098221
[2024-01-22 13:48:14,740] INFO: Iter 4100 Summary: 
[2024-01-22 13:48:14,740] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05714754059910774
[2024-01-22 13:48:17,616] INFO: Iter 4200 Summary: 
[2024-01-22 13:48:17,616] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05678297732025385
[2024-01-22 13:48:20,312] INFO: Iter 4300 Summary: 
[2024-01-22 13:48:20,312] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05653638005256653
[2024-01-22 13:48:22,836] INFO: Iter 4400 Summary: 
[2024-01-22 13:48:22,836] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.056920159794390204
[2024-01-22 13:48:25,452] INFO: Iter 4500 Summary: 
[2024-01-22 13:48:25,452] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.056308372914791106
[2024-01-22 13:48:28,114] INFO: Iter 4600 Summary: 
[2024-01-22 13:48:28,114] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05617184966802597
[2024-01-22 13:48:30,825] INFO: Iter 4700 Summary: 
[2024-01-22 13:48:30,825] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05592278331518173
[2024-01-22 13:48:33,363] INFO: Iter 4800 Summary: 
[2024-01-22 13:48:33,363] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05615484993904829
[2024-01-22 13:48:36,117] INFO: Iter 4900 Summary: 
[2024-01-22 13:48:36,117] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05590682648122311
[2024-01-22 13:48:38,846] INFO: Iter 5000 Summary: 
[2024-01-22 13:48:38,846] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05588081151247024
[2024-01-22 13:48:41,338] INFO: Iter 5100 Summary: 
[2024-01-22 13:48:41,338] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055782786533236504
[2024-01-22 13:48:43,899] INFO: Iter 5200 Summary: 
[2024-01-22 13:48:43,899] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05535776685923338
[2024-01-22 13:48:46,563] INFO: Iter 5300 Summary: 
[2024-01-22 13:48:46,564] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055585704147815704
[2024-01-22 13:48:49,085] INFO: Iter 5400 Summary: 
[2024-01-22 13:48:49,085] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05469353105872869
[2024-01-22 13:48:51,673] INFO: Iter 5500 Summary: 
[2024-01-22 13:48:51,673] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054982448033988475
[2024-01-22 13:48:54,403] INFO: Iter 5600 Summary: 
[2024-01-22 13:48:54,403] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05510354209691286
[2024-01-22 13:48:57,152] INFO: Iter 5700 Summary: 
[2024-01-22 13:48:57,152] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05501505043357611
[2024-01-22 13:48:59,657] INFO: Iter 5800 Summary: 
[2024-01-22 13:48:59,657] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055004279762506485
[2024-01-22 13:49:02,435] INFO: Iter 5900 Summary: 
[2024-01-22 13:49:02,435] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05431962750852108
[2024-01-22 13:49:05,166] INFO: Iter 6000 Summary: 
[2024-01-22 13:49:05,167] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05465593431144953
[2024-01-22 13:49:07,918] INFO: Iter 6100 Summary: 
[2024-01-22 13:49:07,918] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05403812997043133
[2024-01-22 13:49:10,656] INFO: Iter 6200 Summary: 
[2024-01-22 13:49:10,657] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05409812517464161
[2024-01-22 13:49:13,227] INFO: Iter 6300 Summary: 
[2024-01-22 13:49:13,227] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05470665525645017
[2024-01-22 13:49:15,850] INFO: Iter 6400 Summary: 
[2024-01-22 13:49:15,850] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05416479777544737
[2024-01-22 13:49:18,413] INFO: Iter 6500 Summary: 
[2024-01-22 13:49:18,413] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05382565550506115
[2024-01-22 13:49:21,017] INFO: Iter 6600 Summary: 
[2024-01-22 13:49:21,017] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054179011806845666
[2024-01-22 13:49:23,866] INFO: Iter 6700 Summary: 
[2024-01-22 13:49:23,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053237805999815466
[2024-01-22 13:49:26,560] INFO: Iter 6800 Summary: 
[2024-01-22 13:49:26,560] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05357514899224043
[2024-01-22 13:49:29,322] INFO: Iter 6900 Summary: 
[2024-01-22 13:49:29,322] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05325998198240996
[2024-01-22 13:49:32,021] INFO: Iter 7000 Summary: 
[2024-01-22 13:49:32,021] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053216110318899154
[2024-01-22 13:49:34,682] INFO: Iter 7100 Summary: 
[2024-01-22 13:49:34,682] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05325237199664116
[2024-01-22 13:49:37,457] INFO: Iter 7200 Summary: 
[2024-01-22 13:49:37,457] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0533755948022008
[2024-01-22 13:49:40,200] INFO: Iter 7300 Summary: 
[2024-01-22 13:49:40,200] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05307588502764702
[2024-01-22 13:49:42,874] INFO: Iter 7400 Summary: 
[2024-01-22 13:49:42,875] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05296941544860601
[2024-01-22 13:49:45,634] INFO: Iter 7500 Summary: 
[2024-01-22 13:49:45,634] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052782181464135644
[2024-01-22 13:49:48,533] INFO: Iter 7600 Summary: 
[2024-01-22 13:49:48,533] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05268177058547735
[2024-01-22 13:49:51,242] INFO: Iter 7700 Summary: 
[2024-01-22 13:49:51,243] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05266140338033438
[2024-01-22 13:49:53,780] INFO: Iter 7800 Summary: 
[2024-01-22 13:49:53,781] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05246302872896194
[2024-01-22 13:49:56,525] INFO: Iter 7900 Summary: 
[2024-01-22 13:49:56,525] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052614581547677515
[2024-01-22 13:49:59,221] INFO: Iter 8000 Summary: 
[2024-01-22 13:49:59,221] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05258752178400755
[2024-01-22 13:50:01,735] INFO: Iter 8100 Summary: 
[2024-01-22 13:50:01,735] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05192710369825363
[2024-01-22 13:50:04,274] INFO: Iter 8200 Summary: 
[2024-01-22 13:50:04,274] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05190893914550543
[2024-01-22 13:50:07,014] INFO: Iter 8300 Summary: 
[2024-01-22 13:50:07,014] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05206202831119299
[2024-01-22 13:50:09,773] INFO: Iter 8400 Summary: 
[2024-01-22 13:50:09,773] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05208901096135378
[2024-01-22 13:50:12,472] INFO: Iter 8500 Summary: 
[2024-01-22 13:50:12,473] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05177669569849968
[2024-01-22 13:50:15,048] INFO: Iter 8600 Summary: 
[2024-01-22 13:50:15,048] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05174641523510218
[2024-01-22 13:50:17,697] INFO: Iter 8700 Summary: 
[2024-01-22 13:50:17,697] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05156041491776705
[2024-01-22 13:50:20,381] INFO: Iter 8800 Summary: 
[2024-01-22 13:50:20,381] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051307754702866075
[2024-01-22 13:50:23,310] INFO: Iter 8900 Summary: 
[2024-01-22 13:50:23,310] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051339527890086174
[2024-01-22 13:50:25,987] INFO: Iter 9000 Summary: 
[2024-01-22 13:50:25,987] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05133415803313255
[2024-01-22 13:50:28,644] INFO: Iter 9100 Summary: 
[2024-01-22 13:50:28,644] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05108424466103315
[2024-01-22 13:50:31,270] INFO: Iter 9200 Summary: 
[2024-01-22 13:50:31,270] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05137106142938137
[2024-01-22 13:50:34,111] INFO: Iter 9300 Summary: 
[2024-01-22 13:50:34,111] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051003604978322986
[2024-01-22 13:50:36,787] INFO: Iter 9400 Summary: 
[2024-01-22 13:50:36,787] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051246046796441076
[2024-01-22 13:50:39,646] INFO: Iter 9500 Summary: 
[2024-01-22 13:50:39,646] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050694367215037346
[2024-01-22 13:50:42,184] INFO: Iter 9600 Summary: 
[2024-01-22 13:50:42,184] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050751032307744026
[2024-01-22 13:50:44,906] INFO: Iter 9700 Summary: 
[2024-01-22 13:50:44,906] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05080414898693562
[2024-01-22 13:50:47,475] INFO: Iter 9800 Summary: 
[2024-01-22 13:50:47,476] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05065543107688427
[2024-01-22 13:50:50,158] INFO: Iter 9900 Summary: 
[2024-01-22 13:50:50,158] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05052512999624014
[2024-01-22 13:50:52,981] INFO: Iter 10000 Summary: 
[2024-01-22 13:50:52,981] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050739133432507515
[2024-01-22 13:50:55,632] INFO: Iter 10100 Summary: 
[2024-01-22 13:50:55,632] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05061889473348856
[2024-01-22 13:50:58,233] INFO: Iter 10200 Summary: 
[2024-01-22 13:50:58,234] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05034176297485828
[2024-01-22 13:51:00,968] INFO: Iter 10300 Summary: 
[2024-01-22 13:51:00,968] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049835887998342515
[2024-01-22 13:51:03,497] INFO: Iter 10400 Summary: 
[2024-01-22 13:51:03,498] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050114813074469564
[2024-01-22 13:51:06,178] INFO: Iter 10500 Summary: 
[2024-01-22 13:51:06,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050141588971018794
[2024-01-22 13:51:08,768] INFO: Iter 10600 Summary: 
[2024-01-22 13:51:08,769] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05001817461103201
[2024-01-22 13:51:11,481] INFO: Iter 10700 Summary: 
[2024-01-22 13:51:11,481] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050164985246956346
[2024-01-22 13:51:14,037] INFO: Iter 10800 Summary: 
[2024-01-22 13:51:14,037] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04958154700696468
[2024-01-22 13:51:16,678] INFO: Iter 10900 Summary: 
[2024-01-22 13:51:16,678] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05026188138872385
[2024-01-22 13:51:19,414] INFO: Iter 11000 Summary: 
[2024-01-22 13:51:19,414] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049614077620208266
[2024-01-22 13:51:22,115] INFO: Iter 11100 Summary: 
[2024-01-22 13:51:22,115] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04995646879076958
[2024-01-22 13:51:24,721] INFO: Iter 11200 Summary: 
[2024-01-22 13:51:24,721] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04985796108841896
[2024-01-22 13:51:27,365] INFO: Iter 11300 Summary: 
[2024-01-22 13:51:27,365] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0495902144163847
[2024-01-22 13:51:30,145] INFO: Iter 11400 Summary: 
[2024-01-22 13:51:30,145] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04913552418351173
[2024-01-22 13:51:32,732] INFO: Iter 11500 Summary: 
[2024-01-22 13:51:32,732] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04918310958892107
[2024-01-22 13:51:35,298] INFO: Iter 11600 Summary: 
[2024-01-22 13:51:35,298] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049702942557632926
[2024-01-22 13:51:37,888] INFO: Iter 11700 Summary: 
[2024-01-22 13:51:37,888] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04929551221430302
[2024-01-22 13:51:40,538] INFO: Iter 11800 Summary: 
[2024-01-22 13:51:40,538] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049170426055788995
[2024-01-22 13:51:43,265] INFO: Iter 11900 Summary: 
[2024-01-22 13:51:43,265] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049224946089088915
[2024-01-22 13:51:45,805] INFO: Iter 12000 Summary: 
[2024-01-22 13:51:45,806] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049208327978849414
[2024-01-22 13:51:48,355] INFO: Iter 12100 Summary: 
[2024-01-22 13:51:48,355] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04921384323388338
[2024-01-22 13:51:50,986] INFO: Iter 12200 Summary: 
[2024-01-22 13:51:50,986] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04871668461710214
[2024-01-22 13:51:53,811] INFO: Iter 12300 Summary: 
[2024-01-22 13:51:53,811] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04925080958753824
[2024-01-22 13:51:56,573] INFO: Iter 12400 Summary: 
[2024-01-22 13:51:56,573] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04942832510918379
[2024-01-22 13:51:59,208] INFO: Iter 12500 Summary: 
[2024-01-22 13:51:59,208] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04891539379954338
[2024-01-22 13:52:02,014] INFO: Iter 12600 Summary: 
[2024-01-22 13:52:02,014] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04848292026668787
[2024-01-22 13:52:04,733] INFO: Iter 12700 Summary: 
[2024-01-22 13:52:04,733] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0490759053081274
[2024-01-22 13:52:07,452] INFO: Iter 12800 Summary: 
[2024-01-22 13:52:07,452] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04874732054769993
[2024-01-22 13:52:10,031] INFO: Iter 12900 Summary: 
[2024-01-22 13:52:10,031] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04874776870012283
[2024-01-22 13:52:12,726] INFO: Iter 13000 Summary: 
[2024-01-22 13:52:12,726] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04820344727486372
[2024-01-22 13:52:15,333] INFO: Iter 13100 Summary: 
[2024-01-22 13:52:15,333] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04875208422541619
[2024-01-22 13:52:17,768] INFO: Iter 13200 Summary: 
[2024-01-22 13:52:17,768] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04805675745010376
[2024-01-22 13:52:20,636] INFO: Iter 13300 Summary: 
[2024-01-22 13:52:20,636] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0481405358761549
[2024-01-22 13:52:23,335] INFO: Iter 13400 Summary: 
[2024-01-22 13:52:23,336] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04801712293177843
[2024-01-22 13:52:26,032] INFO: Iter 13500 Summary: 
[2024-01-22 13:52:26,032] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04816172625869512
[2024-01-22 13:52:28,834] INFO: Iter 13600 Summary: 
[2024-01-22 13:52:28,835] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048054136596620084
[2024-01-22 13:52:31,380] INFO: Iter 13700 Summary: 
[2024-01-22 13:52:31,381] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048855821676552294
[2024-01-22 13:52:34,231] INFO: Iter 13800 Summary: 
[2024-01-22 13:52:34,231] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04832859043031931
[2024-01-22 13:52:36,865] INFO: Iter 13900 Summary: 
[2024-01-22 13:52:36,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048464207872748376
[2024-01-22 13:52:39,842] INFO: Iter 14000 Summary: 
[2024-01-22 13:52:39,843] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048259609192609784
[2024-01-22 13:52:42,718] INFO: Iter 14100 Summary: 
[2024-01-22 13:52:42,718] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04823946196585894
[2024-01-22 13:52:45,304] INFO: Iter 14200 Summary: 
[2024-01-22 13:52:45,305] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04781377851963043
[2024-01-22 13:52:47,932] INFO: Iter 14300 Summary: 
[2024-01-22 13:52:47,932] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04793288294225931
[2024-01-22 13:52:50,575] INFO: Iter 14400 Summary: 
[2024-01-22 13:52:50,575] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0481900504976511
[2024-01-22 13:52:53,180] INFO: Iter 14500 Summary: 
[2024-01-22 13:52:53,180] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04784232240170241
[2024-01-22 13:52:55,833] INFO: Iter 14600 Summary: 
[2024-01-22 13:52:55,833] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047575336731970314
[2024-01-22 13:52:58,541] INFO: Iter 14700 Summary: 
[2024-01-22 13:52:58,541] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04763128485530615
[2024-01-22 13:53:01,191] INFO: Iter 14800 Summary: 
[2024-01-22 13:53:01,191] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04746546201407909
[2024-01-22 13:53:03,849] INFO: Iter 14900 Summary: 
[2024-01-22 13:53:03,850] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047342269979417324
[2024-01-22 13:53:06,397] INFO: Iter 15000 Summary: 
[2024-01-22 13:53:06,397] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04741741269826889
[2024-01-22 13:53:08,971] INFO: Iter 15100 Summary: 
[2024-01-22 13:53:08,972] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04785954024642706
[2024-01-22 13:53:11,447] INFO: Iter 15200 Summary: 
[2024-01-22 13:53:11,448] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0476397667825222
[2024-01-22 13:53:14,021] INFO: Iter 15300 Summary: 
[2024-01-22 13:53:14,021] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04831626486033201
[2024-01-22 13:53:16,880] INFO: Iter 15400 Summary: 
[2024-01-22 13:53:16,880] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04753889866173267
[2024-01-22 13:53:19,678] INFO: Iter 15500 Summary: 
[2024-01-22 13:53:19,678] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04758598864078522
[2024-01-22 13:53:22,401] INFO: Iter 15600 Summary: 
[2024-01-22 13:53:22,401] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04775155868381262
[2024-01-22 13:53:24,980] INFO: Iter 15700 Summary: 
[2024-01-22 13:53:24,980] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047487380243837835
[2024-01-22 13:53:27,672] INFO: Iter 15800 Summary: 
[2024-01-22 13:53:27,672] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04772300001233816
[2024-01-22 13:53:30,536] INFO: Iter 15900 Summary: 
[2024-01-22 13:53:30,537] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04786310192197561
[2024-01-22 13:53:33,355] INFO: Iter 16000 Summary: 
[2024-01-22 13:53:33,355] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04755220837891102
[2024-01-22 13:53:36,169] INFO: Iter 16100 Summary: 
[2024-01-22 13:53:36,170] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047527375034987926
[2024-01-22 13:53:38,839] INFO: Iter 16200 Summary: 
[2024-01-22 13:53:38,839] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04709677532315254
[2024-01-22 13:53:41,527] INFO: Iter 16300 Summary: 
[2024-01-22 13:53:41,527] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047104740515351295
[2024-01-22 13:53:44,205] INFO: Iter 16400 Summary: 
[2024-01-22 13:53:44,205] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047521691508591174
[2024-01-22 13:53:46,705] INFO: Iter 16500 Summary: 
[2024-01-22 13:53:46,706] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04749669812619686
[2024-01-22 13:53:49,152] INFO: Iter 16600 Summary: 
[2024-01-22 13:53:49,152] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04702256716787815
[2024-01-22 13:53:51,881] INFO: Iter 16700 Summary: 
[2024-01-22 13:53:51,881] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047142961360514164
[2024-01-22 13:53:54,553] INFO: Iter 16800 Summary: 
[2024-01-22 13:53:54,554] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047391940020024774
[2024-01-22 13:53:57,205] INFO: Iter 16900 Summary: 
[2024-01-22 13:53:57,205] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04696229338645935
[2024-01-22 13:54:00,096] INFO: Iter 17000 Summary: 
[2024-01-22 13:54:00,096] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0470848548412323
[2024-01-22 13:54:02,609] INFO: Iter 17100 Summary: 
[2024-01-22 13:54:02,609] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04704034149646759
[2024-01-22 13:54:05,196] INFO: Iter 17200 Summary: 
[2024-01-22 13:54:05,196] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04688613008707762
[2024-01-22 13:54:07,842] INFO: Iter 17300 Summary: 
[2024-01-22 13:54:07,842] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047303516417741776
[2024-01-22 13:54:10,542] INFO: Iter 17400 Summary: 
[2024-01-22 13:54:10,542] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04702277224510908
[2024-01-22 13:54:13,254] INFO: Iter 17500 Summary: 
[2024-01-22 13:54:13,255] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046994357593357565
[2024-01-22 13:54:15,852] INFO: Iter 17600 Summary: 
[2024-01-22 13:54:15,852] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047015302143990995
[2024-01-22 13:54:18,550] INFO: Iter 17700 Summary: 
[2024-01-22 13:54:18,550] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04691867329180241
[2024-01-22 13:54:21,186] INFO: Iter 17800 Summary: 
[2024-01-22 13:54:21,186] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04682570464909077
[2024-01-22 13:54:23,730] INFO: Iter 17900 Summary: 
[2024-01-22 13:54:23,730] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04699292298406363
[2024-01-22 13:54:26,358] INFO: Iter 18000 Summary: 
[2024-01-22 13:54:26,359] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04688390050083399
[2024-01-22 13:54:29,004] INFO: Iter 18100 Summary: 
[2024-01-22 13:54:29,005] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0466834082826972
[2024-01-22 13:54:31,742] INFO: Iter 18200 Summary: 
[2024-01-22 13:54:31,742] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046771634519100186
[2024-01-22 13:54:34,328] INFO: Iter 18300 Summary: 
[2024-01-22 13:54:34,328] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046411977522075175
[2024-01-22 13:54:37,041] INFO: Iter 18400 Summary: 
[2024-01-22 13:54:37,041] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04663747265934944
[2024-01-22 13:54:39,875] INFO: Iter 18500 Summary: 
[2024-01-22 13:54:39,875] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046301402300596234
[2024-01-22 13:54:42,366] INFO: Iter 18600 Summary: 
[2024-01-22 13:54:42,366] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046533486507833004
[2024-01-22 13:54:45,249] INFO: Iter 18700 Summary: 
[2024-01-22 13:54:45,250] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04670566245913506
[2024-01-22 13:54:47,923] INFO: Iter 18800 Summary: 
[2024-01-22 13:54:47,923] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04641850620508194
[2024-01-22 13:54:50,501] INFO: Iter 18900 Summary: 
[2024-01-22 13:54:50,501] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046864335387945176
[2024-01-22 13:54:53,234] INFO: Iter 19000 Summary: 
[2024-01-22 13:54:53,234] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04654295220971107
[2024-01-22 13:54:56,055] INFO: Iter 19100 Summary: 
[2024-01-22 13:54:56,055] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04670261606574058
[2024-01-22 13:54:58,834] INFO: Iter 19200 Summary: 
[2024-01-22 13:54:58,834] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04689058270305395
[2024-01-22 13:55:01,598] INFO: Iter 19300 Summary: 
[2024-01-22 13:55:01,598] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04621621657162905
[2024-01-22 13:55:04,176] INFO: Iter 19400 Summary: 
[2024-01-22 13:55:04,177] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0469163141772151
[2024-01-22 13:55:06,778] INFO: Iter 19500 Summary: 
[2024-01-22 13:55:06,779] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0468096200376749
[2024-01-22 13:55:09,349] INFO: Iter 19600 Summary: 
[2024-01-22 13:55:09,350] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04663789842277765
[2024-01-22 13:55:11,999] INFO: Iter 19700 Summary: 
[2024-01-22 13:55:11,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046686282120645045
[2024-01-22 13:55:14,734] INFO: Iter 19800 Summary: 
[2024-01-22 13:55:14,734] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04639848861843347
[2024-01-22 13:55:17,501] INFO: Iter 19900 Summary: 
[2024-01-22 13:55:17,501] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04600336257368326
[2024-01-22 13:55:20,062] INFO: Iter 20000 Summary: 
[2024-01-22 13:55:20,063] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04631198897957802
[2024-01-22 13:55:22,942] INFO: Iter 20100 Summary: 
[2024-01-22 13:55:22,942] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04669143240898847
[2024-01-22 13:55:25,595] INFO: Iter 20200 Summary: 
[2024-01-22 13:55:25,596] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0463581769913435
[2024-01-22 13:55:28,204] INFO: Iter 20300 Summary: 
[2024-01-22 13:55:28,205] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04643050208687782
[2024-01-22 13:55:30,847] INFO: Iter 20400 Summary: 
[2024-01-22 13:55:30,847] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046057623066008094
[2024-01-22 13:55:33,688] INFO: Iter 20500 Summary: 
[2024-01-22 13:55:33,689] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04609758701175451
[2024-01-22 13:55:36,458] INFO: Iter 20600 Summary: 
[2024-01-22 13:55:36,458] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04617631785571575
[2024-01-22 13:55:39,126] INFO: Iter 20700 Summary: 
[2024-01-22 13:55:39,126] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0463547533005476
[2024-01-22 13:55:41,806] INFO: Iter 20800 Summary: 
[2024-01-22 13:55:41,806] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04659795332700014
[2024-01-22 13:55:44,633] INFO: Iter 20900 Summary: 
[2024-01-22 13:55:44,633] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046528822146356105
[2024-01-22 13:55:47,429] INFO: Iter 21000 Summary: 
[2024-01-22 13:55:47,429] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045476094707846644
[2024-01-22 13:55:50,089] INFO: Iter 21100 Summary: 
[2024-01-22 13:55:50,089] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04590485267341137
[2024-01-22 13:55:52,848] INFO: Iter 21200 Summary: 
[2024-01-22 13:55:52,848] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04617258235812187
[2024-01-22 13:55:55,386] INFO: Iter 21300 Summary: 
[2024-01-22 13:55:55,386] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046248045191168784
[2024-01-22 13:55:58,239] INFO: Iter 21400 Summary: 
[2024-01-22 13:55:58,239] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0465095903724432
[2024-01-22 13:56:00,734] INFO: Iter 21500 Summary: 
[2024-01-22 13:56:00,734] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045941870622336865
[2024-01-22 13:56:03,260] INFO: Iter 21600 Summary: 
[2024-01-22 13:56:03,260] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046163269355893134
[2024-01-22 13:56:05,913] INFO: Iter 21700 Summary: 
[2024-01-22 13:56:05,913] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04616012528538704
[2024-01-22 13:56:08,497] INFO: Iter 21800 Summary: 
[2024-01-22 13:56:08,497] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04622200038284063
[2024-01-22 13:56:11,074] INFO: Iter 21900 Summary: 
[2024-01-22 13:56:11,074] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045699743926525115
[2024-01-22 13:56:13,526] INFO: Iter 22000 Summary: 
[2024-01-22 13:56:13,526] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045917109809815886
[2024-01-22 13:56:16,088] INFO: Iter 22100 Summary: 
[2024-01-22 13:56:16,089] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04606839537620545
[2024-01-22 13:56:18,765] INFO: Iter 22200 Summary: 
[2024-01-22 13:56:18,766] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04588129080832005
[2024-01-22 13:56:21,427] INFO: Iter 22300 Summary: 
[2024-01-22 13:56:21,428] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0459102388843894
[2024-01-22 13:56:24,282] INFO: Iter 22400 Summary: 
[2024-01-22 13:56:24,282] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04579776998609304
[2024-01-22 13:56:26,868] INFO: Iter 22500 Summary: 
[2024-01-22 13:56:26,868] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046116192154586316
[2024-01-22 13:56:29,621] INFO: Iter 22600 Summary: 
[2024-01-22 13:56:29,621] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0459375723823905
[2024-01-22 13:56:32,212] INFO: Iter 22700 Summary: 
[2024-01-22 13:56:32,213] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046108558923006054
[2024-01-22 13:56:35,156] INFO: Iter 22800 Summary: 
[2024-01-22 13:56:35,156] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04576605558395386
[2024-01-22 13:56:37,999] INFO: Iter 22900 Summary: 
[2024-01-22 13:56:37,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04603926829993725
[2024-01-22 13:56:40,426] INFO: Iter 23000 Summary: 
[2024-01-22 13:56:40,426] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045849187150597576
[2024-01-22 13:56:43,091] INFO: Iter 23100 Summary: 
[2024-01-22 13:56:43,091] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0459752606600523
[2024-01-22 13:56:45,769] INFO: Iter 23200 Summary: 
[2024-01-22 13:56:45,770] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04567125964909792
[2024-01-22 13:56:48,332] INFO: Iter 23300 Summary: 
[2024-01-22 13:56:48,332] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045734838321805
[2024-01-22 13:56:51,112] INFO: Iter 23400 Summary: 
[2024-01-22 13:56:51,112] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04584750261157751
[2024-01-22 13:56:53,870] INFO: Iter 23500 Summary: 
[2024-01-22 13:56:53,870] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04551331385970116
[2024-01-22 13:56:56,598] INFO: Iter 23600 Summary: 
[2024-01-22 13:56:56,599] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04559624195098877
[2024-01-22 13:56:59,238] INFO: Iter 23700 Summary: 
[2024-01-22 13:56:59,238] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04579251352697611
[2024-01-22 13:57:02,002] INFO: Iter 23800 Summary: 
[2024-01-22 13:57:02,002] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04550216592848301
[2024-01-22 13:57:04,740] INFO: Iter 23900 Summary: 
[2024-01-22 13:57:04,740] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04571064051240683
[2024-01-22 13:57:07,285] INFO: Iter 24000 Summary: 
[2024-01-22 13:57:07,285] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04557710617780685
[2024-01-22 13:57:09,857] INFO: Iter 24100 Summary: 
[2024-01-22 13:57:09,857] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04555993128567934
[2024-01-22 13:57:12,571] INFO: Iter 24200 Summary: 
[2024-01-22 13:57:12,572] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04579159088432789
[2024-01-22 13:57:15,036] INFO: Iter 24300 Summary: 
[2024-01-22 13:57:15,036] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04565765466541052
[2024-01-22 13:57:17,682] INFO: Iter 24400 Summary: 
[2024-01-22 13:57:17,682] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0458477670699358
[2024-01-22 13:57:20,410] INFO: Iter 24500 Summary: 
[2024-01-22 13:57:20,410] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045329656042158606
[2024-01-22 13:57:23,186] INFO: Iter 24600 Summary: 
[2024-01-22 13:57:23,186] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045272482372820375
[2024-01-22 13:57:25,885] INFO: Iter 24700 Summary: 
[2024-01-22 13:57:25,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045387177541851995
[2024-01-22 13:57:28,527] INFO: Iter 24800 Summary: 
[2024-01-22 13:57:28,527] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04557086136192083
[2024-01-22 13:57:31,229] INFO: Iter 24900 Summary: 
[2024-01-22 13:57:31,229] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0457330897077918
[2024-01-22 13:57:33,723] INFO: Iter 25000 Summary: 
[2024-01-22 13:57:33,723] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04597416065633297
[2024-01-22 13:57:36,589] INFO: Iter 25100 Summary: 
[2024-01-22 13:57:36,589] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04544025357812643
[2024-01-22 13:57:39,398] INFO: Iter 25200 Summary: 
[2024-01-22 13:57:39,398] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045272440016269684
[2024-01-22 13:57:42,133] INFO: Iter 25300 Summary: 
[2024-01-22 13:57:42,134] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04574873484671116
[2024-01-22 13:57:44,904] INFO: Iter 25400 Summary: 
[2024-01-22 13:57:44,904] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0456286221370101
[2024-01-22 13:57:47,686] INFO: Iter 25500 Summary: 
[2024-01-22 13:57:47,686] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04562038067728281
[2024-01-22 13:57:50,264] INFO: Iter 25600 Summary: 
[2024-01-22 13:57:50,264] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04541127849370241
[2024-01-22 13:57:52,940] INFO: Iter 25700 Summary: 
[2024-01-22 13:57:52,940] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045482168793678286
[2024-01-22 13:57:55,557] INFO: Iter 25800 Summary: 
[2024-01-22 13:57:55,557] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04515011660754681
[2024-01-22 13:57:58,181] INFO: Iter 25900 Summary: 
[2024-01-22 13:57:58,182] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04576940082013607
[2024-01-22 13:58:00,836] INFO: Iter 26000 Summary: 
[2024-01-22 13:58:00,837] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045604912266135215
[2024-01-22 13:58:03,521] INFO: Iter 26100 Summary: 
[2024-01-22 13:58:03,521] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04572034109383821
[2024-01-22 13:58:06,293] INFO: Iter 26200 Summary: 
[2024-01-22 13:58:06,293] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045092867575585845
[2024-01-22 13:58:09,109] INFO: Iter 26300 Summary: 
[2024-01-22 13:58:09,110] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045705103501677516
[2024-01-22 13:58:11,647] INFO: Iter 26400 Summary: 
[2024-01-22 13:58:11,647] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04557540062814951
[2024-01-22 13:58:14,316] INFO: Iter 26500 Summary: 
[2024-01-22 13:58:14,316] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045473386235535145
[2024-01-22 13:58:17,045] INFO: Iter 26600 Summary: 
[2024-01-22 13:58:17,045] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04540474429726601
[2024-01-22 13:58:19,674] INFO: Iter 26700 Summary: 
[2024-01-22 13:58:19,674] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045290111899375915
[2024-01-22 13:58:22,408] INFO: Iter 26800 Summary: 
[2024-01-22 13:58:22,408] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045386251993477345
[2024-01-22 13:58:25,190] INFO: Iter 26900 Summary: 
[2024-01-22 13:58:25,190] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045400111600756646
[2024-01-22 13:58:27,825] INFO: Iter 27000 Summary: 
[2024-01-22 13:58:27,825] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04529704123735428
[2024-01-22 13:58:30,498] INFO: Iter 27100 Summary: 
[2024-01-22 13:58:30,498] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045281190387904645
[2024-01-22 13:58:33,370] INFO: Iter 27200 Summary: 
[2024-01-22 13:58:33,370] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045114658549427984
[2024-01-22 13:58:36,323] INFO: Iter 27300 Summary: 
[2024-01-22 13:58:36,323] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045332025252282616
[2024-01-22 13:58:38,868] INFO: Iter 27400 Summary: 
[2024-01-22 13:58:38,868] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04528139937669039
[2024-01-22 13:58:41,697] INFO: Iter 27500 Summary: 
[2024-01-22 13:58:41,698] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04512061383575201
[2024-01-22 13:58:44,453] INFO: Iter 27600 Summary: 
[2024-01-22 13:58:44,453] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04549434572458267
[2024-01-22 13:58:47,024] INFO: Iter 27700 Summary: 
[2024-01-22 13:58:47,024] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04555407240986824
[2024-01-22 13:58:49,774] INFO: Iter 27800 Summary: 
[2024-01-22 13:58:49,774] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045460631363093855
[2024-01-22 13:58:52,384] INFO: Iter 27900 Summary: 
[2024-01-22 13:58:52,385] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04525915149599314
[2024-01-22 13:58:55,198] INFO: Iter 28000 Summary: 
[2024-01-22 13:58:55,198] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045549661591649054
[2024-01-22 13:58:57,986] INFO: Iter 28100 Summary: 
[2024-01-22 13:58:57,986] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04523634601384401
[2024-01-22 13:59:00,769] INFO: Iter 28200 Summary: 
[2024-01-22 13:59:00,769] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04535118743777275
[2024-01-22 13:59:03,429] INFO: Iter 28300 Summary: 
[2024-01-22 13:59:03,429] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04502084832638502
[2024-01-22 13:59:06,101] INFO: Iter 28400 Summary: 
[2024-01-22 13:59:06,102] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045128929577767846
[2024-01-22 13:59:08,989] INFO: Iter 28500 Summary: 
[2024-01-22 13:59:08,989] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04518560249358416
[2024-01-22 13:59:11,892] INFO: Iter 28600 Summary: 
[2024-01-22 13:59:11,892] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045131494887173176
[2024-01-22 13:59:14,603] INFO: Iter 28700 Summary: 
[2024-01-22 13:59:14,603] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045109961591660974
[2024-01-22 13:59:17,459] INFO: Iter 28800 Summary: 
[2024-01-22 13:59:17,459] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04525389067828655
[2024-01-22 13:59:20,241] INFO: Iter 28900 Summary: 
[2024-01-22 13:59:20,241] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045159721858799455
[2024-01-22 13:59:23,156] INFO: Iter 29000 Summary: 
[2024-01-22 13:59:23,156] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04508607517927885
[2024-01-22 13:59:25,978] INFO: Iter 29100 Summary: 
[2024-01-22 13:59:25,978] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0453876207023859
[2024-01-22 13:59:28,764] INFO: Iter 29200 Summary: 
[2024-01-22 13:59:28,764] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04487114928662777
[2024-01-22 13:59:31,527] INFO: Iter 29300 Summary: 
[2024-01-22 13:59:31,527] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04521334007382393
[2024-01-22 13:59:34,404] INFO: Iter 29400 Summary: 
[2024-01-22 13:59:34,404] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0449483897164464
[2024-01-22 13:59:37,309] INFO: Iter 29500 Summary: 
[2024-01-22 13:59:37,309] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04547998912632465
[2024-01-22 13:59:40,129] INFO: Iter 29600 Summary: 
[2024-01-22 13:59:40,129] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04518247541040182
[2024-01-22 13:59:42,760] INFO: Iter 29700 Summary: 
[2024-01-22 13:59:42,760] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044944132678210735
[2024-01-22 13:59:45,537] INFO: Iter 29800 Summary: 
[2024-01-22 13:59:45,537] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04486124984920025
[2024-01-22 13:59:48,402] INFO: Iter 29900 Summary: 
[2024-01-22 13:59:48,402] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04527585532516241
[2024-01-22 13:59:51,338] INFO: Iter 30000 Summary: 
[2024-01-22 13:59:51,339] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045254053995013234
[2024-01-22 13:59:54,061] INFO: Iter 30100 Summary: 
[2024-01-22 13:59:54,061] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045291625708341596
[2024-01-22 13:59:56,825] INFO: Iter 30200 Summary: 
[2024-01-22 13:59:56,825] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04491608295589686
[2024-01-22 13:59:59,638] INFO: Iter 30300 Summary: 
[2024-01-22 13:59:59,638] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045014666356146336
[2024-01-22 14:00:02,419] INFO: Iter 30400 Summary: 
[2024-01-22 14:00:02,419] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04510005299001932
[2024-01-22 14:00:05,250] INFO: Iter 30500 Summary: 
[2024-01-22 14:00:05,250] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04506829239428043
[2024-01-22 14:00:08,110] INFO: Iter 30600 Summary: 
[2024-01-22 14:00:08,110] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04537637062370777
[2024-01-22 14:00:10,973] INFO: Iter 30700 Summary: 
[2024-01-22 14:00:10,973] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04508043888956308
[2024-01-22 14:00:13,585] INFO: Iter 30800 Summary: 
[2024-01-22 14:00:13,586] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04499805189669132
[2024-01-22 14:00:16,435] INFO: Iter 30900 Summary: 
[2024-01-22 14:00:16,435] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045044495239853856
[2024-01-22 14:00:19,299] INFO: Iter 31000 Summary: 
[2024-01-22 14:00:19,299] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045101208761334416
[2024-01-22 14:00:22,011] INFO: Iter 31100 Summary: 
[2024-01-22 14:00:22,012] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04489686530083418
[2024-01-22 14:00:24,758] INFO: Iter 31200 Summary: 
[2024-01-22 14:00:24,758] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04504450768232346
[2024-01-22 14:00:27,501] INFO: Iter 31300 Summary: 
[2024-01-22 14:00:27,501] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04472967263311148
[2024-01-22 14:00:30,268] INFO: Iter 31400 Summary: 
[2024-01-22 14:00:30,268] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04519813824445009
[2024-01-22 14:00:32,971] INFO: Iter 31500 Summary: 
[2024-01-22 14:00:32,971] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04519815061241388
[2024-01-22 14:00:35,896] INFO: Iter 31600 Summary: 
[2024-01-22 14:00:35,896] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04485997144132853
[2024-01-22 14:00:38,732] INFO: Iter 31700 Summary: 
[2024-01-22 14:00:38,732] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04482630867511034
[2024-01-22 14:00:41,481] INFO: Iter 31800 Summary: 
[2024-01-22 14:00:41,481] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04503353230655193
[2024-01-22 14:00:44,281] INFO: Iter 31900 Summary: 
[2024-01-22 14:00:44,281] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04480606198310852
[2024-01-22 14:00:47,124] INFO: Iter 32000 Summary: 
[2024-01-22 14:00:47,124] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04479106158018112
[2024-01-22 14:00:49,699] INFO: Iter 32100 Summary: 
[2024-01-22 14:00:49,699] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04524477321654558
[2024-01-22 14:00:52,529] INFO: Iter 32200 Summary: 
[2024-01-22 14:00:52,529] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04498413778841495
[2024-01-22 14:00:55,240] INFO: Iter 32300 Summary: 
[2024-01-22 14:00:55,240] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044817196764051916
[2024-01-22 14:00:58,013] INFO: Iter 32400 Summary: 
[2024-01-22 14:00:58,013] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044846213236451146
[2024-01-22 14:01:00,808] INFO: Iter 32500 Summary: 
[2024-01-22 14:01:00,808] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044576186873018744
[2024-01-22 14:01:03,798] INFO: Iter 32600 Summary: 
[2024-01-22 14:01:03,798] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044748813956975934
[2024-01-22 14:01:06,659] INFO: Iter 32700 Summary: 
[2024-01-22 14:01:06,659] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04461326595395804
[2024-01-22 14:01:09,308] INFO: Iter 32800 Summary: 
[2024-01-22 14:01:09,308] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04505858018994331
[2024-01-22 14:01:12,186] INFO: Iter 32900 Summary: 
[2024-01-22 14:01:12,186] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04503054067492485
[2024-01-22 14:01:15,064] INFO: Iter 33000 Summary: 
[2024-01-22 14:01:15,064] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04482305847108364
[2024-01-22 14:01:17,961] INFO: Iter 33100 Summary: 
[2024-01-22 14:01:17,961] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044468103237450124
[2024-01-22 14:01:21,027] INFO: Iter 33200 Summary: 
[2024-01-22 14:01:21,027] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04470083937048912
[2024-01-22 14:01:23,901] INFO: Iter 33300 Summary: 
[2024-01-22 14:01:23,901] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04483660466969013
[2024-01-22 14:01:26,665] INFO: Iter 33400 Summary: 
[2024-01-22 14:01:26,666] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045464206263422964
[2024-01-22 14:01:29,474] INFO: Iter 33500 Summary: 
[2024-01-22 14:01:29,475] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04491284091025591
[2024-01-22 14:01:32,365] INFO: Iter 33600 Summary: 
[2024-01-22 14:01:32,365] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04502538487315178
[2024-01-22 14:01:35,139] INFO: Iter 33700 Summary: 
[2024-01-22 14:01:35,139] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04496302921324968
[2024-01-22 14:01:37,789] INFO: Iter 33800 Summary: 
[2024-01-22 14:01:37,789] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044838326275348665
[2024-01-22 14:01:40,541] INFO: Iter 33900 Summary: 
[2024-01-22 14:01:40,541] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0448283039405942
[2024-01-22 14:01:43,462] INFO: Iter 34000 Summary: 
[2024-01-22 14:01:43,462] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04492612939327955
[2024-01-22 14:01:46,096] INFO: Iter 34100 Summary: 
[2024-01-22 14:01:46,096] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04484850499778986
[2024-01-22 14:01:48,906] INFO: Iter 34200 Summary: 
[2024-01-22 14:01:48,906] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044051897674798966
[2024-01-22 14:01:51,801] INFO: Iter 34300 Summary: 
[2024-01-22 14:01:51,801] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04471685200929642
[2024-01-22 14:01:54,730] INFO: Iter 34400 Summary: 
[2024-01-22 14:01:54,730] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04495439939200878
[2024-01-22 14:01:57,617] INFO: Iter 34500 Summary: 
[2024-01-22 14:01:57,617] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044357880018651485
[2024-01-22 14:02:00,493] INFO: Iter 34600 Summary: 
[2024-01-22 14:02:00,493] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04471980236470699
[2024-01-22 14:02:03,116] INFO: Iter 34700 Summary: 
[2024-01-22 14:02:03,116] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04486629579216242
[2024-01-22 14:02:05,814] INFO: Iter 34800 Summary: 
[2024-01-22 14:02:05,814] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044809349440038206
[2024-01-22 14:02:08,614] INFO: Iter 34900 Summary: 
[2024-01-22 14:02:08,615] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04463400688022375
[2024-01-22 14:02:11,463] INFO: Iter 35000 Summary: 
[2024-01-22 14:02:11,463] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044905510544776914
[2024-01-22 14:02:13,994] INFO: Iter 35100 Summary: 
[2024-01-22 14:02:13,994] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044722521379590034
[2024-01-22 14:02:16,819] INFO: Iter 35200 Summary: 
[2024-01-22 14:02:16,820] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04476324062794447
[2024-01-22 14:02:19,495] INFO: Iter 35300 Summary: 
[2024-01-22 14:02:19,495] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04465288441628218
[2024-01-22 14:02:22,390] INFO: Iter 35400 Summary: 
[2024-01-22 14:02:22,391] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044735405296087265
[2024-01-22 14:02:24,969] INFO: Iter 35500 Summary: 
[2024-01-22 14:02:24,969] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04492924679070711
[2024-01-22 14:02:27,663] INFO: Iter 35600 Summary: 
[2024-01-22 14:02:27,663] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04455345705151558
[2024-01-22 14:02:30,328] INFO: Iter 35700 Summary: 
[2024-01-22 14:02:30,329] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044925301261246206
[2024-01-22 14:02:33,022] INFO: Iter 35800 Summary: 
[2024-01-22 14:02:33,022] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04443782515823841
[2024-01-22 14:02:35,806] INFO: Iter 35900 Summary: 
[2024-01-22 14:02:35,806] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044402707666158676
[2024-01-22 14:02:38,520] INFO: Iter 36000 Summary: 
[2024-01-22 14:02:38,521] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04445127822458744
[2024-01-22 14:02:41,407] INFO: Iter 36100 Summary: 
[2024-01-22 14:02:41,407] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04492859445512295
[2024-01-22 14:02:44,083] INFO: Iter 36200 Summary: 
[2024-01-22 14:02:44,084] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044697328321635726
[2024-01-22 14:02:46,734] INFO: Iter 36300 Summary: 
[2024-01-22 14:02:46,734] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044330012425780295
[2024-01-22 14:02:49,687] INFO: Iter 36400 Summary: 
[2024-01-22 14:02:49,687] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044534142836928366
[2024-01-22 14:02:52,376] INFO: Iter 36500 Summary: 
[2024-01-22 14:02:52,376] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04464236844331026
[2024-01-22 14:02:55,366] INFO: Iter 36600 Summary: 
[2024-01-22 14:02:55,366] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044708198420703414
[2024-01-22 14:02:58,321] INFO: Iter 36700 Summary: 
[2024-01-22 14:02:58,321] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04465511154383421
[2024-01-22 14:03:01,031] INFO: Iter 36800 Summary: 
[2024-01-22 14:03:01,032] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04464791379868984
[2024-01-22 14:03:03,774] INFO: Iter 36900 Summary: 
[2024-01-22 14:03:03,774] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04443353489041328
[2024-01-22 14:03:06,490] INFO: Iter 37000 Summary: 
[2024-01-22 14:03:06,491] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04423059891909361
[2024-01-22 14:03:09,216] INFO: Iter 37100 Summary: 
[2024-01-22 14:03:09,216] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04437287826091051
[2024-01-22 14:03:11,838] INFO: Iter 37200 Summary: 
[2024-01-22 14:03:11,838] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04466878406703472
[2024-01-22 14:03:14,634] INFO: Iter 37300 Summary: 
[2024-01-22 14:03:14,634] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04461691327393055
[2024-01-22 14:03:17,518] INFO: Iter 37400 Summary: 
[2024-01-22 14:03:17,518] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04443209517747164
[2024-01-22 14:03:20,049] INFO: Iter 37500 Summary: 
[2024-01-22 14:03:20,049] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04481430195271969
[2024-01-22 14:03:22,789] INFO: Iter 37600 Summary: 
[2024-01-22 14:03:22,789] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04429483227431774
[2024-01-22 14:03:25,629] INFO: Iter 37700 Summary: 
[2024-01-22 14:03:25,629] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04476049568504095
[2024-01-22 14:03:28,164] INFO: Iter 37800 Summary: 
[2024-01-22 14:03:28,165] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04469236940145493
[2024-01-22 14:03:30,942] INFO: Iter 37900 Summary: 
[2024-01-22 14:03:30,942] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0443402872979641
[2024-01-22 14:03:33,766] INFO: Iter 38000 Summary: 
[2024-01-22 14:03:33,766] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044399369210004806
[2024-01-22 14:03:36,621] INFO: Iter 38100 Summary: 
[2024-01-22 14:03:36,621] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04439152993261814
[2024-01-22 14:03:39,214] INFO: Iter 38200 Summary: 
[2024-01-22 14:03:39,214] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04447150401771069
[2024-01-22 14:03:42,104] INFO: Iter 38300 Summary: 
[2024-01-22 14:03:42,104] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044981707483530045
[2024-01-22 14:03:44,982] INFO: Iter 38400 Summary: 
[2024-01-22 14:03:44,982] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04449398070573807
[2024-01-22 14:03:47,734] INFO: Iter 38500 Summary: 
[2024-01-22 14:03:47,734] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04444608751684427
[2024-01-22 14:03:50,566] INFO: Iter 38600 Summary: 
[2024-01-22 14:03:50,566] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044701854921877386
[2024-01-22 14:03:53,351] INFO: Iter 38700 Summary: 
[2024-01-22 14:03:53,351] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04434756238013506
[2024-01-22 14:03:56,296] INFO: Iter 38800 Summary: 
[2024-01-22 14:03:56,296] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04489927925169468
[2024-01-22 14:03:58,994] INFO: Iter 38900 Summary: 
[2024-01-22 14:03:58,994] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04475240811705589
[2024-01-22 14:04:01,732] INFO: Iter 39000 Summary: 
[2024-01-22 14:04:01,732] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04452080335468054
[2024-01-22 14:04:04,518] INFO: Iter 39100 Summary: 
[2024-01-22 14:04:04,518] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04487766440957785
[2024-01-22 14:04:07,094] INFO: Iter 39200 Summary: 
[2024-01-22 14:04:07,094] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04395133994519711
[2024-01-22 14:04:09,832] INFO: Iter 39300 Summary: 
[2024-01-22 14:04:09,832] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044668477699160575
[2024-01-22 14:04:12,704] INFO: Iter 39400 Summary: 
[2024-01-22 14:04:12,704] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04467225484549999
[2024-01-22 14:04:15,432] INFO: Iter 39500 Summary: 
[2024-01-22 14:04:15,432] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044399302788078784
[2024-01-22 14:04:18,208] INFO: Iter 39600 Summary: 
[2024-01-22 14:04:18,208] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044568312168121335
[2024-01-22 14:04:20,944] INFO: Iter 39700 Summary: 
[2024-01-22 14:04:20,944] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044782719686627386
[2024-01-22 14:04:23,852] INFO: Iter 39800 Summary: 
[2024-01-22 14:04:23,853] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04455156814306974
[2024-01-22 14:04:26,680] INFO: Iter 39900 Summary: 
[2024-01-22 14:04:26,681] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04481954544782638
[2024-01-22 14:04:29,528] INFO: Iter 40000 Summary: 
[2024-01-22 14:04:29,529] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044212659522891044
[2024-01-22 14:04:32,406] INFO: Iter 40100 Summary: 
[2024-01-22 14:04:32,406] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0445568160712719
[2024-01-22 14:04:35,234] INFO: Iter 40200 Summary: 
[2024-01-22 14:04:35,234] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0445643762126565
[2024-01-22 14:04:38,136] INFO: Iter 40300 Summary: 
[2024-01-22 14:04:38,136] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04422792997211218
[2024-01-22 14:04:40,974] INFO: Iter 40400 Summary: 
[2024-01-22 14:04:40,974] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044246027283370494
[2024-01-22 14:04:43,816] INFO: Iter 40500 Summary: 
[2024-01-22 14:04:43,816] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04470537330955267
[2024-01-22 14:04:46,680] INFO: Iter 40600 Summary: 
[2024-01-22 14:04:46,680] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04475648570805788
[2024-01-22 14:04:49,481] INFO: Iter 40700 Summary: 
[2024-01-22 14:04:49,481] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04435725409537554
[2024-01-22 14:04:52,403] INFO: Iter 40800 Summary: 
[2024-01-22 14:04:52,403] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04413379952311516
[2024-01-22 14:04:55,136] INFO: Iter 40900 Summary: 
[2024-01-22 14:04:55,136] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044557572491466996
[2024-01-22 14:04:57,942] INFO: Iter 41000 Summary: 
[2024-01-22 14:04:57,942] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0443427662178874
[2024-01-22 14:05:00,804] INFO: Iter 41100 Summary: 
[2024-01-22 14:05:00,804] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04436950463801623
[2024-01-22 14:05:03,429] INFO: Iter 41200 Summary: 
[2024-01-22 14:05:03,430] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044499370455741885
[2024-01-22 14:05:06,339] INFO: Iter 41300 Summary: 
[2024-01-22 14:05:06,339] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04460002228617668
[2024-01-22 14:05:09,156] INFO: Iter 41400 Summary: 
[2024-01-22 14:05:09,156] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044349553808569905
[2024-01-22 14:05:11,936] INFO: Iter 41500 Summary: 
[2024-01-22 14:05:11,937] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04429648321121931
[2024-01-22 14:05:14,653] INFO: Iter 41600 Summary: 
[2024-01-22 14:05:14,654] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044626469165086745
[2024-01-22 14:05:17,278] INFO: Iter 41700 Summary: 
[2024-01-22 14:05:17,278] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04431590519845486
[2024-01-22 14:05:20,119] INFO: Iter 41800 Summary: 
[2024-01-22 14:05:20,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04440809927880764
[2024-01-22 14:05:22,886] INFO: Iter 41900 Summary: 
[2024-01-22 14:05:22,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04421343807131052
[2024-01-22 14:05:25,701] INFO: Iter 42000 Summary: 
[2024-01-22 14:05:25,701] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04414321180433035
[2024-01-22 14:05:28,441] INFO: Iter 42100 Summary: 
[2024-01-22 14:05:28,441] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04454992685467005
[2024-01-22 14:05:30,990] INFO: Iter 42200 Summary: 
[2024-01-22 14:05:30,990] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04425445590168238
[2024-01-22 14:05:33,661] INFO: Iter 42300 Summary: 
[2024-01-22 14:05:33,661] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04469703525304794
[2024-01-22 14:05:36,389] INFO: Iter 42400 Summary: 
[2024-01-22 14:05:36,389] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394940726459026
[2024-01-22 14:05:39,156] INFO: Iter 42500 Summary: 
[2024-01-22 14:05:39,156] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04434901475906372
[2024-01-22 14:05:41,666] INFO: Iter 42600 Summary: 
[2024-01-22 14:05:41,666] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044093768410384655
[2024-01-22 14:05:44,551] INFO: Iter 42700 Summary: 
[2024-01-22 14:05:44,552] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04474753171205521
[2024-01-22 14:05:47,411] INFO: Iter 42800 Summary: 
[2024-01-22 14:05:47,412] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04424941297620535
[2024-01-22 14:05:50,174] INFO: Iter 42900 Summary: 
[2024-01-22 14:05:50,174] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044090617001056674
[2024-01-22 14:05:52,875] INFO: Iter 43000 Summary: 
[2024-01-22 14:05:52,875] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04463572792708874
[2024-01-22 14:05:55,770] INFO: Iter 43100 Summary: 
[2024-01-22 14:05:55,770] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04419359404593706
[2024-01-22 14:05:58,524] INFO: Iter 43200 Summary: 
[2024-01-22 14:05:58,524] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04409001473337412
[2024-01-22 14:06:01,273] INFO: Iter 43300 Summary: 
[2024-01-22 14:06:01,274] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04431921426206827
[2024-01-22 14:06:04,079] INFO: Iter 43400 Summary: 
[2024-01-22 14:06:04,079] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04431312762200832
[2024-01-22 14:06:06,650] INFO: Iter 43500 Summary: 
[2024-01-22 14:06:06,650] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044134990833699705
[2024-01-22 14:06:09,378] INFO: Iter 43600 Summary: 
[2024-01-22 14:06:09,378] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044267678670585155
[2024-01-22 14:06:12,266] INFO: Iter 43700 Summary: 
[2024-01-22 14:06:12,266] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044191661328077315
[2024-01-22 14:06:15,041] INFO: Iter 43800 Summary: 
[2024-01-22 14:06:15,042] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04417985554784536
[2024-01-22 14:06:17,688] INFO: Iter 43900 Summary: 
[2024-01-22 14:06:17,688] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04406870752573013
[2024-01-22 14:06:20,574] INFO: Iter 44000 Summary: 
[2024-01-22 14:06:20,574] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044047780372202396
[2024-01-22 14:06:23,452] INFO: Iter 44100 Summary: 
[2024-01-22 14:06:23,452] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04425002094358206
[2024-01-22 14:06:26,219] INFO: Iter 44200 Summary: 
[2024-01-22 14:06:26,219] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044706161059439184
[2024-01-22 14:06:28,979] INFO: Iter 44300 Summary: 
[2024-01-22 14:06:28,979] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044686140194535255
[2024-01-22 14:06:31,697] INFO: Iter 44400 Summary: 
[2024-01-22 14:06:31,697] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0441898363083601
[2024-01-22 14:06:34,486] INFO: Iter 44500 Summary: 
[2024-01-22 14:06:34,486] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044293852038681504
[2024-01-22 14:06:37,086] INFO: Iter 44600 Summary: 
[2024-01-22 14:06:37,086] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418995801359415
[2024-01-22 14:06:39,802] INFO: Iter 44700 Summary: 
[2024-01-22 14:06:39,802] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044537892527878285
[2024-01-22 14:06:42,736] INFO: Iter 44800 Summary: 
[2024-01-22 14:06:42,736] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04393640100955963
[2024-01-22 14:06:45,393] INFO: Iter 44900 Summary: 
[2024-01-22 14:06:45,393] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04425602272152901
[2024-01-22 14:06:48,346] INFO: Iter 45000 Summary: 
[2024-01-22 14:06:48,346] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04417009674012661
[2024-01-22 14:06:51,199] INFO: Iter 45100 Summary: 
[2024-01-22 14:06:51,199] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04422391451895237
[2024-01-22 14:06:53,930] INFO: Iter 45200 Summary: 
[2024-01-22 14:06:53,930] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04406875152140856
[2024-01-22 14:06:56,546] INFO: Iter 45300 Summary: 
[2024-01-22 14:06:56,546] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04411280136555433
[2024-01-22 14:06:59,393] INFO: Iter 45400 Summary: 
[2024-01-22 14:06:59,393] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044458531737327577
[2024-01-22 14:07:02,258] INFO: Iter 45500 Summary: 
[2024-01-22 14:07:02,258] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044343303851783276
[2024-01-22 14:07:05,114] INFO: Iter 45600 Summary: 
[2024-01-22 14:07:05,114] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044400685504078864
[2024-01-22 14:07:08,022] INFO: Iter 45700 Summary: 
[2024-01-22 14:07:08,022] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04411705203354359
[2024-01-22 14:07:10,835] INFO: Iter 45800 Summary: 
[2024-01-22 14:07:10,835] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044347580783069135
[2024-01-22 14:07:13,620] INFO: Iter 45900 Summary: 
[2024-01-22 14:07:13,620] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044464138559997084
[2024-01-22 14:07:16,359] INFO: Iter 46000 Summary: 
[2024-01-22 14:07:16,359] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044408814236521724
[2024-01-22 14:07:19,138] INFO: Iter 46100 Summary: 
[2024-01-22 14:07:19,139] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0442649644613266
[2024-01-22 14:07:22,069] INFO: Iter 46200 Summary: 
[2024-01-22 14:07:22,069] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04459896825253964
[2024-01-22 14:07:24,622] INFO: Iter 46300 Summary: 
[2024-01-22 14:07:24,622] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04436649113893509
[2024-01-22 14:07:27,442] INFO: Iter 46400 Summary: 
[2024-01-22 14:07:27,442] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04448272041976452
[2024-01-22 14:07:30,219] INFO: Iter 46500 Summary: 
[2024-01-22 14:07:30,220] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04421328019350767
[2024-01-22 14:07:33,149] INFO: Iter 46600 Summary: 
[2024-01-22 14:07:33,149] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04402024310082197
[2024-01-22 14:07:36,118] INFO: Iter 46700 Summary: 
[2024-01-22 14:07:36,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044214896857738495
[2024-01-22 14:07:38,968] INFO: Iter 46800 Summary: 
[2024-01-22 14:07:38,968] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044452504627406596
[2024-01-22 14:07:41,488] INFO: Iter 46900 Summary: 
[2024-01-22 14:07:41,488] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044416547641158106
[2024-01-22 14:07:44,090] INFO: Iter 47000 Summary: 
[2024-01-22 14:07:44,090] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04395751610398293
[2024-01-22 14:07:46,898] INFO: Iter 47100 Summary: 
[2024-01-22 14:07:46,898] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394534755498171
[2024-01-22 14:07:49,757] INFO: Iter 47200 Summary: 
[2024-01-22 14:07:49,757] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04461777653545141
[2024-01-22 14:07:52,518] INFO: Iter 47300 Summary: 
[2024-01-22 14:07:52,519] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04403651304543019
[2024-01-22 14:07:55,257] INFO: Iter 47400 Summary: 
[2024-01-22 14:07:55,257] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043670238740742204
[2024-01-22 14:07:58,250] INFO: Iter 47500 Summary: 
[2024-01-22 14:07:58,251] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043949416093528273
[2024-01-22 14:08:00,942] INFO: Iter 47600 Summary: 
[2024-01-22 14:08:00,942] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04413371510803699
[2024-01-22 14:08:03,749] INFO: Iter 47700 Summary: 
[2024-01-22 14:08:03,749] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04390901431441307
[2024-01-22 14:08:06,602] INFO: Iter 47800 Summary: 
[2024-01-22 14:08:06,602] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044164850004017354
[2024-01-22 14:08:09,507] INFO: Iter 47900 Summary: 
[2024-01-22 14:08:09,507] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044299255013465884
[2024-01-22 14:08:12,246] INFO: Iter 48000 Summary: 
[2024-01-22 14:08:12,246] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044095539189875126
[2024-01-22 14:08:14,967] INFO: Iter 48100 Summary: 
[2024-01-22 14:08:14,968] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04410740088671446
[2024-01-22 14:08:17,806] INFO: Iter 48200 Summary: 
[2024-01-22 14:08:17,806] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04430768486112356
[2024-01-22 14:08:20,495] INFO: Iter 48300 Summary: 
[2024-01-22 14:08:20,495] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0439737718552351
[2024-01-22 14:08:23,410] INFO: Iter 48400 Summary: 
[2024-01-22 14:08:23,410] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04379990708082914
[2024-01-22 14:08:26,279] INFO: Iter 48500 Summary: 
[2024-01-22 14:08:26,279] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044049440510571004
[2024-01-22 14:08:29,132] INFO: Iter 48600 Summary: 
[2024-01-22 14:08:29,133] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0438743345066905
[2024-01-22 14:08:32,065] INFO: Iter 48700 Summary: 
[2024-01-22 14:08:32,065] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04411283746361733
[2024-01-22 14:08:35,016] INFO: Iter 48800 Summary: 
[2024-01-22 14:08:35,016] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04396601743996143
[2024-01-22 14:08:37,853] INFO: Iter 48900 Summary: 
[2024-01-22 14:08:37,853] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04410636231303215
[2024-01-22 14:08:40,538] INFO: Iter 49000 Summary: 
[2024-01-22 14:08:40,538] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04380808066576719
[2024-01-22 14:08:43,241] INFO: Iter 49100 Summary: 
[2024-01-22 14:08:43,241] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044020503535866735
[2024-01-22 14:08:46,066] INFO: Iter 49200 Summary: 
[2024-01-22 14:08:46,066] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043788551092147826
[2024-01-22 14:08:48,720] INFO: Iter 49300 Summary: 
[2024-01-22 14:08:48,721] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04397916577756405
[2024-01-22 14:08:51,629] INFO: Iter 49400 Summary: 
[2024-01-22 14:08:51,630] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418273381888867
[2024-01-22 14:08:54,529] INFO: Iter 49500 Summary: 
[2024-01-22 14:08:54,529] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04419493719935417
[2024-01-22 14:08:57,270] INFO: Iter 49600 Summary: 
[2024-01-22 14:08:57,270] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04413770224899054
[2024-01-22 14:09:00,052] INFO: Iter 49700 Summary: 
[2024-01-22 14:09:00,052] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394608270376921
[2024-01-22 14:09:02,888] INFO: Iter 49800 Summary: 
[2024-01-22 14:09:02,888] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04376608394086361
[2024-01-22 14:09:05,811] INFO: Iter 49900 Summary: 
[2024-01-22 14:09:05,811] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04420320503413677
[2024-01-22 14:09:08,581] INFO: Iter 50000 Summary: 
[2024-01-22 14:09:08,582] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0442481679096818
[2024-01-22 14:09:11,428] INFO: Iter 50100 Summary: 
[2024-01-22 14:09:11,428] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04395203884691
[2024-01-22 14:09:14,312] INFO: Iter 50200 Summary: 
[2024-01-22 14:09:14,313] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04383245714008808
[2024-01-22 14:09:16,995] INFO: Iter 50300 Summary: 
[2024-01-22 14:09:16,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04382617417722941
[2024-01-22 14:09:19,930] INFO: Iter 50400 Summary: 
[2024-01-22 14:09:19,931] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04406652592122555
[2024-01-22 14:09:22,787] INFO: Iter 50500 Summary: 
[2024-01-22 14:09:22,787] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04390277836471796
[2024-01-22 14:09:25,742] INFO: Iter 50600 Summary: 
[2024-01-22 14:09:25,742] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04420916713774204
[2024-01-22 14:09:28,562] INFO: Iter 50700 Summary: 
[2024-01-22 14:09:28,563] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043891807273030284
[2024-01-22 14:09:31,554] INFO: Iter 50800 Summary: 
[2024-01-22 14:09:31,554] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04406230077147484
[2024-01-22 14:09:34,278] INFO: Iter 50900 Summary: 
[2024-01-22 14:09:34,279] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394301112741232
[2024-01-22 14:09:37,024] INFO: Iter 51000 Summary: 
[2024-01-22 14:09:37,024] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04406560115516186
[2024-01-22 14:09:39,972] INFO: Iter 51100 Summary: 
[2024-01-22 14:09:39,973] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04406145315617323
[2024-01-22 14:09:42,847] INFO: Iter 51200 Summary: 
[2024-01-22 14:09:42,847] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04369499616324902
[2024-01-22 14:09:45,546] INFO: Iter 51300 Summary: 
[2024-01-22 14:09:45,546] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044002484939992426
[2024-01-22 14:09:48,414] INFO: Iter 51400 Summary: 
[2024-01-22 14:09:48,414] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04395523644983768
[2024-01-22 14:09:51,385] INFO: Iter 51500 Summary: 
[2024-01-22 14:09:51,385] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043987751975655555
[2024-01-22 14:09:54,165] INFO: Iter 51600 Summary: 
[2024-01-22 14:09:54,165] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043885808177292346
[2024-01-22 14:09:56,891] INFO: Iter 51700 Summary: 
[2024-01-22 14:09:56,891] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04378698837012052
[2024-01-22 14:09:59,686] INFO: Iter 51800 Summary: 
[2024-01-22 14:09:59,686] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04399302560836077
[2024-01-22 14:10:02,518] INFO: Iter 51900 Summary: 
[2024-01-22 14:10:02,518] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044159659557044505
[2024-01-22 14:10:05,274] INFO: Iter 52000 Summary: 
[2024-01-22 14:10:05,274] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04414406523108482
[2024-01-22 14:10:08,043] INFO: Iter 52100 Summary: 
[2024-01-22 14:10:08,043] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04413069352507591
[2024-01-22 14:10:10,726] INFO: Iter 52200 Summary: 
[2024-01-22 14:10:10,726] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04396277103573084
[2024-01-22 14:10:13,353] INFO: Iter 52300 Summary: 
[2024-01-22 14:10:13,353] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04405186913907528
[2024-01-22 14:10:16,056] INFO: Iter 52400 Summary: 
[2024-01-22 14:10:16,057] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04404786244034767
[2024-01-22 14:10:18,745] INFO: Iter 52500 Summary: 
[2024-01-22 14:10:18,745] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04377143792808056
[2024-01-22 14:10:21,521] INFO: Iter 52600 Summary: 
[2024-01-22 14:10:21,522] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043929076716303826
[2024-01-22 14:10:24,168] INFO: Iter 52700 Summary: 
[2024-01-22 14:10:24,169] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04401570755988359
[2024-01-22 14:10:26,838] INFO: Iter 52800 Summary: 
[2024-01-22 14:10:26,838] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043996781036257746
[2024-01-22 14:10:29,670] INFO: Iter 52900 Summary: 
[2024-01-22 14:10:29,670] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04402135241776705
[2024-01-22 14:10:32,116] INFO: Iter 53000 Summary: 
[2024-01-22 14:10:32,116] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04405866872519255
[2024-01-22 14:10:34,695] INFO: Iter 53100 Summary: 
[2024-01-22 14:10:34,695] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04385048046708107
[2024-01-22 14:10:37,441] INFO: Iter 53200 Summary: 
[2024-01-22 14:10:37,441] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04381654851138592
[2024-01-22 14:10:40,266] INFO: Iter 53300 Summary: 
[2024-01-22 14:10:40,266] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043693387284874916
[2024-01-22 14:10:42,962] INFO: Iter 53400 Summary: 
[2024-01-22 14:10:42,962] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043944409005343917
[2024-01-22 14:10:45,704] INFO: Iter 53500 Summary: 
[2024-01-22 14:10:45,704] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04423116996884346
[2024-01-22 14:10:48,426] INFO: Iter 53600 Summary: 
[2024-01-22 14:10:48,427] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04417278077453375
[2024-01-22 14:10:51,166] INFO: Iter 53700 Summary: 
[2024-01-22 14:10:51,166] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044004873856902126
[2024-01-22 14:10:53,891] INFO: Iter 53800 Summary: 
[2024-01-22 14:10:53,891] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361132580786944
[2024-01-22 14:10:56,803] INFO: Iter 53900 Summary: 
[2024-01-22 14:10:56,803] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371348008513451
[2024-01-22 14:10:59,610] INFO: Iter 54000 Summary: 
[2024-01-22 14:10:59,610] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04391452830284834
[2024-01-22 14:11:02,392] INFO: Iter 54100 Summary: 
[2024-01-22 14:11:02,392] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04397476401180029
[2024-01-22 14:11:05,170] INFO: Iter 54200 Summary: 
[2024-01-22 14:11:05,170] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043603163585066794
[2024-01-22 14:11:07,852] INFO: Iter 54300 Summary: 
[2024-01-22 14:11:07,853] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04391431480646133
[2024-01-22 14:11:10,472] INFO: Iter 54400 Summary: 
[2024-01-22 14:11:10,472] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043898954689502716
[2024-01-22 14:11:13,033] INFO: Iter 54500 Summary: 
[2024-01-22 14:11:13,034] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043878773264586925
[2024-01-22 14:11:15,717] INFO: Iter 54600 Summary: 
[2024-01-22 14:11:15,717] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04392325233668089
[2024-01-22 14:11:18,363] INFO: Iter 54700 Summary: 
[2024-01-22 14:11:18,363] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04428016416728497
[2024-01-22 14:11:21,287] INFO: Iter 54800 Summary: 
[2024-01-22 14:11:21,288] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04408650983124971
[2024-01-22 14:11:24,072] INFO: Iter 54900 Summary: 
[2024-01-22 14:11:24,072] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04392718560993671
[2024-01-22 14:11:26,790] INFO: Iter 55000 Summary: 
[2024-01-22 14:11:26,790] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043966658599674704
[2024-01-22 14:11:29,423] INFO: Iter 55100 Summary: 
[2024-01-22 14:11:29,423] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0439915781468153
[2024-01-22 14:11:32,269] INFO: Iter 55200 Summary: 
[2024-01-22 14:11:32,269] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0438004157319665
[2024-01-22 14:11:35,059] INFO: Iter 55300 Summary: 
[2024-01-22 14:11:35,059] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044013229608535764
[2024-01-22 14:11:37,894] INFO: Iter 55400 Summary: 
[2024-01-22 14:11:37,894] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043968032598495486
[2024-01-22 14:11:40,751] INFO: Iter 55500 Summary: 
[2024-01-22 14:11:40,752] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044005999192595485
[2024-01-22 14:11:43,573] INFO: Iter 55600 Summary: 
[2024-01-22 14:11:43,573] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394926346838474
[2024-01-22 14:11:46,418] INFO: Iter 55700 Summary: 
[2024-01-22 14:11:46,419] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043673373274505135
[2024-01-22 14:11:49,295] INFO: Iter 55800 Summary: 
[2024-01-22 14:11:49,295] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043812400735914706
[2024-01-22 14:11:52,047] INFO: Iter 55900 Summary: 
[2024-01-22 14:11:52,047] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04381252605468035
[2024-01-22 14:11:54,862] INFO: Iter 56000 Summary: 
[2024-01-22 14:11:54,862] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044081120640039446
[2024-01-22 14:11:57,606] INFO: Iter 56100 Summary: 
[2024-01-22 14:11:57,606] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043831348866224286
[2024-01-22 14:12:00,441] INFO: Iter 56200 Summary: 
[2024-01-22 14:12:00,441] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0439503188803792
[2024-01-22 14:12:03,393] INFO: Iter 56300 Summary: 
[2024-01-22 14:12:03,393] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043984300829470156
[2024-01-22 14:12:06,093] INFO: Iter 56400 Summary: 
[2024-01-22 14:12:06,093] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04382073782384396
[2024-01-22 14:12:08,831] INFO: Iter 56500 Summary: 
[2024-01-22 14:12:08,831] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04416194286197424
[2024-01-22 14:12:11,619] INFO: Iter 56600 Summary: 
[2024-01-22 14:12:11,619] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0439672913402319
[2024-01-22 14:12:14,277] INFO: Iter 56700 Summary: 
[2024-01-22 14:12:14,277] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04389167048037052
[2024-01-22 14:12:17,110] INFO: Iter 56800 Summary: 
[2024-01-22 14:12:17,110] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04374634020030498
[2024-01-22 14:12:19,981] INFO: Iter 56900 Summary: 
[2024-01-22 14:12:19,981] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0440454512834549
[2024-01-22 14:12:22,954] INFO: Iter 57000 Summary: 
[2024-01-22 14:12:22,954] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044008293896913526
[2024-01-22 14:12:25,620] INFO: Iter 57100 Summary: 
[2024-01-22 14:12:25,620] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0437512531504035
[2024-01-22 14:12:28,486] INFO: Iter 57200 Summary: 
[2024-01-22 14:12:28,487] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043806279301643374
[2024-01-22 14:12:31,340] INFO: Iter 57300 Summary: 
[2024-01-22 14:12:31,340] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418331295251846
[2024-01-22 14:12:33,933] INFO: Iter 57400 Summary: 
[2024-01-22 14:12:33,933] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361459665000439
[2024-01-22 14:12:36,827] INFO: Iter 57500 Summary: 
[2024-01-22 14:12:36,827] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04375662863254547
[2024-01-22 14:12:39,695] INFO: Iter 57600 Summary: 
[2024-01-22 14:12:39,695] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04368181087076664
[2024-01-22 14:12:42,313] INFO: Iter 57700 Summary: 
[2024-01-22 14:12:42,313] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04398749429732561
[2024-01-22 14:12:45,100] INFO: Iter 57800 Summary: 
[2024-01-22 14:12:45,100] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370632160454988
[2024-01-22 14:12:47,891] INFO: Iter 57900 Summary: 
[2024-01-22 14:12:47,892] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04387380611151457
[2024-01-22 14:12:50,834] INFO: Iter 58000 Summary: 
[2024-01-22 14:12:50,834] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04380170818418264
[2024-01-22 14:12:53,551] INFO: Iter 58100 Summary: 
[2024-01-22 14:12:53,551] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04377272270619869
[2024-01-22 14:12:56,330] INFO: Iter 58200 Summary: 
[2024-01-22 14:12:56,331] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04411325812339783
[2024-01-22 14:12:59,148] INFO: Iter 58300 Summary: 
[2024-01-22 14:12:59,148] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371258415281773
[2024-01-22 14:13:01,905] INFO: Iter 58400 Summary: 
[2024-01-22 14:13:01,906] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04348294634371996
[2024-01-22 14:13:04,884] INFO: Iter 58500 Summary: 
[2024-01-22 14:13:04,884] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04373836178332567
[2024-01-22 14:13:07,682] INFO: Iter 58600 Summary: 
[2024-01-22 14:13:07,682] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04385723926126957
[2024-01-22 14:13:10,547] INFO: Iter 58700 Summary: 
[2024-01-22 14:13:10,547] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04374010764062405
[2024-01-22 14:13:13,322] INFO: Iter 58800 Summary: 
[2024-01-22 14:13:13,322] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043859375081956385
[2024-01-22 14:13:16,180] INFO: Iter 58900 Summary: 
[2024-01-22 14:13:16,180] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04364267259836197
[2024-01-22 14:13:19,194] INFO: Iter 59000 Summary: 
[2024-01-22 14:13:19,194] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04377552077174187
[2024-01-22 14:13:21,837] INFO: Iter 59100 Summary: 
[2024-01-22 14:13:21,838] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04378167048096657
[2024-01-22 14:13:24,630] INFO: Iter 59200 Summary: 
[2024-01-22 14:13:24,630] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04364916026592255
[2024-01-22 14:13:27,540] INFO: Iter 59300 Summary: 
[2024-01-22 14:13:27,540] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04382210783660412
[2024-01-22 14:13:30,139] INFO: Iter 59400 Summary: 
[2024-01-22 14:13:30,139] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0437055841460824
[2024-01-22 14:13:33,001] INFO: Iter 59500 Summary: 
[2024-01-22 14:13:33,001] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04399091050028801
[2024-01-22 14:13:35,841] INFO: Iter 59600 Summary: 
[2024-01-22 14:13:35,841] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043941318467259405
[2024-01-22 14:13:38,748] INFO: Iter 59700 Summary: 
[2024-01-22 14:13:38,748] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043750988841056826
[2024-01-22 14:13:41,366] INFO: Iter 59800 Summary: 
[2024-01-22 14:13:41,367] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04368003584444523
[2024-01-22 14:13:44,074] INFO: Iter 59900 Summary: 
[2024-01-22 14:13:44,075] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04381321035325527
[2024-01-22 14:13:46,703] INFO: Iter 60000 Summary: 
[2024-01-22 14:13:46,703] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04385171119123697
[2024-01-22 14:13:49,383] INFO: Iter 60100 Summary: 
[2024-01-22 14:13:49,383] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04375530656427145
[2024-01-22 14:13:52,263] INFO: Iter 60200 Summary: 
[2024-01-22 14:13:52,264] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370055615901947
[2024-01-22 14:13:55,268] INFO: Iter 60300 Summary: 
[2024-01-22 14:13:55,268] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043595707528293136
[2024-01-22 14:13:58,005] INFO: Iter 60400 Summary: 
[2024-01-22 14:13:58,005] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043691749684512615
[2024-01-22 14:14:00,707] INFO: Iter 60500 Summary: 
[2024-01-22 14:14:00,707] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04383294709026814
[2024-01-22 14:14:03,205] INFO: Iter 60600 Summary: 
[2024-01-22 14:14:03,205] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04355602696537972
[2024-01-22 14:14:05,990] INFO: Iter 60700 Summary: 
[2024-01-22 14:14:05,991] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361922495067119
[2024-01-22 14:14:08,629] INFO: Iter 60800 Summary: 
[2024-01-22 14:14:08,629] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0435191597789526
[2024-01-22 14:14:11,572] INFO: Iter 60900 Summary: 
[2024-01-22 14:14:11,572] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043622123189270494
[2024-01-22 14:14:14,371] INFO: Iter 61000 Summary: 
[2024-01-22 14:14:14,371] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043447716347873214
[2024-01-22 14:14:17,213] INFO: Iter 61100 Summary: 
[2024-01-22 14:14:17,213] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04379081025719642
[2024-01-22 14:14:20,037] INFO: Iter 61200 Summary: 
[2024-01-22 14:14:20,038] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04401082307100296
[2024-01-22 14:14:22,875] INFO: Iter 61300 Summary: 
[2024-01-22 14:14:22,875] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044025570787489414
[2024-01-22 14:14:25,814] INFO: Iter 61400 Summary: 
[2024-01-22 14:14:25,814] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04358486995100975
[2024-01-22 14:14:28,485] INFO: Iter 61500 Summary: 
[2024-01-22 14:14:28,485] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04415338721126318
[2024-01-22 14:14:31,337] INFO: Iter 61600 Summary: 
[2024-01-22 14:14:31,337] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394786037504673
[2024-01-22 14:14:34,055] INFO: Iter 61700 Summary: 
[2024-01-22 14:14:34,055] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044114644527435305
[2024-01-22 14:14:36,755] INFO: Iter 61800 Summary: 
[2024-01-22 14:14:36,755] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04409801039844751
[2024-01-22 14:14:39,626] INFO: Iter 61900 Summary: 
[2024-01-22 14:14:39,627] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04409663025289774
[2024-01-22 14:14:42,554] INFO: Iter 62000 Summary: 
[2024-01-22 14:14:42,554] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043617171831429005
[2024-01-22 14:14:45,118] INFO: Iter 62100 Summary: 
[2024-01-22 14:14:45,118] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366516623646021
[2024-01-22 14:14:47,907] INFO: Iter 62200 Summary: 
[2024-01-22 14:14:47,907] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043984350934624675
[2024-01-22 14:14:50,748] INFO: Iter 62300 Summary: 
[2024-01-22 14:14:50,748] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04358872193843126
[2024-01-22 14:14:53,320] INFO: Iter 62400 Summary: 
[2024-01-22 14:14:53,320] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04351601250469685
[2024-01-22 14:14:55,964] INFO: Iter 62500 Summary: 
[2024-01-22 14:14:55,965] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04380443222820759
[2024-01-22 14:14:58,884] INFO: Iter 62600 Summary: 
[2024-01-22 14:14:58,884] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04363199103623629
[2024-01-22 14:15:01,650] INFO: Iter 62700 Summary: 
[2024-01-22 14:15:01,651] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043905631192028526
[2024-01-22 14:15:04,474] INFO: Iter 62800 Summary: 
[2024-01-22 14:15:04,474] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370848998427391
[2024-01-22 14:15:07,319] INFO: Iter 62900 Summary: 
[2024-01-22 14:15:07,319] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043720544055104255
[2024-01-22 14:15:10,151] INFO: Iter 63000 Summary: 
[2024-01-22 14:15:10,151] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043667732551693914
[2024-01-22 14:15:12,814] INFO: Iter 63100 Summary: 
[2024-01-22 14:15:12,814] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043694602772593495
[2024-01-22 14:15:15,629] INFO: Iter 63200 Summary: 
[2024-01-22 14:15:15,629] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043427230790257454
[2024-01-22 14:15:18,517] INFO: Iter 63300 Summary: 
[2024-01-22 14:15:18,517] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04354327809065581
[2024-01-22 14:15:21,295] INFO: Iter 63400 Summary: 
[2024-01-22 14:15:21,296] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043560834489762786
[2024-01-22 14:15:24,056] INFO: Iter 63500 Summary: 
[2024-01-22 14:15:24,056] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043714905865490435
[2024-01-22 14:15:26,946] INFO: Iter 63600 Summary: 
[2024-01-22 14:15:26,946] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04396879818290472
[2024-01-22 14:15:29,842] INFO: Iter 63700 Summary: 
[2024-01-22 14:15:29,842] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043819690942764285
[2024-01-22 14:15:32,549] INFO: Iter 63800 Summary: 
[2024-01-22 14:15:32,549] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04375265303999185
[2024-01-22 14:15:35,364] INFO: Iter 63900 Summary: 
[2024-01-22 14:15:35,364] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04367480374872684
[2024-01-22 14:15:38,082] INFO: Iter 64000 Summary: 
[2024-01-22 14:15:38,082] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043987782262265684
[2024-01-22 14:15:40,998] INFO: Iter 64100 Summary: 
[2024-01-22 14:15:40,998] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04379615738987923
[2024-01-22 14:15:43,654] INFO: Iter 64200 Summary: 
[2024-01-22 14:15:43,654] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04375646755099297
[2024-01-22 14:15:46,468] INFO: Iter 64300 Summary: 
[2024-01-22 14:15:46,469] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04396442342549563
[2024-01-22 14:15:49,066] INFO: Iter 64400 Summary: 
[2024-01-22 14:15:49,066] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04386663474142551
[2024-01-22 14:15:51,810] INFO: Iter 64500 Summary: 
[2024-01-22 14:15:51,810] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04382357154041529
[2024-01-22 14:15:54,718] INFO: Iter 64600 Summary: 
[2024-01-22 14:15:54,718] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371554452925921
[2024-01-22 14:15:57,536] INFO: Iter 64700 Summary: 
[2024-01-22 14:15:57,536] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043398458994925025
[2024-01-22 14:16:00,234] INFO: Iter 64800 Summary: 
[2024-01-22 14:16:00,234] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0437769316136837
[2024-01-22 14:16:03,178] INFO: Iter 64900 Summary: 
[2024-01-22 14:16:03,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04388281222432852
[2024-01-22 14:16:06,004] INFO: Iter 65000 Summary: 
[2024-01-22 14:16:06,004] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043721876479685305
[2024-01-22 14:16:08,788] INFO: Iter 65100 Summary: 
[2024-01-22 14:16:08,788] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04404311038553715
[2024-01-22 14:16:11,521] INFO: Iter 65200 Summary: 
[2024-01-22 14:16:11,521] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043577001281082633
[2024-01-22 14:16:14,392] INFO: Iter 65300 Summary: 
[2024-01-22 14:16:14,392] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043643440641462805
[2024-01-22 14:16:17,052] INFO: Iter 65400 Summary: 
[2024-01-22 14:16:17,052] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04368160501122475
[2024-01-22 14:16:19,728] INFO: Iter 65500 Summary: 
[2024-01-22 14:16:19,728] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043605214729905126
[2024-01-22 14:16:22,634] INFO: Iter 65600 Summary: 
[2024-01-22 14:16:22,635] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04344647966325283
[2024-01-22 14:16:25,442] INFO: Iter 65700 Summary: 
[2024-01-22 14:16:25,442] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04348080135881901
[2024-01-22 14:16:28,439] INFO: Iter 65800 Summary: 
[2024-01-22 14:16:28,439] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370475180447102
[2024-01-22 14:16:31,085] INFO: Iter 65900 Summary: 
[2024-01-22 14:16:31,085] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361182805150747
[2024-01-22 14:16:34,017] INFO: Iter 66000 Summary: 
[2024-01-22 14:16:34,018] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04351190950721502
[2024-01-22 14:16:36,912] INFO: Iter 66100 Summary: 
[2024-01-22 14:16:36,912] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04343254514038563
[2024-01-22 14:16:39,594] INFO: Iter 66200 Summary: 
[2024-01-22 14:16:39,594] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04343311220407486
[2024-01-22 14:16:42,293] INFO: Iter 66300 Summary: 
[2024-01-22 14:16:42,293] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371261548250913
[2024-01-22 14:16:45,121] INFO: Iter 66400 Summary: 
[2024-01-22 14:16:45,121] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04364630475640297
[2024-01-22 14:16:47,852] INFO: Iter 66500 Summary: 
[2024-01-22 14:16:47,852] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043899623900651934
[2024-01-22 14:16:50,674] INFO: Iter 66600 Summary: 
[2024-01-22 14:16:50,674] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04347966499626636
[2024-01-22 14:16:53,477] INFO: Iter 66700 Summary: 
[2024-01-22 14:16:53,477] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04365246102213859
[2024-01-22 14:16:56,147] INFO: Iter 66800 Summary: 
[2024-01-22 14:16:56,147] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04312603462487459
[2024-01-22 14:16:58,694] INFO: Iter 66900 Summary: 
[2024-01-22 14:16:58,694] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043658733852207664
[2024-01-22 14:17:01,374] INFO: Iter 67000 Summary: 
[2024-01-22 14:17:01,374] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04380481068044901
[2024-01-22 14:17:04,183] INFO: Iter 67100 Summary: 
[2024-01-22 14:17:04,183] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361628867685795
[2024-01-22 14:17:06,877] INFO: Iter 67200 Summary: 
[2024-01-22 14:17:06,877] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04363527249544859
[2024-01-22 14:17:09,851] INFO: Iter 67300 Summary: 
[2024-01-22 14:17:09,851] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04376976814121008
[2024-01-22 14:17:12,814] INFO: Iter 67400 Summary: 
[2024-01-22 14:17:12,814] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394607570022344
[2024-01-22 14:17:15,568] INFO: Iter 67500 Summary: 
[2024-01-22 14:17:15,569] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043786444403231144
[2024-01-22 14:17:18,356] INFO: Iter 67600 Summary: 
[2024-01-22 14:17:18,356] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043412695527076724
[2024-01-22 14:17:21,115] INFO: Iter 67700 Summary: 
[2024-01-22 14:17:21,115] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043951212540268896
[2024-01-22 14:17:23,989] INFO: Iter 67800 Summary: 
[2024-01-22 14:17:23,989] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043383635394275186
[2024-01-22 14:17:26,648] INFO: Iter 67900 Summary: 
[2024-01-22 14:17:26,648] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346650216728449
[2024-01-22 14:17:29,596] INFO: Iter 68000 Summary: 
[2024-01-22 14:17:29,596] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04368628416210413
[2024-01-22 14:17:32,541] INFO: Iter 68100 Summary: 
[2024-01-22 14:17:32,541] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359770447015762
[2024-01-22 14:17:35,243] INFO: Iter 68200 Summary: 
[2024-01-22 14:17:35,243] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043399958200752736
[2024-01-22 14:17:38,089] INFO: Iter 68300 Summary: 
[2024-01-22 14:17:38,089] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361589442938566
[2024-01-22 14:17:40,807] INFO: Iter 68400 Summary: 
[2024-01-22 14:17:40,807] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04380265589803457
[2024-01-22 14:17:43,619] INFO: Iter 68500 Summary: 
[2024-01-22 14:17:43,619] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04387708313763142
[2024-01-22 14:17:46,314] INFO: Iter 68600 Summary: 
[2024-01-22 14:17:46,314] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04364868454635143
[2024-01-22 14:17:49,092] INFO: Iter 68700 Summary: 
[2024-01-22 14:17:49,092] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04354778684675693
[2024-01-22 14:17:51,694] INFO: Iter 68800 Summary: 
[2024-01-22 14:17:51,694] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043215221576392654
[2024-01-22 14:17:54,514] INFO: Iter 68900 Summary: 
[2024-01-22 14:17:54,514] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043621012680232524
[2024-01-22 14:17:57,243] INFO: Iter 69000 Summary: 
[2024-01-22 14:17:57,243] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04373412281274795
[2024-01-22 14:18:00,058] INFO: Iter 69100 Summary: 
[2024-01-22 14:18:00,058] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043670022115111354
[2024-01-22 14:18:02,679] INFO: Iter 69200 Summary: 
[2024-01-22 14:18:02,680] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04365690845996142
[2024-01-22 14:18:05,518] INFO: Iter 69300 Summary: 
[2024-01-22 14:18:05,518] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371593862771988
[2024-01-22 14:18:08,532] INFO: Iter 69400 Summary: 
[2024-01-22 14:18:08,533] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04380413852632046
[2024-01-22 14:18:11,407] INFO: Iter 69500 Summary: 
[2024-01-22 14:18:11,408] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04364110391587019
[2024-01-22 14:18:14,194] INFO: Iter 69600 Summary: 
[2024-01-22 14:18:14,194] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04341613933444023
[2024-01-22 14:18:16,898] INFO: Iter 69700 Summary: 
[2024-01-22 14:18:16,898] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0433100188523531
[2024-01-22 14:18:19,731] INFO: Iter 69800 Summary: 
[2024-01-22 14:18:19,731] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04348074831068516
[2024-01-22 14:18:22,429] INFO: Iter 69900 Summary: 
[2024-01-22 14:18:22,429] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04344931356608868
[2024-01-22 14:18:25,268] INFO: Iter 70000 Summary: 
[2024-01-22 14:18:25,268] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04367717456072569
[2024-01-22 14:18:28,185] INFO: Iter 70100 Summary: 
[2024-01-22 14:18:28,185] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043447107449173926
[2024-01-22 14:18:31,022] INFO: Iter 70200 Summary: 
[2024-01-22 14:18:31,022] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370456352829933
[2024-01-22 14:18:33,924] INFO: Iter 70300 Summary: 
[2024-01-22 14:18:33,924] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043748412020504474
[2024-01-22 14:18:36,687] INFO: Iter 70400 Summary: 
[2024-01-22 14:18:36,687] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043638220503926274
[2024-01-22 14:18:39,537] INFO: Iter 70500 Summary: 
[2024-01-22 14:18:39,537] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04352983679622412
[2024-01-22 14:18:42,155] INFO: Iter 70600 Summary: 
[2024-01-22 14:18:42,155] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043519110195338724
[2024-01-22 14:18:45,016] INFO: Iter 70700 Summary: 
[2024-01-22 14:18:45,016] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043390785567462446
[2024-01-22 14:18:47,615] INFO: Iter 70800 Summary: 
[2024-01-22 14:18:47,616] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043764624148607253
[2024-01-22 14:18:50,207] INFO: Iter 70900 Summary: 
[2024-01-22 14:18:50,207] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04384419783949852
[2024-01-22 14:18:52,901] INFO: Iter 71000 Summary: 
[2024-01-22 14:18:52,901] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336127769201994
[2024-01-22 14:18:55,769] INFO: Iter 71100 Summary: 
[2024-01-22 14:18:55,769] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043380195684731004
[2024-01-22 14:18:58,529] INFO: Iter 71200 Summary: 
[2024-01-22 14:18:58,529] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043354481346905234
[2024-01-22 14:19:01,212] INFO: Iter 71300 Summary: 
[2024-01-22 14:19:01,212] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04331942442804575
[2024-01-22 14:19:03,925] INFO: Iter 71400 Summary: 
[2024-01-22 14:19:03,925] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04411253299564123
[2024-01-22 14:19:06,830] INFO: Iter 71500 Summary: 
[2024-01-22 14:19:06,830] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043102583810687066
[2024-01-22 14:19:09,636] INFO: Iter 71600 Summary: 
[2024-01-22 14:19:09,636] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043516275696456434
[2024-01-22 14:19:12,478] INFO: Iter 71700 Summary: 
[2024-01-22 14:19:12,478] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366301909089088
[2024-01-22 14:19:15,383] INFO: Iter 71800 Summary: 
[2024-01-22 14:19:15,383] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04356555610895157
[2024-01-22 14:19:18,034] INFO: Iter 71900 Summary: 
[2024-01-22 14:19:18,034] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043274553604424
[2024-01-22 14:19:20,988] INFO: Iter 72000 Summary: 
[2024-01-22 14:19:20,988] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370859183371067
[2024-01-22 14:19:23,832] INFO: Iter 72100 Summary: 
[2024-01-22 14:19:23,832] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043365505747497084
[2024-01-22 14:19:26,604] INFO: Iter 72200 Summary: 
[2024-01-22 14:19:26,604] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04357731696218252
[2024-01-22 14:19:29,220] INFO: Iter 72300 Summary: 
[2024-01-22 14:19:29,220] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043076813891530036
[2024-01-22 14:19:31,986] INFO: Iter 72400 Summary: 
[2024-01-22 14:19:31,986] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0435798678919673
[2024-01-22 14:19:34,833] INFO: Iter 72500 Summary: 
[2024-01-22 14:19:34,833] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04362543698400259
[2024-01-22 14:19:37,474] INFO: Iter 72600 Summary: 
[2024-01-22 14:19:37,474] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04360463455319405
[2024-01-22 14:19:40,144] INFO: Iter 72700 Summary: 
[2024-01-22 14:19:40,145] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04334386602044105
[2024-01-22 14:19:42,981] INFO: Iter 72800 Summary: 
[2024-01-22 14:19:42,982] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043545426055789
[2024-01-22 14:19:45,662] INFO: Iter 72900 Summary: 
[2024-01-22 14:19:45,662] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04384866401553154
[2024-01-22 14:19:48,433] INFO: Iter 73000 Summary: 
[2024-01-22 14:19:48,433] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043593539893627166
[2024-01-22 14:19:51,239] INFO: Iter 73100 Summary: 
[2024-01-22 14:19:51,239] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044029801413416864
[2024-01-22 14:19:54,007] INFO: Iter 73200 Summary: 
[2024-01-22 14:19:54,007] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04347579654306173
[2024-01-22 14:19:56,662] INFO: Iter 73300 Summary: 
[2024-01-22 14:19:56,663] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04331395424902439
[2024-01-22 14:19:59,457] INFO: Iter 73400 Summary: 
[2024-01-22 14:19:59,458] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04363138046115637
[2024-01-22 14:20:02,377] INFO: Iter 73500 Summary: 
[2024-01-22 14:20:02,378] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043759246319532395
[2024-01-22 14:20:04,959] INFO: Iter 73600 Summary: 
[2024-01-22 14:20:04,959] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361896209418774
[2024-01-22 14:20:07,803] INFO: Iter 73700 Summary: 
[2024-01-22 14:20:07,803] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370198350399732
[2024-01-22 14:20:10,517] INFO: Iter 73800 Summary: 
[2024-01-22 14:20:10,517] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043683761060237886
[2024-01-22 14:20:13,332] INFO: Iter 73900 Summary: 
[2024-01-22 14:20:13,332] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340778846293688
[2024-01-22 14:20:15,940] INFO: Iter 74000 Summary: 
[2024-01-22 14:20:15,941] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04342455442994833
[2024-01-22 14:20:18,608] INFO: Iter 74100 Summary: 
[2024-01-22 14:20:18,608] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04356412872672081
[2024-01-22 14:20:21,347] INFO: Iter 74200 Summary: 
[2024-01-22 14:20:21,347] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043407970406115055
[2024-01-22 14:20:24,113] INFO: Iter 74300 Summary: 
[2024-01-22 14:20:24,113] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04325553707778454
[2024-01-22 14:20:26,882] INFO: Iter 74400 Summary: 
[2024-01-22 14:20:26,882] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043231780789792534
[2024-01-22 14:20:29,561] INFO: Iter 74500 Summary: 
[2024-01-22 14:20:29,561] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043571616671979424
[2024-01-22 14:20:32,349] INFO: Iter 74600 Summary: 
[2024-01-22 14:20:32,349] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043582400493323806
[2024-01-22 14:20:35,134] INFO: Iter 74700 Summary: 
[2024-01-22 14:20:35,134] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043406876362860204
[2024-01-22 14:20:38,066] INFO: Iter 74800 Summary: 
[2024-01-22 14:20:38,066] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04343889970332384
[2024-01-22 14:20:40,831] INFO: Iter 74900 Summary: 
[2024-01-22 14:20:40,832] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346246913075447
[2024-01-22 14:20:43,523] INFO: Iter 75000 Summary: 
[2024-01-22 14:20:43,523] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043454589433968065
[2024-01-22 14:20:46,386] INFO: Iter 75100 Summary: 
[2024-01-22 14:20:46,386] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04341808445751667
[2024-01-22 14:20:49,352] INFO: Iter 75200 Summary: 
[2024-01-22 14:20:49,352] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043433256410062314
[2024-01-22 14:20:52,079] INFO: Iter 75300 Summary: 
[2024-01-22 14:20:52,079] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04317480191588402
[2024-01-22 14:20:54,754] INFO: Iter 75400 Summary: 
[2024-01-22 14:20:54,754] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0433956141024828
[2024-01-22 14:20:57,496] INFO: Iter 75500 Summary: 
[2024-01-22 14:20:57,496] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043206429891288284
[2024-01-22 14:21:00,202] INFO: Iter 75600 Summary: 
[2024-01-22 14:21:00,202] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04332566242665052
[2024-01-22 14:21:03,132] INFO: Iter 75700 Summary: 
[2024-01-22 14:21:03,132] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336880922317505
[2024-01-22 14:21:05,749] INFO: Iter 75800 Summary: 
[2024-01-22 14:21:05,750] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043753867149353025
[2024-01-22 14:21:08,503] INFO: Iter 75900 Summary: 
[2024-01-22 14:21:08,504] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04331870224326849
[2024-01-22 14:21:11,166] INFO: Iter 76000 Summary: 
[2024-01-22 14:21:11,166] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04360065810382366
[2024-01-22 14:21:13,949] INFO: Iter 76100 Summary: 
[2024-01-22 14:21:13,949] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0436843416467309
[2024-01-22 14:21:16,674] INFO: Iter 76200 Summary: 
[2024-01-22 14:21:16,675] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043331697136163715
[2024-01-22 14:21:19,240] INFO: Iter 76300 Summary: 
[2024-01-22 14:21:19,240] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04339187171310186
[2024-01-22 14:21:21,978] INFO: Iter 76400 Summary: 
[2024-01-22 14:21:21,978] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043386095054447654
[2024-01-22 14:21:24,680] INFO: Iter 76500 Summary: 
[2024-01-22 14:21:24,680] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043923934251070024
[2024-01-22 14:21:27,561] INFO: Iter 76600 Summary: 
[2024-01-22 14:21:27,561] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04334192484617233
[2024-01-22 14:21:30,144] INFO: Iter 76700 Summary: 
[2024-01-22 14:21:30,144] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043285615779459476
[2024-01-22 14:21:33,035] INFO: Iter 76800 Summary: 
[2024-01-22 14:21:33,036] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04320446625351906
[2024-01-22 14:21:35,855] INFO: Iter 76900 Summary: 
[2024-01-22 14:21:35,855] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359168231487274
[2024-01-22 14:21:38,542] INFO: Iter 77000 Summary: 
[2024-01-22 14:21:38,543] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04368268482387066
[2024-01-22 14:21:41,320] INFO: Iter 77100 Summary: 
[2024-01-22 14:21:41,320] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043179885372519496
[2024-01-22 14:21:44,190] INFO: Iter 77200 Summary: 
[2024-01-22 14:21:44,190] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316841594874859
[2024-01-22 14:21:46,972] INFO: Iter 77300 Summary: 
[2024-01-22 14:21:46,973] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043829651959240434
[2024-01-22 14:21:49,859] INFO: Iter 77400 Summary: 
[2024-01-22 14:21:49,859] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043589731566607955
[2024-01-22 14:21:52,471] INFO: Iter 77500 Summary: 
[2024-01-22 14:21:52,471] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043267777301371095
[2024-01-22 14:21:55,326] INFO: Iter 77600 Summary: 
[2024-01-22 14:21:55,326] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04314902648329735
[2024-01-22 14:21:58,096] INFO: Iter 77700 Summary: 
[2024-01-22 14:21:58,096] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04341268621385098
[2024-01-22 14:22:00,962] INFO: Iter 77800 Summary: 
[2024-01-22 14:22:00,962] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336006101220846
[2024-01-22 14:22:03,815] INFO: Iter 77900 Summary: 
[2024-01-22 14:22:03,815] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043416620939970015
[2024-01-22 14:22:06,547] INFO: Iter 78000 Summary: 
[2024-01-22 14:22:06,547] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04356469128280878
[2024-01-22 14:22:09,461] INFO: Iter 78100 Summary: 
[2024-01-22 14:22:09,461] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04334167331457138
[2024-01-22 14:22:12,277] INFO: Iter 78200 Summary: 
[2024-01-22 14:22:12,278] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043869867622852325
[2024-01-22 14:22:14,938] INFO: Iter 78300 Summary: 
[2024-01-22 14:22:14,938] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043132298700511454
[2024-01-22 14:22:17,851] INFO: Iter 78400 Summary: 
[2024-01-22 14:22:17,852] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04328206080943346
[2024-01-22 14:22:20,769] INFO: Iter 78500 Summary: 
[2024-01-22 14:22:20,769] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04308443292975426
[2024-01-22 14:22:23,515] INFO: Iter 78600 Summary: 
[2024-01-22 14:22:23,516] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04350932277739048
[2024-01-22 14:22:25,949] INFO: Iter 78700 Summary: 
[2024-01-22 14:22:25,949] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04337265681475401
[2024-01-22 14:22:28,787] INFO: Iter 78800 Summary: 
[2024-01-22 14:22:28,787] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043608869649469854
[2024-01-22 14:22:31,612] INFO: Iter 78900 Summary: 
[2024-01-22 14:22:31,612] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336432360112667
[2024-01-22 14:22:34,363] INFO: Iter 79000 Summary: 
[2024-01-22 14:22:34,363] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340858463197947
[2024-01-22 14:22:37,179] INFO: Iter 79100 Summary: 
[2024-01-22 14:22:37,179] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043272333964705464
[2024-01-22 14:22:39,968] INFO: Iter 79200 Summary: 
[2024-01-22 14:22:39,968] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04314869467169046
[2024-01-22 14:22:42,730] INFO: Iter 79300 Summary: 
[2024-01-22 14:22:42,730] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04319385685026646
[2024-01-22 14:22:45,327] INFO: Iter 79400 Summary: 
[2024-01-22 14:22:45,328] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04319484133273363
[2024-01-22 14:22:48,109] INFO: Iter 79500 Summary: 
[2024-01-22 14:22:48,109] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04307370446622372
[2024-01-22 14:22:50,706] INFO: Iter 79600 Summary: 
[2024-01-22 14:22:50,706] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043292499519884585
[2024-01-22 14:22:53,553] INFO: Iter 79700 Summary: 
[2024-01-22 14:22:53,553] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04326844125986099
[2024-01-22 14:22:56,434] INFO: Iter 79800 Summary: 
[2024-01-22 14:22:56,434] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043290341086685655
[2024-01-22 14:22:59,245] INFO: Iter 79900 Summary: 
[2024-01-22 14:22:59,245] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04335882350802422
[2024-01-22 14:23:01,917] INFO: Iter 80000 Summary: 
[2024-01-22 14:23:01,917] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04321763169020414
[2024-01-22 14:23:04,785] INFO: Iter 80100 Summary: 
[2024-01-22 14:23:04,786] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336818616837263
[2024-01-22 14:23:07,428] INFO: Iter 80200 Summary: 
[2024-01-22 14:23:07,428] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366318121552468
[2024-01-22 14:23:10,310] INFO: Iter 80300 Summary: 
[2024-01-22 14:23:10,310] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043182923570275304
[2024-01-22 14:23:12,907] INFO: Iter 80400 Summary: 
[2024-01-22 14:23:12,907] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043330780938267706
[2024-01-22 14:23:15,665] INFO: Iter 80500 Summary: 
[2024-01-22 14:23:15,666] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04291883382946253
[2024-01-22 14:23:18,470] INFO: Iter 80600 Summary: 
[2024-01-22 14:23:18,471] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0433831449970603
[2024-01-22 14:23:21,175] INFO: Iter 80700 Summary: 
[2024-01-22 14:23:21,175] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346240133047104
[2024-01-22 14:23:24,126] INFO: Iter 80800 Summary: 
[2024-01-22 14:23:24,126] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04327773042023182
[2024-01-22 14:23:26,851] INFO: Iter 80900 Summary: 
[2024-01-22 14:23:26,851] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043577533811330796
[2024-01-22 14:23:29,657] INFO: Iter 81000 Summary: 
[2024-01-22 14:23:29,657] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043466709814965726
[2024-01-22 14:23:32,466] INFO: Iter 81100 Summary: 
[2024-01-22 14:23:32,466] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04329991158097982
[2024-01-22 14:23:35,327] INFO: Iter 81200 Summary: 
[2024-01-22 14:23:35,327] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361103795468807
[2024-01-22 14:23:38,068] INFO: Iter 81300 Summary: 
[2024-01-22 14:23:38,068] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361592911183834
[2024-01-22 14:23:40,882] INFO: Iter 81400 Summary: 
[2024-01-22 14:23:40,882] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04382609471678734
[2024-01-22 14:23:43,774] INFO: Iter 81500 Summary: 
[2024-01-22 14:23:43,775] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0433304875716567
[2024-01-22 14:23:46,566] INFO: Iter 81600 Summary: 
[2024-01-22 14:23:46,566] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346253450959921
[2024-01-22 14:23:49,113] INFO: Iter 81700 Summary: 
[2024-01-22 14:23:49,114] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043247314505279064
[2024-01-22 14:23:51,885] INFO: Iter 81800 Summary: 
[2024-01-22 14:23:51,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340956002473831
[2024-01-22 14:23:54,709] INFO: Iter 81900 Summary: 
[2024-01-22 14:23:54,709] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043584625646471975
[2024-01-22 14:23:57,581] INFO: Iter 82000 Summary: 
[2024-01-22 14:23:57,582] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04321331292390823
[2024-01-22 14:24:00,119] INFO: Iter 82100 Summary: 
[2024-01-22 14:24:00,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043421723730862144
[2024-01-22 14:24:02,958] INFO: Iter 82200 Summary: 
[2024-01-22 14:24:02,959] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336035784333944
[2024-01-22 14:24:05,626] INFO: Iter 82300 Summary: 
[2024-01-22 14:24:05,626] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04378952443599701
[2024-01-22 14:24:08,262] INFO: Iter 82400 Summary: 
[2024-01-22 14:24:08,262] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359675969928503
[2024-01-22 14:24:11,000] INFO: Iter 82500 Summary: 
[2024-01-22 14:24:11,000] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04343111082911491
[2024-01-22 14:24:13,784] INFO: Iter 82600 Summary: 
[2024-01-22 14:24:13,784] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04363118156790733
[2024-01-22 14:24:16,534] INFO: Iter 82700 Summary: 
[2024-01-22 14:24:16,534] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043502520695328714
[2024-01-22 14:24:19,337] INFO: Iter 82800 Summary: 
[2024-01-22 14:24:19,337] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04344806883484125
[2024-01-22 14:24:22,005] INFO: Iter 82900 Summary: 
[2024-01-22 14:24:22,005] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04353118948638439
[2024-01-22 14:24:24,837] INFO: Iter 83000 Summary: 
[2024-01-22 14:24:24,837] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04300241023302078
[2024-01-22 14:24:27,596] INFO: Iter 83100 Summary: 
[2024-01-22 14:24:27,597] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04321115303784609
[2024-01-22 14:24:30,534] INFO: Iter 83200 Summary: 
[2024-01-22 14:24:30,534] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04307061117142439
[2024-01-22 14:24:33,427] INFO: Iter 83300 Summary: 
[2024-01-22 14:24:33,427] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04369625736027956
[2024-01-22 14:24:36,160] INFO: Iter 83400 Summary: 
[2024-01-22 14:24:36,160] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346456132829189
[2024-01-22 14:24:38,944] INFO: Iter 83500 Summary: 
[2024-01-22 14:24:38,944] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043611500225961206
[2024-01-22 14:24:41,694] INFO: Iter 83600 Summary: 
[2024-01-22 14:24:41,695] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043268460929393765
[2024-01-22 14:24:44,384] INFO: Iter 83700 Summary: 
[2024-01-22 14:24:44,384] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04314376626163721
[2024-01-22 14:24:47,246] INFO: Iter 83800 Summary: 
[2024-01-22 14:24:47,246] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043334945365786554
[2024-01-22 14:24:49,958] INFO: Iter 83900 Summary: 
[2024-01-22 14:24:49,958] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043396958932280544
[2024-01-22 14:24:52,719] INFO: Iter 84000 Summary: 
[2024-01-22 14:24:52,720] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043410730995237824
[2024-01-22 14:24:55,437] INFO: Iter 84100 Summary: 
[2024-01-22 14:24:55,437] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04308335911482573
[2024-01-22 14:24:58,276] INFO: Iter 84200 Summary: 
[2024-01-22 14:24:58,276] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043279886357486246
[2024-01-22 14:25:01,208] INFO: Iter 84300 Summary: 
[2024-01-22 14:25:01,208] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336208123713732
[2024-01-22 14:25:03,982] INFO: Iter 84400 Summary: 
[2024-01-22 14:25:03,982] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04335480105131864
[2024-01-22 14:25:06,837] INFO: Iter 84500 Summary: 
[2024-01-22 14:25:06,837] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336327295750379
[2024-01-22 14:25:09,576] INFO: Iter 84600 Summary: 
[2024-01-22 14:25:09,576] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04323085006326437
[2024-01-22 14:25:12,495] INFO: Iter 84700 Summary: 
[2024-01-22 14:25:12,496] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043432129584252833
[2024-01-22 14:25:15,287] INFO: Iter 84800 Summary: 
[2024-01-22 14:25:15,287] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04331092324107885
[2024-01-22 14:25:18,222] INFO: Iter 84900 Summary: 
[2024-01-22 14:25:18,222] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359710153192282
[2024-01-22 14:25:20,937] INFO: Iter 85000 Summary: 
[2024-01-22 14:25:20,937] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04311074290424585
[2024-01-22 14:25:23,742] INFO: Iter 85100 Summary: 
[2024-01-22 14:25:23,742] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04309409651905298
[2024-01-22 14:25:26,541] INFO: Iter 85200 Summary: 
[2024-01-22 14:25:26,541] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04344778507947922
[2024-01-22 14:25:29,200] INFO: Iter 85300 Summary: 
[2024-01-22 14:25:29,200] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04321796335279941
[2024-01-22 14:25:31,993] INFO: Iter 85400 Summary: 
[2024-01-22 14:25:31,993] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04332456834614277
[2024-01-22 14:25:34,821] INFO: Iter 85500 Summary: 
[2024-01-22 14:25:34,821] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043394703269004825
[2024-01-22 14:25:37,699] INFO: Iter 85600 Summary: 
[2024-01-22 14:25:37,699] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04337221868336201
[2024-01-22 14:25:40,604] INFO: Iter 85700 Summary: 
[2024-01-22 14:25:40,605] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043329250849783424
[2024-01-22 14:25:43,275] INFO: Iter 85800 Summary: 
[2024-01-22 14:25:43,275] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043551964536309246
[2024-01-22 14:25:46,068] INFO: Iter 85900 Summary: 
[2024-01-22 14:25:46,068] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043125650472939016
[2024-01-22 14:25:48,952] INFO: Iter 86000 Summary: 
[2024-01-22 14:25:48,953] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043256264068186286
[2024-01-22 14:25:51,763] INFO: Iter 86100 Summary: 
[2024-01-22 14:25:51,764] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04332536615431309
[2024-01-22 14:25:54,485] INFO: Iter 86200 Summary: 
[2024-01-22 14:25:54,485] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04330685470253229
[2024-01-22 14:25:57,396] INFO: Iter 86300 Summary: 
[2024-01-22 14:25:57,396] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0432522527128458
[2024-01-22 14:26:00,104] INFO: Iter 86400 Summary: 
[2024-01-22 14:26:00,104] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043061635233461855
[2024-01-22 14:26:02,846] INFO: Iter 86500 Summary: 
[2024-01-22 14:26:02,846] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04334894426167011
[2024-01-22 14:26:05,635] INFO: Iter 86600 Summary: 
[2024-01-22 14:26:05,636] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336918257176876
[2024-01-22 14:26:08,296] INFO: Iter 86700 Summary: 
[2024-01-22 14:26:08,296] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04325488328933716
[2024-01-22 14:26:10,917] INFO: Iter 86800 Summary: 
[2024-01-22 14:26:10,917] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043215543031692505
[2024-01-22 14:26:13,564] INFO: Iter 86900 Summary: 
[2024-01-22 14:26:13,565] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04376253742724657
[2024-01-22 14:26:16,416] INFO: Iter 87000 Summary: 
[2024-01-22 14:26:16,417] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340815905481577
[2024-01-22 14:26:19,207] INFO: Iter 87100 Summary: 
[2024-01-22 14:26:19,207] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04378621894866228
[2024-01-22 14:26:22,141] INFO: Iter 87200 Summary: 
[2024-01-22 14:26:22,141] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043330206461250784
[2024-01-22 14:26:24,902] INFO: Iter 87300 Summary: 
[2024-01-22 14:26:24,902] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04296167518943548
[2024-01-22 14:26:27,656] INFO: Iter 87400 Summary: 
[2024-01-22 14:26:27,656] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04339113313704729
[2024-01-22 14:26:30,451] INFO: Iter 87500 Summary: 
[2024-01-22 14:26:30,451] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043137014508247376
[2024-01-22 14:26:33,203] INFO: Iter 87600 Summary: 
[2024-01-22 14:26:33,204] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04314780507236719
[2024-01-22 14:26:35,909] INFO: Iter 87700 Summary: 
[2024-01-22 14:26:35,909] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04362394109368324
[2024-01-22 14:26:38,748] INFO: Iter 87800 Summary: 
[2024-01-22 14:26:38,749] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04333660084754229
[2024-01-22 14:26:41,666] INFO: Iter 87900 Summary: 
[2024-01-22 14:26:41,666] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04349095296114683
[2024-01-22 14:26:44,633] INFO: Iter 88000 Summary: 
[2024-01-22 14:26:44,633] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043367360867559906
[2024-01-22 14:26:47,223] INFO: Iter 88100 Summary: 
[2024-01-22 14:26:47,224] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04326644372195006
[2024-01-22 14:26:50,111] INFO: Iter 88200 Summary: 
[2024-01-22 14:26:50,111] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0432961667701602
[2024-01-22 14:26:52,904] INFO: Iter 88300 Summary: 
[2024-01-22 14:26:52,904] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04321753460913896
[2024-01-22 14:26:55,679] INFO: Iter 88400 Summary: 
[2024-01-22 14:26:55,679] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0433412329480052
[2024-01-22 14:26:58,509] INFO: Iter 88500 Summary: 
[2024-01-22 14:26:58,509] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04358437485992908
[2024-01-22 14:27:01,266] INFO: Iter 88600 Summary: 
[2024-01-22 14:27:01,266] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04312219858169555
[2024-01-22 14:27:04,168] INFO: Iter 88700 Summary: 
[2024-01-22 14:27:04,168] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04337312202900648
[2024-01-22 14:27:06,900] INFO: Iter 88800 Summary: 
[2024-01-22 14:27:06,901] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043321581967175005
[2024-01-22 14:27:09,761] INFO: Iter 88900 Summary: 
[2024-01-22 14:27:09,761] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0432736811041832
[2024-01-22 14:27:12,618] INFO: Iter 89000 Summary: 
[2024-01-22 14:27:12,618] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04325113881379366
[2024-01-22 14:27:15,233] INFO: Iter 89100 Summary: 
[2024-01-22 14:27:15,234] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04319146778434515
[2024-01-22 14:27:18,057] INFO: Iter 89200 Summary: 
[2024-01-22 14:27:18,057] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043550431281328204
[2024-01-22 14:27:21,095] INFO: Iter 89300 Summary: 
[2024-01-22 14:27:21,095] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043043594509363174
[2024-01-22 14:27:24,061] INFO: Iter 89400 Summary: 
[2024-01-22 14:27:24,061] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04319339543581009
[2024-01-22 14:27:26,881] INFO: Iter 89500 Summary: 
[2024-01-22 14:27:26,882] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043717965222895144
[2024-01-22 14:27:29,631] INFO: Iter 89600 Summary: 
[2024-01-22 14:27:29,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04363544669002294
[2024-01-22 14:27:32,511] INFO: Iter 89700 Summary: 
[2024-01-22 14:27:32,511] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04326269455254078
[2024-01-22 14:27:35,327] INFO: Iter 89800 Summary: 
[2024-01-22 14:27:35,327] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04333375226706267
[2024-01-22 14:27:38,218] INFO: Iter 89900 Summary: 
[2024-01-22 14:27:38,218] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04318089429289103
[2024-01-22 14:27:41,135] INFO: Iter 90000 Summary: 
[2024-01-22 14:27:41,135] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0430977687984705
[2024-01-22 14:27:44,023] INFO: Iter 90100 Summary: 
[2024-01-22 14:27:44,023] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04315621569752693
[2024-01-22 14:27:46,759] INFO: Iter 90200 Summary: 
[2024-01-22 14:27:46,759] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04327131979167462
[2024-01-22 14:27:49,517] INFO: Iter 90300 Summary: 
[2024-01-22 14:27:49,517] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043549295477569104
[2024-01-22 14:27:52,341] INFO: Iter 90400 Summary: 
[2024-01-22 14:27:52,342] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04348042745143175
[2024-01-22 14:27:55,176] INFO: Iter 90500 Summary: 
[2024-01-22 14:27:55,176] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043285665884613994
[2024-01-22 14:27:58,114] INFO: Iter 90600 Summary: 
[2024-01-22 14:27:58,114] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04319872934371233
[2024-01-22 14:28:00,951] INFO: Iter 90700 Summary: 
[2024-01-22 14:28:00,951] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043160635605454446
[2024-01-22 14:28:03,645] INFO: Iter 90800 Summary: 
[2024-01-22 14:28:03,645] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04326745569705963
[2024-01-22 14:28:06,570] INFO: Iter 90900 Summary: 
[2024-01-22 14:28:06,571] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043591180220246314
[2024-01-22 14:28:09,476] INFO: Iter 91000 Summary: 
[2024-01-22 14:28:09,476] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04306953325867653
[2024-01-22 14:28:12,375] INFO: Iter 91100 Summary: 
[2024-01-22 14:28:12,375] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04343901157379151
[2024-01-22 14:28:15,129] INFO: Iter 91200 Summary: 
[2024-01-22 14:28:15,129] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04301239006221294
[2024-01-22 14:28:17,906] INFO: Iter 91300 Summary: 
[2024-01-22 14:28:17,907] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0429293704777956
[2024-01-22 14:28:20,697] INFO: Iter 91400 Summary: 
[2024-01-22 14:28:20,697] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043420150838792325
[2024-01-22 14:28:23,353] INFO: Iter 91500 Summary: 
[2024-01-22 14:28:23,353] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04305223055183888
[2024-01-22 14:28:26,032] INFO: Iter 91600 Summary: 
[2024-01-22 14:28:26,033] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04351179011166096
[2024-01-22 14:28:28,660] INFO: Iter 91700 Summary: 
[2024-01-22 14:28:28,660] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04319765381515026
[2024-01-22 14:28:31,263] INFO: Iter 91800 Summary: 
[2024-01-22 14:28:31,263] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0432280383259058
[2024-01-22 14:28:34,022] INFO: Iter 91900 Summary: 
[2024-01-22 14:28:34,022] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04321014229208231
[2024-01-22 14:28:36,879] INFO: Iter 92000 Summary: 
[2024-01-22 14:28:36,879] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043162052296102045
[2024-01-22 14:28:39,633] INFO: Iter 92100 Summary: 
[2024-01-22 14:28:39,633] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04330710358917713
[2024-01-22 14:28:42,348] INFO: Iter 92200 Summary: 
[2024-01-22 14:28:42,349] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04322153557091951
[2024-01-22 14:28:45,213] INFO: Iter 92300 Summary: 
[2024-01-22 14:28:45,213] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04355635028332472
[2024-01-22 14:28:48,098] INFO: Iter 92400 Summary: 
[2024-01-22 14:28:48,098] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043353035151958465
[2024-01-22 14:28:50,937] INFO: Iter 92500 Summary: 
[2024-01-22 14:28:50,937] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043399010226130484
[2024-01-22 14:28:53,759] INFO: Iter 92600 Summary: 
[2024-01-22 14:28:53,759] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0430699859559536
[2024-01-22 14:28:56,693] INFO: Iter 92700 Summary: 
[2024-01-22 14:28:56,693] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043168357871472836
[2024-01-22 14:28:59,549] INFO: Iter 92800 Summary: 
[2024-01-22 14:28:59,549] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043399621285498145
[2024-01-22 14:29:02,234] INFO: Iter 92900 Summary: 
[2024-01-22 14:29:02,234] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043470893204212185
[2024-01-22 14:29:04,910] INFO: Iter 93000 Summary: 
[2024-01-22 14:29:04,911] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0431088575348258
[2024-01-22 14:29:07,721] INFO: Iter 93100 Summary: 
[2024-01-22 14:29:07,721] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04330090206116438
[2024-01-22 14:29:10,470] INFO: Iter 93200 Summary: 
[2024-01-22 14:29:10,470] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316158752888441
[2024-01-22 14:29:13,307] INFO: Iter 93300 Summary: 
[2024-01-22 14:29:13,307] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04324068795889616
[2024-01-22 14:29:16,088] INFO: Iter 93400 Summary: 
[2024-01-22 14:29:16,088] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043122538924217226
[2024-01-22 14:29:18,863] INFO: Iter 93500 Summary: 
[2024-01-22 14:29:18,863] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0433151825889945
[2024-01-22 14:29:21,679] INFO: Iter 93600 Summary: 
[2024-01-22 14:29:21,680] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043398212864995005
[2024-01-22 14:29:24,511] INFO: Iter 93700 Summary: 
[2024-01-22 14:29:24,511] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043166883289813995
[2024-01-22 14:29:27,577] INFO: Iter 93800 Summary: 
[2024-01-22 14:29:27,577] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04322485085576773
[2024-01-22 14:29:30,271] INFO: Iter 93900 Summary: 
[2024-01-22 14:29:30,271] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04334530271589756
[2024-01-22 14:29:33,077] INFO: Iter 94000 Summary: 
[2024-01-22 14:29:33,077] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043206088431179526
[2024-01-22 14:29:35,960] INFO: Iter 94100 Summary: 
[2024-01-22 14:29:35,960] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043226476460695266
[2024-01-22 14:29:38,646] INFO: Iter 94200 Summary: 
[2024-01-22 14:29:38,646] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04315554216504097
[2024-01-22 14:29:41,460] INFO: Iter 94300 Summary: 
[2024-01-22 14:29:41,461] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04310118518769741
[2024-01-22 14:29:44,428] INFO: Iter 94400 Summary: 
[2024-01-22 14:29:44,428] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042713516801595686
[2024-01-22 14:29:47,235] INFO: Iter 94500 Summary: 
[2024-01-22 14:29:47,235] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04324570905417204
[2024-01-22 14:29:50,122] INFO: Iter 94600 Summary: 
[2024-01-22 14:29:50,123] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04339624516665935
[2024-01-22 14:29:53,042] INFO: Iter 94700 Summary: 
[2024-01-22 14:29:53,042] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04309376552700996
[2024-01-22 14:29:55,988] INFO: Iter 94800 Summary: 
[2024-01-22 14:29:55,988] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043308719396591186
[2024-01-22 14:29:58,797] INFO: Iter 94900 Summary: 
[2024-01-22 14:29:58,797] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043202886655926705
[2024-01-22 14:30:01,669] INFO: Iter 95000 Summary: 
[2024-01-22 14:30:01,669] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04295872423797846
[2024-01-22 14:30:04,518] INFO: Iter 95100 Summary: 
[2024-01-22 14:30:04,518] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340660382062197
[2024-01-22 14:30:07,248] INFO: Iter 95200 Summary: 
[2024-01-22 14:30:07,249] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04281930774450302
[2024-01-22 14:30:09,978] INFO: Iter 95300 Summary: 
[2024-01-22 14:30:09,978] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04335564147680998
[2024-01-22 14:30:12,750] INFO: Iter 95400 Summary: 
[2024-01-22 14:30:12,750] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043269167132675646
[2024-01-22 14:30:15,454] INFO: Iter 95500 Summary: 
[2024-01-22 14:30:15,454] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043589989766478536
[2024-01-22 14:30:18,117] INFO: Iter 95600 Summary: 
[2024-01-22 14:30:18,117] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359470102936029
[2024-01-22 14:30:20,952] INFO: Iter 95700 Summary: 
[2024-01-22 14:30:20,952] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04296972874552012
[2024-01-22 14:30:23,798] INFO: Iter 95800 Summary: 
[2024-01-22 14:30:23,798] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04320350274443627
[2024-01-22 14:30:26,578] INFO: Iter 95900 Summary: 
[2024-01-22 14:30:26,578] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043500044792890546
[2024-01-22 14:30:29,453] INFO: Iter 96000 Summary: 
[2024-01-22 14:30:29,453] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04330457363277674
[2024-01-22 14:30:32,429] INFO: Iter 96100 Summary: 
[2024-01-22 14:30:32,429] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04315291482955217
[2024-01-22 14:30:35,072] INFO: Iter 96200 Summary: 
[2024-01-22 14:30:35,072] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043189899884164336
[2024-01-22 14:30:37,986] INFO: Iter 96300 Summary: 
[2024-01-22 14:30:37,986] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043199575990438464
[2024-01-22 14:30:40,816] INFO: Iter 96400 Summary: 
[2024-01-22 14:30:40,816] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04321652166545391
[2024-01-22 14:30:43,615] INFO: Iter 96500 Summary: 
[2024-01-22 14:30:43,615] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043087716586887835
[2024-01-22 14:30:46,394] INFO: Iter 96600 Summary: 
[2024-01-22 14:30:46,394] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04339130636304617
[2024-01-22 14:30:49,261] INFO: Iter 96700 Summary: 
[2024-01-22 14:30:49,262] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340651705861091
[2024-01-22 14:30:52,051] INFO: Iter 96800 Summary: 
[2024-01-22 14:30:52,051] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04310585685074329
[2024-01-22 14:30:54,794] INFO: Iter 96900 Summary: 
[2024-01-22 14:30:54,794] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04349215868860483
[2024-01-22 14:30:57,679] INFO: Iter 97000 Summary: 
[2024-01-22 14:30:57,679] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04297722764313221
[2024-01-22 14:31:00,567] INFO: Iter 97100 Summary: 
[2024-01-22 14:31:00,567] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04291278224438429
[2024-01-22 14:31:03,209] INFO: Iter 97200 Summary: 
[2024-01-22 14:31:03,209] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04323334947228432
[2024-01-22 14:31:06,057] INFO: Iter 97300 Summary: 
[2024-01-22 14:31:06,057] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04330221064388752
[2024-01-22 14:31:08,882] INFO: Iter 97400 Summary: 
[2024-01-22 14:31:08,882] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04328654922544956
[2024-01-22 14:31:11,783] INFO: Iter 97500 Summary: 
[2024-01-22 14:31:11,783] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043253795243799686
[2024-01-22 14:31:14,558] INFO: Iter 97600 Summary: 
[2024-01-22 14:31:14,558] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043265289328992367
[2024-01-22 14:31:17,441] INFO: Iter 97700 Summary: 
[2024-01-22 14:31:17,441] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316348612308502
[2024-01-22 14:31:20,024] INFO: Iter 97800 Summary: 
[2024-01-22 14:31:20,025] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04298086203634739
[2024-01-22 14:31:22,710] INFO: Iter 97900 Summary: 
[2024-01-22 14:31:22,711] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043441864624619485
[2024-01-22 14:31:25,636] INFO: Iter 98000 Summary: 
[2024-01-22 14:31:25,636] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04320315845310688
[2024-01-22 14:31:28,465] INFO: Iter 98100 Summary: 
[2024-01-22 14:31:28,465] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04318555172532797
[2024-01-22 14:31:31,315] INFO: Iter 98200 Summary: 
[2024-01-22 14:31:31,315] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04328099995851517
[2024-01-22 14:31:34,102] INFO: Iter 98300 Summary: 
[2024-01-22 14:31:34,103] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043345788419246675
[2024-01-22 14:31:36,826] INFO: Iter 98400 Summary: 
[2024-01-22 14:31:36,826] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042964852415025234
[2024-01-22 14:31:39,664] INFO: Iter 98500 Summary: 
[2024-01-22 14:31:39,664] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043101236931979654
[2024-01-22 14:31:42,509] INFO: Iter 98600 Summary: 
[2024-01-22 14:31:42,509] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04301708202809095
[2024-01-22 14:31:45,402] INFO: Iter 98700 Summary: 
[2024-01-22 14:31:45,402] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043237831890583035
[2024-01-22 14:31:48,230] INFO: Iter 98800 Summary: 
[2024-01-22 14:31:48,231] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043021786808967594
[2024-01-22 14:31:50,927] INFO: Iter 98900 Summary: 
[2024-01-22 14:31:50,928] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340923123061657
[2024-01-22 14:31:53,783] INFO: Iter 99000 Summary: 
[2024-01-22 14:31:53,783] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04330835770815611
[2024-01-22 14:31:56,614] INFO: Iter 99100 Summary: 
[2024-01-22 14:31:56,614] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04300343055278063
[2024-01-22 14:31:59,382] INFO: Iter 99200 Summary: 
[2024-01-22 14:31:59,382] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04315609320998192
[2024-01-22 14:32:02,031] INFO: Iter 99300 Summary: 
[2024-01-22 14:32:02,031] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043049372397363186
[2024-01-22 14:32:04,746] INFO: Iter 99400 Summary: 
[2024-01-22 14:32:04,746] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0431581774353981
[2024-01-22 14:32:07,461] INFO: Iter 99500 Summary: 
[2024-01-22 14:32:07,461] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04289864089339972
[2024-01-22 14:32:10,205] INFO: Iter 99600 Summary: 
[2024-01-22 14:32:10,205] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0431738843023777
[2024-01-22 14:32:13,055] INFO: Iter 99700 Summary: 
[2024-01-22 14:32:13,055] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043234402872622014
[2024-01-22 14:32:15,961] INFO: Iter 99800 Summary: 
[2024-01-22 14:32:15,961] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043334142975509166
[2024-01-22 14:32:18,805] INFO: Iter 99900 Summary: 
[2024-01-22 14:32:18,805] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042528198063373567
[2024-01-22 14:32:21,636] INFO: Iter 100000 Summary: 
[2024-01-22 14:32:21,636] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04278517112135887
[2024-01-22 14:32:24,326] INFO: Iter 100100 Summary: 
[2024-01-22 14:32:24,326] INFO: 	 lr: 1.2900000000000026e-05 	 Training loss: 0.04174726735800505
[2024-01-22 14:32:27,238] INFO: Iter 100200 Summary: 
[2024-01-22 14:32:27,239] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04131290312856436
[2024-01-22 14:32:29,928] INFO: Iter 100300 Summary: 
[2024-01-22 14:32:29,928] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.041043280586600304
[2024-01-22 14:32:32,891] INFO: Iter 100400 Summary: 
[2024-01-22 14:32:32,891] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.041132740452885626
[2024-01-22 14:32:35,690] INFO: Iter 100500 Summary: 
[2024-01-22 14:32:35,690] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04107717353850603
[2024-01-22 14:32:38,417] INFO: Iter 100600 Summary: 
[2024-01-22 14:32:38,417] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04105785455554724
[2024-01-22 14:32:41,290] INFO: Iter 100700 Summary: 
[2024-01-22 14:32:41,290] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040767465978860856
[2024-01-22 14:32:44,175] INFO: Iter 100800 Summary: 
[2024-01-22 14:32:44,175] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04088238824158907
[2024-01-22 14:32:47,004] INFO: Iter 100900 Summary: 
[2024-01-22 14:32:47,004] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04095139130949974
[2024-01-22 14:32:49,779] INFO: Iter 101000 Summary: 
[2024-01-22 14:32:49,779] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04090138506144285
[2024-01-22 14:32:52,598] INFO: Iter 101100 Summary: 
[2024-01-22 14:32:52,598] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040517062172293665
[2024-01-22 14:32:55,365] INFO: Iter 101200 Summary: 
[2024-01-22 14:32:55,365] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040880058482289316
[2024-01-22 14:32:58,077] INFO: Iter 101300 Summary: 
[2024-01-22 14:32:58,077] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04092390969395637
[2024-01-22 14:33:00,904] INFO: Iter 101400 Summary: 
[2024-01-22 14:33:00,904] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04079420786350965
[2024-01-22 14:33:03,713] INFO: Iter 101500 Summary: 
[2024-01-22 14:33:03,714] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04084659956395626
[2024-01-22 14:33:06,481] INFO: Iter 101600 Summary: 
[2024-01-22 14:33:06,481] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04084970660507679
[2024-01-22 14:33:09,262] INFO: Iter 101700 Summary: 
[2024-01-22 14:33:09,262] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040456504411995414
[2024-01-22 14:33:12,185] INFO: Iter 101800 Summary: 
[2024-01-22 14:33:12,185] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040764740742743014
[2024-01-22 14:33:15,102] INFO: Iter 101900 Summary: 
[2024-01-22 14:33:15,102] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040512860119342804
[2024-01-22 14:33:18,042] INFO: Iter 102000 Summary: 
[2024-01-22 14:33:18,042] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040691986083984374
[2024-01-22 14:33:20,745] INFO: Iter 102100 Summary: 
[2024-01-22 14:33:20,745] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040303669087588784
[2024-01-22 14:33:23,437] INFO: Iter 102200 Summary: 
[2024-01-22 14:33:23,438] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04064858760684729
[2024-01-22 14:33:26,247] INFO: Iter 102300 Summary: 
[2024-01-22 14:33:26,247] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04077907245606184
[2024-01-22 14:33:29,035] INFO: Iter 102400 Summary: 
[2024-01-22 14:33:29,035] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04046691853553057
[2024-01-22 14:33:31,835] INFO: Iter 102500 Summary: 
[2024-01-22 14:33:31,835] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04081105150282383
[2024-01-22 14:33:34,580] INFO: Iter 102600 Summary: 
[2024-01-22 14:33:34,580] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04058349158614874
[2024-01-22 14:33:37,270] INFO: Iter 102700 Summary: 
[2024-01-22 14:33:37,271] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040753283612430094
[2024-01-22 14:33:40,065] INFO: Iter 102800 Summary: 
[2024-01-22 14:33:40,065] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04043942403048277
[2024-01-22 14:33:43,015] INFO: Iter 102900 Summary: 
[2024-01-22 14:33:43,016] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04055497247725725
[2024-01-22 14:33:45,892] INFO: Iter 103000 Summary: 
[2024-01-22 14:33:45,892] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04052550762891769
[2024-01-22 14:33:48,725] INFO: Iter 103100 Summary: 
[2024-01-22 14:33:48,725] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040196266509592536
[2024-01-22 14:33:51,667] INFO: Iter 103200 Summary: 
[2024-01-22 14:33:51,667] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04041284833103418
[2024-01-22 14:33:54,453] INFO: Iter 103300 Summary: 
[2024-01-22 14:33:54,453] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04064191747456789
[2024-01-22 14:33:57,292] INFO: Iter 103400 Summary: 
[2024-01-22 14:33:57,293] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04044346533715725
[2024-01-22 14:34:00,068] INFO: Iter 103500 Summary: 
[2024-01-22 14:34:00,068] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040435242764651776
[2024-01-22 14:34:03,017] INFO: Iter 103600 Summary: 
[2024-01-22 14:34:03,017] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04051686234772205
[2024-01-22 14:34:05,681] INFO: Iter 103700 Summary: 
[2024-01-22 14:34:05,682] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040420528762042524
[2024-01-22 14:34:08,663] INFO: Iter 103800 Summary: 
[2024-01-22 14:34:08,663] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04007362429052591
[2024-01-22 14:34:11,351] INFO: Iter 103900 Summary: 
[2024-01-22 14:34:11,351] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04049754027277231
[2024-01-22 14:34:13,923] INFO: Iter 104000 Summary: 
[2024-01-22 14:34:13,923] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04010758221149444
[2024-01-22 14:34:16,662] INFO: Iter 104100 Summary: 
[2024-01-22 14:34:16,662] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040249662436544896
[2024-01-22 14:34:19,365] INFO: Iter 104200 Summary: 
[2024-01-22 14:34:19,365] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040231188535690306
[2024-01-22 14:34:22,044] INFO: Iter 104300 Summary: 
[2024-01-22 14:34:22,044] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040319494903087616
[2024-01-22 14:34:24,965] INFO: Iter 104400 Summary: 
[2024-01-22 14:34:24,965] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04024822801351547
[2024-01-22 14:34:27,868] INFO: Iter 104500 Summary: 
[2024-01-22 14:34:27,868] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04038021571934223
[2024-01-22 14:34:30,839] INFO: Iter 104600 Summary: 
[2024-01-22 14:34:30,839] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04024528779089451
[2024-01-22 14:34:33,502] INFO: Iter 104700 Summary: 
[2024-01-22 14:34:33,502] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040278332605957985
[2024-01-22 14:34:36,298] INFO: Iter 104800 Summary: 
[2024-01-22 14:34:36,298] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04015074905008078
[2024-01-22 14:34:39,139] INFO: Iter 104900 Summary: 
[2024-01-22 14:34:39,139] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04018383037298918
[2024-01-22 14:34:41,734] INFO: Iter 105000 Summary: 
[2024-01-22 14:34:41,735] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040283375047147275
[2024-01-22 14:34:44,575] INFO: Iter 105100 Summary: 
[2024-01-22 14:34:44,576] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03990562833845615
[2024-01-22 14:34:47,419] INFO: Iter 105200 Summary: 
[2024-01-22 14:34:47,419] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04026258960366249
[2024-01-22 14:34:50,310] INFO: Iter 105300 Summary: 
[2024-01-22 14:34:50,311] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04011769503355026
[2024-01-22 14:34:53,005] INFO: Iter 105400 Summary: 
[2024-01-22 14:34:53,006] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040207268707454204
[2024-01-22 14:34:55,769] INFO: Iter 105500 Summary: 
[2024-01-22 14:34:55,769] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04033503498882055
[2024-01-22 14:34:58,558] INFO: Iter 105600 Summary: 
[2024-01-22 14:34:58,558] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040278920866549016
[2024-01-22 14:35:01,336] INFO: Iter 105700 Summary: 
[2024-01-22 14:35:01,337] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03996938109397888
[2024-01-22 14:35:04,118] INFO: Iter 105800 Summary: 
[2024-01-22 14:35:04,118] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040315496176481246
[2024-01-22 14:35:06,855] INFO: Iter 105900 Summary: 
[2024-01-22 14:35:06,856] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0400116116181016
[2024-01-22 14:35:09,689] INFO: Iter 106000 Summary: 
[2024-01-22 14:35:09,690] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040335774011909964
[2024-01-22 14:35:12,628] INFO: Iter 106100 Summary: 
[2024-01-22 14:35:12,628] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04002016343176365
[2024-01-22 14:35:15,425] INFO: Iter 106200 Summary: 
[2024-01-22 14:35:15,425] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040590275675058365
[2024-01-22 14:35:18,253] INFO: Iter 106300 Summary: 
[2024-01-22 14:35:18,253] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04016178280115128
[2024-01-22 14:35:21,071] INFO: Iter 106400 Summary: 
[2024-01-22 14:35:21,071] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040405033491551876
[2024-01-22 14:35:23,949] INFO: Iter 106500 Summary: 
[2024-01-22 14:35:23,949] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03998575046658516
[2024-01-22 14:35:26,626] INFO: Iter 106600 Summary: 
[2024-01-22 14:35:26,626] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04027827553451061
[2024-01-22 14:35:29,153] INFO: Iter 106700 Summary: 
[2024-01-22 14:35:29,154] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04016139671206474
[2024-01-22 14:35:32,045] INFO: Iter 106800 Summary: 
[2024-01-22 14:35:32,046] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04024116836488247
[2024-01-22 14:35:34,827] INFO: Iter 106900 Summary: 
[2024-01-22 14:35:34,827] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040392355807125566
[2024-01-22 14:35:37,505] INFO: Iter 107000 Summary: 
[2024-01-22 14:35:37,505] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03988117121160031
[2024-01-22 14:35:40,367] INFO: Iter 107100 Summary: 
[2024-01-22 14:35:40,367] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04029654946178198
[2024-01-22 14:35:43,364] INFO: Iter 107200 Summary: 
[2024-01-22 14:35:43,364] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04025014542043209
[2024-01-22 14:35:46,120] INFO: Iter 107300 Summary: 
[2024-01-22 14:35:46,120] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040118198208510876
[2024-01-22 14:35:48,685] INFO: Iter 107400 Summary: 
[2024-01-22 14:35:48,685] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0402251885458827
[2024-01-22 14:35:51,372] INFO: Iter 107500 Summary: 
[2024-01-22 14:35:51,373] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040049730874598025
[2024-01-22 14:35:54,097] INFO: Iter 107600 Summary: 
[2024-01-22 14:35:54,097] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040242215506732466
[2024-01-22 14:35:56,887] INFO: Iter 107700 Summary: 
[2024-01-22 14:35:56,887] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04035109639167786
[2024-01-22 14:35:59,733] INFO: Iter 107800 Summary: 
[2024-01-22 14:35:59,733] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03989992789924145
[2024-01-22 14:36:02,578] INFO: Iter 107900 Summary: 
[2024-01-22 14:36:02,578] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04026699665933847
[2024-01-22 14:36:05,189] INFO: Iter 108000 Summary: 
[2024-01-22 14:36:05,190] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040278237387537955
[2024-01-22 14:36:07,946] INFO: Iter 108100 Summary: 
[2024-01-22 14:36:07,946] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039803117662668225
[2024-01-22 14:36:10,587] INFO: Iter 108200 Summary: 
[2024-01-22 14:36:10,587] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040030137933790684
[2024-01-22 14:36:13,469] INFO: Iter 108300 Summary: 
[2024-01-22 14:36:13,469] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04026621848344803
[2024-01-22 14:36:16,237] INFO: Iter 108400 Summary: 
[2024-01-22 14:36:16,238] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04019490510225296
[2024-01-22 14:36:18,961] INFO: Iter 108500 Summary: 
[2024-01-22 14:36:18,961] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04004624392837286
[2024-01-22 14:36:21,760] INFO: Iter 108600 Summary: 
[2024-01-22 14:36:21,761] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040141792558133604
[2024-01-22 14:36:24,475] INFO: Iter 108700 Summary: 
[2024-01-22 14:36:24,476] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0401214037463069
[2024-01-22 14:36:27,275] INFO: Iter 108800 Summary: 
[2024-01-22 14:36:27,275] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04024200852960348
[2024-01-22 14:36:29,925] INFO: Iter 108900 Summary: 
[2024-01-22 14:36:29,925] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039957833960652354
[2024-01-22 14:36:32,722] INFO: Iter 109000 Summary: 
[2024-01-22 14:36:32,722] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04016981389373541
[2024-01-22 14:36:35,410] INFO: Iter 109100 Summary: 
[2024-01-22 14:36:35,410] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03991521988064051
[2024-01-22 14:36:38,500] INFO: Iter 109200 Summary: 
[2024-01-22 14:36:38,500] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0401188887283206
[2024-01-22 14:36:41,487] INFO: Iter 109300 Summary: 
[2024-01-22 14:36:41,487] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03980120614171028
[2024-01-22 14:36:44,173] INFO: Iter 109400 Summary: 
[2024-01-22 14:36:44,173] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039906388334929946
[2024-01-22 14:36:46,816] INFO: Iter 109500 Summary: 
[2024-01-22 14:36:46,816] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04036563351750374
[2024-01-22 14:36:49,686] INFO: Iter 109600 Summary: 
[2024-01-22 14:36:49,686] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04003064218908548
[2024-01-22 14:36:52,271] INFO: Iter 109700 Summary: 
[2024-01-22 14:36:52,271] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04018042080104351
[2024-01-22 14:36:54,987] INFO: Iter 109800 Summary: 
[2024-01-22 14:36:54,987] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040235735811293125
[2024-01-22 14:36:57,523] INFO: Iter 109900 Summary: 
[2024-01-22 14:36:57,523] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039906641505658626
[2024-01-22 14:37:00,481] INFO: Iter 110000 Summary: 
[2024-01-22 14:37:00,481] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04021320018917322
[2024-01-22 14:37:03,044] INFO: Iter 110100 Summary: 
[2024-01-22 14:37:03,044] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039998093992471694
[2024-01-22 14:37:05,480] INFO: Iter 110200 Summary: 
[2024-01-22 14:37:05,480] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039865457974374297
[2024-01-22 14:37:08,274] INFO: Iter 110300 Summary: 
[2024-01-22 14:37:08,275] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040167109854519364
[2024-01-22 14:37:11,033] INFO: Iter 110400 Summary: 
[2024-01-22 14:37:11,033] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040207143127918246
[2024-01-22 14:37:13,786] INFO: Iter 110500 Summary: 
[2024-01-22 14:37:13,786] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03984847765415907
[2024-01-22 14:37:16,624] INFO: Iter 110600 Summary: 
[2024-01-22 14:37:16,624] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03983835302293301
[2024-01-22 14:37:19,263] INFO: Iter 110700 Summary: 
[2024-01-22 14:37:19,263] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040040679946541785
[2024-01-22 14:37:22,063] INFO: Iter 110800 Summary: 
[2024-01-22 14:37:22,063] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03992396846413612
[2024-01-22 14:37:24,795] INFO: Iter 110900 Summary: 
[2024-01-22 14:37:24,795] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03983908448368311
[2024-01-22 14:37:27,665] INFO: Iter 111000 Summary: 
[2024-01-22 14:37:27,665] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040047959461808204
[2024-01-22 14:37:30,309] INFO: Iter 111100 Summary: 
[2024-01-22 14:37:30,310] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040363709852099416
[2024-01-22 14:37:33,098] INFO: Iter 111200 Summary: 
[2024-01-22 14:37:33,098] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03986946988850832
[2024-01-22 14:37:35,960] INFO: Iter 111300 Summary: 
[2024-01-22 14:37:35,960] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03991910606622696
[2024-01-22 14:37:38,499] INFO: Iter 111400 Summary: 
[2024-01-22 14:37:38,500] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040057860501110557
[2024-01-22 14:37:41,387] INFO: Iter 111500 Summary: 
[2024-01-22 14:37:41,387] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03993999890983105
[2024-01-22 14:37:44,183] INFO: Iter 111600 Summary: 
[2024-01-22 14:37:44,183] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03998496852815151
[2024-01-22 14:37:46,971] INFO: Iter 111700 Summary: 
[2024-01-22 14:37:46,971] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03999470140784979
[2024-01-22 14:37:49,705] INFO: Iter 111800 Summary: 
[2024-01-22 14:37:49,706] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04014978364109993
[2024-01-22 14:37:52,626] INFO: Iter 111900 Summary: 
[2024-01-22 14:37:52,627] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03971776820719242
[2024-01-22 14:37:55,469] INFO: Iter 112000 Summary: 
[2024-01-22 14:37:55,470] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0401540007814765
[2024-01-22 14:37:58,282] INFO: Iter 112100 Summary: 
[2024-01-22 14:37:58,282] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03993711620569229
[2024-01-22 14:38:01,092] INFO: Iter 112200 Summary: 
[2024-01-22 14:38:01,092] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040163213461637495
[2024-01-22 14:38:03,821] INFO: Iter 112300 Summary: 
[2024-01-22 14:38:03,822] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03978743813931942
[2024-01-22 14:38:06,333] INFO: Iter 112400 Summary: 
[2024-01-22 14:38:06,333] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039682850725948814
[2024-01-22 14:38:08,994] INFO: Iter 112500 Summary: 
[2024-01-22 14:38:08,994] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04013194914907217
[2024-01-22 14:38:11,744] INFO: Iter 112600 Summary: 
[2024-01-22 14:38:11,744] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03985425643622875
[2024-01-22 14:38:14,553] INFO: Iter 112700 Summary: 
[2024-01-22 14:38:14,553] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039976424500346185
[2024-01-22 14:38:17,193] INFO: Iter 112800 Summary: 
[2024-01-22 14:38:17,193] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03962756272405386
[2024-01-22 14:38:20,062] INFO: Iter 112900 Summary: 
[2024-01-22 14:38:20,062] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039979365468025205
[2024-01-22 14:38:22,903] INFO: Iter 113000 Summary: 
[2024-01-22 14:38:22,903] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04001565534621477
[2024-01-22 14:38:25,542] INFO: Iter 113100 Summary: 
[2024-01-22 14:38:25,542] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040146961063146594
[2024-01-22 14:38:28,291] INFO: Iter 113200 Summary: 
[2024-01-22 14:38:28,291] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04008762177079916
[2024-01-22 14:38:31,100] INFO: Iter 113300 Summary: 
[2024-01-22 14:38:31,100] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03975199557840824
[2024-01-22 14:38:33,828] INFO: Iter 113400 Summary: 
[2024-01-22 14:38:33,829] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04010572448372841
[2024-01-22 14:38:36,629] INFO: Iter 113500 Summary: 
[2024-01-22 14:38:36,629] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03982433523982763
[2024-01-22 14:38:39,475] INFO: Iter 113600 Summary: 
[2024-01-22 14:38:39,475] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03991207268089056
[2024-01-22 14:38:42,186] INFO: Iter 113700 Summary: 
[2024-01-22 14:38:42,186] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03998806782066822
[2024-01-22 14:38:44,995] INFO: Iter 113800 Summary: 
[2024-01-22 14:38:44,995] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03985734477639198
[2024-01-22 14:38:47,997] INFO: Iter 113900 Summary: 
[2024-01-22 14:38:47,997] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039547758959233764
[2024-01-22 14:38:50,918] INFO: Iter 114000 Summary: 
[2024-01-22 14:38:50,918] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04007723789662123
[2024-01-22 14:38:53,606] INFO: Iter 114100 Summary: 
[2024-01-22 14:38:53,606] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0399622917920351
[2024-01-22 14:38:56,307] INFO: Iter 114200 Summary: 
[2024-01-22 14:38:56,307] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040241438075900075
[2024-01-22 14:38:58,980] INFO: Iter 114300 Summary: 
[2024-01-22 14:38:58,980] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03979501992464066
[2024-01-22 14:39:01,624] INFO: Iter 114400 Summary: 
[2024-01-22 14:39:01,624] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039642022401094434
[2024-01-22 14:39:04,376] INFO: Iter 114500 Summary: 
[2024-01-22 14:39:04,376] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039925022535026077
[2024-01-22 14:39:07,308] INFO: Iter 114600 Summary: 
[2024-01-22 14:39:07,309] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03962619390338659
[2024-01-22 14:39:10,194] INFO: Iter 114700 Summary: 
[2024-01-22 14:39:10,194] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03975536562502384
[2024-01-22 14:39:12,924] INFO: Iter 114800 Summary: 
[2024-01-22 14:39:12,924] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03988947860896588
[2024-01-22 14:39:15,661] INFO: Iter 114900 Summary: 
[2024-01-22 14:39:15,661] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039589091390371325
[2024-01-22 14:39:18,464] INFO: Iter 115000 Summary: 
[2024-01-22 14:39:18,465] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04019000485539436
