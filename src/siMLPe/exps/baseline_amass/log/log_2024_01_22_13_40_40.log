[2024-01-22 13:41:18,587] INFO: {
    "abs_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass",
    "amass_anno_dir": "/home/aiRobots/src/siMLPe/data/amass/",
    "batch_size": 256,
    "cos_lr_max": 0.0003,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 115000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "link_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/log_last.log",
    "link_val_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/val_last.log",
    "log_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log",
    "log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/log_2024_01_22_13_40_40.log",
    "model_pth": null,
    "motion": {
        "amass_input_length": 50,
        "amass_input_length_dct": 50,
        "amass_target_length": 25,
        "amass_target_length_eval": 25,
        "amass_target_length_train": 25,
        "dim": 54,
        "pw3d_input_length": 50,
        "pw3d_target_length_eval": 25,
        "pw3d_target_length_train": 25
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": false,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": true,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 54,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 8,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "pw3d_anno_dir": "/home/aiRobots/src/siMLPe/data/3dpw/sequenceFiles/",
    "repo_name": "siMLPe",
    "root_dir": "/home/aiRobots/src/siMLPe",
    "save_every": 5000,
    "seed": 888,
    "shift_step": 5,
    "snapshot_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/snapshot",
    "this_dir": "baseline_amass",
    "use_relative_loss": true,
    "val_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/val_2024_01_22_13_40_40.log",
    "weight_decay": 0.0001
}
[2024-01-22 13:41:21,607] INFO: Iter 100 Summary: 
[2024-01-22 13:41:21,607] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09866055086255074
[2024-01-22 13:41:24,397] INFO: Iter 200 Summary: 
[2024-01-22 13:41:24,397] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07970086477696896
[2024-01-22 13:41:26,954] INFO: Iter 300 Summary: 
[2024-01-22 13:41:26,955] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07207509778439998
[2024-01-22 13:41:29,642] INFO: Iter 400 Summary: 
[2024-01-22 13:41:29,643] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06888341203331948
[2024-01-22 13:41:32,331] INFO: Iter 500 Summary: 
[2024-01-22 13:41:32,331] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06719265434890985
[2024-01-22 13:41:35,086] INFO: Iter 600 Summary: 
[2024-01-22 13:41:35,086] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06574191194027662
[2024-01-22 13:41:37,597] INFO: Iter 700 Summary: 
[2024-01-22 13:41:37,597] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06505372971296311
[2024-01-22 13:41:40,202] INFO: Iter 800 Summary: 
[2024-01-22 13:41:40,202] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06387996215373277
[2024-01-22 13:41:42,940] INFO: Iter 900 Summary: 
[2024-01-22 13:41:42,940] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0637226140126586
[2024-01-22 13:41:45,560] INFO: Iter 1000 Summary: 
[2024-01-22 13:41:45,560] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06281336572021246
[2024-01-22 13:41:48,097] INFO: Iter 1100 Summary: 
[2024-01-22 13:41:48,097] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.062083161622285846
[2024-01-22 13:41:50,932] INFO: Iter 1200 Summary: 
[2024-01-22 13:41:50,932] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0616617776080966
[2024-01-22 13:41:53,616] INFO: Iter 1300 Summary: 
[2024-01-22 13:41:53,616] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06138411946594715
[2024-01-22 13:41:56,483] INFO: Iter 1400 Summary: 
[2024-01-22 13:41:56,484] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06102396670728922
[2024-01-22 13:41:59,390] INFO: Iter 1500 Summary: 
[2024-01-22 13:41:59,390] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06052622187882662
[2024-01-22 13:42:02,070] INFO: Iter 1600 Summary: 
[2024-01-22 13:42:02,070] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.060147434398531915
[2024-01-22 13:42:04,658] INFO: Iter 1700 Summary: 
[2024-01-22 13:42:04,658] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05966938678175211
[2024-01-22 13:42:07,408] INFO: Iter 1800 Summary: 
[2024-01-22 13:42:07,408] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05860090240836144
[2024-01-22 13:42:09,977] INFO: Iter 1900 Summary: 
[2024-01-22 13:42:09,977] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05886689271777868
[2024-01-22 13:42:12,785] INFO: Iter 2000 Summary: 
[2024-01-22 13:42:12,785] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.058561157025396826
[2024-01-22 13:42:15,513] INFO: Iter 2100 Summary: 
[2024-01-22 13:42:15,513] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.058301390409469606
[2024-01-22 13:42:18,189] INFO: Iter 2200 Summary: 
[2024-01-22 13:42:18,189] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05770072065293789
[2024-01-22 13:42:20,918] INFO: Iter 2300 Summary: 
[2024-01-22 13:42:20,919] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05735681269317865
[2024-01-22 13:42:23,546] INFO: Iter 2400 Summary: 
[2024-01-22 13:42:23,547] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.057302918545901776
[2024-01-22 13:42:26,159] INFO: Iter 2500 Summary: 
[2024-01-22 13:42:26,160] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05703567836433649
[2024-01-22 13:42:28,956] INFO: Iter 2600 Summary: 
[2024-01-22 13:42:28,956] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05690142896026373
[2024-01-22 13:42:31,454] INFO: Iter 2700 Summary: 
[2024-01-22 13:42:31,454] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.056595995798707006
[2024-01-22 13:42:34,325] INFO: Iter 2800 Summary: 
[2024-01-22 13:42:34,325] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.056499514020979406
[2024-01-22 13:42:37,071] INFO: Iter 2900 Summary: 
[2024-01-22 13:42:37,071] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05616317145526409
[2024-01-22 13:42:39,614] INFO: Iter 3000 Summary: 
[2024-01-22 13:42:39,614] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055999592058360574
[2024-01-22 13:42:42,274] INFO: Iter 3100 Summary: 
[2024-01-22 13:42:42,274] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055527230724692345
[2024-01-22 13:42:44,781] INFO: Iter 3200 Summary: 
[2024-01-22 13:42:44,781] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055334230996668336
[2024-01-22 13:42:47,264] INFO: Iter 3300 Summary: 
[2024-01-22 13:42:47,264] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05524085976183415
[2024-01-22 13:42:49,840] INFO: Iter 3400 Summary: 
[2024-01-22 13:42:49,840] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05536742754280567
[2024-01-22 13:42:52,650] INFO: Iter 3500 Summary: 
[2024-01-22 13:42:52,651] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05535300519317388
[2024-01-22 13:42:55,478] INFO: Iter 3600 Summary: 
[2024-01-22 13:42:55,479] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05545871268957853
[2024-01-22 13:42:58,279] INFO: Iter 3700 Summary: 
[2024-01-22 13:42:58,279] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05522134453058243
[2024-01-22 13:43:00,965] INFO: Iter 3800 Summary: 
[2024-01-22 13:43:00,965] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05495358947664499
[2024-01-22 13:43:03,905] INFO: Iter 3900 Summary: 
[2024-01-22 13:43:03,905] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054538472704589364
[2024-01-22 13:43:06,636] INFO: Iter 4000 Summary: 
[2024-01-22 13:43:06,636] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05458749920129776
[2024-01-22 13:43:09,178] INFO: Iter 4100 Summary: 
[2024-01-22 13:43:09,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054525446929037574
[2024-01-22 13:43:11,828] INFO: Iter 4200 Summary: 
[2024-01-22 13:43:11,828] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054882142841815945
[2024-01-22 13:43:14,482] INFO: Iter 4300 Summary: 
[2024-01-22 13:43:14,482] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05440907523036003
[2024-01-22 13:43:17,024] INFO: Iter 4400 Summary: 
[2024-01-22 13:43:17,024] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054322384409606454
[2024-01-22 13:43:19,731] INFO: Iter 4500 Summary: 
[2024-01-22 13:43:19,731] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05415802184492349
[2024-01-22 13:43:22,302] INFO: Iter 4600 Summary: 
[2024-01-22 13:43:22,302] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05454121269285679
[2024-01-22 13:43:24,908] INFO: Iter 4700 Summary: 
[2024-01-22 13:43:24,908] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05315664891153574
[2024-01-22 13:43:27,353] INFO: Iter 4800 Summary: 
[2024-01-22 13:43:27,353] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05389671057462692
[2024-01-22 13:43:30,005] INFO: Iter 4900 Summary: 
[2024-01-22 13:43:30,006] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05370066542178392
[2024-01-22 13:43:32,652] INFO: Iter 5000 Summary: 
[2024-01-22 13:43:32,652] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053745503947138784
[2024-01-22 13:43:35,205] INFO: Iter 5100 Summary: 
[2024-01-22 13:43:35,205] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05384746540337801
[2024-01-22 13:43:37,949] INFO: Iter 5200 Summary: 
[2024-01-22 13:43:37,950] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05329228848218918
[2024-01-22 13:43:40,748] INFO: Iter 5300 Summary: 
[2024-01-22 13:43:40,748] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05300307709723711
[2024-01-22 13:43:43,312] INFO: Iter 5400 Summary: 
[2024-01-22 13:43:43,312] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05336924072355032
[2024-01-22 13:43:45,903] INFO: Iter 5500 Summary: 
[2024-01-22 13:43:45,903] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053297074735164644
[2024-01-22 13:43:48,505] INFO: Iter 5600 Summary: 
[2024-01-22 13:43:48,505] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05319382280111313
[2024-01-22 13:43:51,130] INFO: Iter 5700 Summary: 
[2024-01-22 13:43:51,130] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05272808615118265
[2024-01-22 13:43:53,699] INFO: Iter 5800 Summary: 
[2024-01-22 13:43:53,699] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052385757565498355
[2024-01-22 13:43:56,412] INFO: Iter 5900 Summary: 
[2024-01-22 13:43:56,412] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052420998811721804
[2024-01-22 13:43:59,239] INFO: Iter 6000 Summary: 
[2024-01-22 13:43:59,239] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052416791953146455
[2024-01-22 13:44:01,748] INFO: Iter 6100 Summary: 
[2024-01-22 13:44:01,749] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05249941978603601
[2024-01-22 13:44:04,481] INFO: Iter 6200 Summary: 
[2024-01-22 13:44:04,481] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05203603569418192
[2024-01-22 13:44:07,161] INFO: Iter 6300 Summary: 
[2024-01-22 13:44:07,161] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05256746016442776
[2024-01-22 13:44:09,815] INFO: Iter 6400 Summary: 
[2024-01-22 13:44:09,815] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05181027941405773
[2024-01-22 13:44:12,352] INFO: Iter 6500 Summary: 
[2024-01-22 13:44:12,352] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05222161553800106
[2024-01-22 13:44:15,065] INFO: Iter 6600 Summary: 
[2024-01-22 13:44:15,065] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051937056481838224
[2024-01-22 13:44:17,779] INFO: Iter 6700 Summary: 
[2024-01-22 13:44:17,779] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05214205823838711
[2024-01-22 13:44:20,246] INFO: Iter 6800 Summary: 
[2024-01-22 13:44:20,246] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05179669953882694
[2024-01-22 13:44:23,170] INFO: Iter 6900 Summary: 
[2024-01-22 13:44:23,171] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05189048957079649
[2024-01-22 13:44:26,008] INFO: Iter 7000 Summary: 
[2024-01-22 13:44:26,008] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05168100520968437
[2024-01-22 13:44:28,564] INFO: Iter 7100 Summary: 
[2024-01-22 13:44:28,564] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051688286364078524
[2024-01-22 13:44:31,325] INFO: Iter 7200 Summary: 
[2024-01-22 13:44:31,325] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0516482587531209
[2024-01-22 13:44:34,068] INFO: Iter 7300 Summary: 
[2024-01-22 13:44:34,068] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051294325962662694
[2024-01-22 13:44:36,696] INFO: Iter 7400 Summary: 
[2024-01-22 13:44:36,696] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051271435990929606
[2024-01-22 13:44:39,413] INFO: Iter 7500 Summary: 
[2024-01-22 13:44:39,413] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05149308450520038
[2024-01-22 13:44:42,244] INFO: Iter 7600 Summary: 
[2024-01-22 13:44:42,245] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051288662366569045
[2024-01-22 13:44:45,068] INFO: Iter 7700 Summary: 
[2024-01-22 13:44:45,068] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05132615584880114
[2024-01-22 13:44:47,479] INFO: Iter 7800 Summary: 
[2024-01-22 13:44:47,479] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05127264615148306
[2024-01-22 13:44:50,150] INFO: Iter 7900 Summary: 
[2024-01-22 13:44:50,150] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05089722163975239
[2024-01-22 13:44:52,977] INFO: Iter 8000 Summary: 
[2024-01-22 13:44:52,977] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0510490308329463
[2024-01-22 13:44:55,493] INFO: Iter 8100 Summary: 
[2024-01-22 13:44:55,494] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05074500098824501
[2024-01-22 13:44:58,370] INFO: Iter 8200 Summary: 
[2024-01-22 13:44:58,370] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050944002158939836
[2024-01-22 13:45:01,088] INFO: Iter 8300 Summary: 
[2024-01-22 13:45:01,088] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05073196563869715
[2024-01-22 13:45:03,694] INFO: Iter 8400 Summary: 
[2024-01-22 13:45:03,694] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05042639281600714
[2024-01-22 13:45:06,073] INFO: Iter 8500 Summary: 
[2024-01-22 13:45:06,073] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05078078698366881
[2024-01-22 13:45:08,678] INFO: Iter 8600 Summary: 
[2024-01-22 13:45:08,678] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0506459628790617
[2024-01-22 13:45:11,234] INFO: Iter 8700 Summary: 
[2024-01-22 13:45:11,234] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050339547209441664
[2024-01-22 13:45:13,804] INFO: Iter 8800 Summary: 
[2024-01-22 13:45:13,805] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05044893391430378
[2024-01-22 13:45:16,541] INFO: Iter 8900 Summary: 
[2024-01-22 13:45:16,541] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050757073760032655
