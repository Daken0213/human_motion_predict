[2024-01-22 09:30:05,913] INFO: {
    "abs_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass",
    "amass_anno_dir": "/home/aiRobots/src/siMLPe/data/amass/",
    "batch_size": 256,
    "cos_lr_max": 0.0003,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 115000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "link_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/log_last.log",
    "link_val_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/val_last.log",
    "log_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log",
    "log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/log_2024_01_22_09_29_25.log",
    "model_pth": null,
    "motion": {
        "amass_input_length": 50,
        "amass_input_length_dct": 50,
        "amass_target_length": 25,
        "amass_target_length_eval": 25,
        "amass_target_length_train": 25,
        "dim": 54,
        "pw3d_input_length": 50,
        "pw3d_target_length_eval": 25,
        "pw3d_target_length_train": 25
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": false,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": true,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 54,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 8,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "pw3d_anno_dir": "/home/aiRobots/src/siMLPe/data/3dpw/sequenceFiles/",
    "repo_name": "siMLPe",
    "root_dir": "/home/aiRobots/src/siMLPe",
    "save_every": 5000,
    "seed": 888,
    "shift_step": 5,
    "snapshot_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/snapshot",
    "this_dir": "baseline_amass",
    "use_relative_loss": true,
    "val_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/val_2024_01_22_09_29_25.log",
    "weight_decay": 0.0001
}
[2024-01-22 09:30:08,997] INFO: Iter 100 Summary: 
[2024-01-22 09:30:08,997] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09866055086255074
[2024-01-22 09:30:11,703] INFO: Iter 200 Summary: 
[2024-01-22 09:30:11,704] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07970086477696896
[2024-01-22 09:30:14,384] INFO: Iter 300 Summary: 
[2024-01-22 09:30:14,384] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07207509778439998
[2024-01-22 09:30:16,886] INFO: Iter 400 Summary: 
[2024-01-22 09:30:16,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06888341203331948
[2024-01-22 09:30:19,469] INFO: Iter 500 Summary: 
[2024-01-22 09:30:19,470] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06719265434890985
[2024-01-22 09:30:22,060] INFO: Iter 600 Summary: 
[2024-01-22 09:30:22,060] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06574191194027662
[2024-01-22 09:30:24,583] INFO: Iter 700 Summary: 
[2024-01-22 09:30:24,583] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06505372971296311
[2024-01-22 09:30:27,140] INFO: Iter 800 Summary: 
[2024-01-22 09:30:27,141] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06387996215373277
[2024-01-22 09:30:29,873] INFO: Iter 900 Summary: 
[2024-01-22 09:30:29,874] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0637226140126586
[2024-01-22 09:30:32,473] INFO: Iter 1000 Summary: 
[2024-01-22 09:30:32,473] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06281336572021246
[2024-01-22 09:30:34,943] INFO: Iter 1100 Summary: 
[2024-01-22 09:30:34,943] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.062083161622285846
[2024-01-22 09:30:37,481] INFO: Iter 1200 Summary: 
[2024-01-22 09:30:37,481] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0616617776080966
[2024-01-22 09:30:40,127] INFO: Iter 1300 Summary: 
[2024-01-22 09:30:40,127] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06138411946594715
[2024-01-22 09:30:42,648] INFO: Iter 1400 Summary: 
[2024-01-22 09:30:42,649] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06102396670728922
[2024-01-22 09:30:45,469] INFO: Iter 1500 Summary: 
[2024-01-22 09:30:45,470] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06052622187882662
[2024-01-22 09:30:48,033] INFO: Iter 1600 Summary: 
[2024-01-22 09:30:48,033] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.060147434398531915
[2024-01-22 09:30:50,705] INFO: Iter 1700 Summary: 
[2024-01-22 09:30:50,705] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05966938678175211
[2024-01-22 09:30:53,341] INFO: Iter 1800 Summary: 
[2024-01-22 09:30:53,341] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05860090240836144
[2024-01-22 09:30:56,060] INFO: Iter 1900 Summary: 
[2024-01-22 09:30:56,060] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05886689271777868
[2024-01-22 09:30:58,630] INFO: Iter 2000 Summary: 
[2024-01-22 09:30:58,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.058561157025396826
[2024-01-22 09:31:01,261] INFO: Iter 2100 Summary: 
[2024-01-22 09:31:01,261] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.058301390409469606
[2024-01-22 09:31:03,781] INFO: Iter 2200 Summary: 
[2024-01-22 09:31:03,781] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05770072065293789
[2024-01-22 09:31:06,289] INFO: Iter 2300 Summary: 
[2024-01-22 09:31:06,289] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05735681269317865
[2024-01-22 09:31:08,850] INFO: Iter 2400 Summary: 
[2024-01-22 09:31:08,850] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.057302918545901776
[2024-01-22 09:31:11,337] INFO: Iter 2500 Summary: 
[2024-01-22 09:31:11,337] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05703567836433649
[2024-01-22 09:31:13,995] INFO: Iter 2600 Summary: 
[2024-01-22 09:31:13,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05690142896026373
[2024-01-22 09:31:16,587] INFO: Iter 2700 Summary: 
[2024-01-22 09:31:16,587] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.056595995798707006
[2024-01-22 09:31:19,395] INFO: Iter 2800 Summary: 
[2024-01-22 09:31:19,395] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.056499514020979406
[2024-01-22 09:31:21,984] INFO: Iter 2900 Summary: 
[2024-01-22 09:31:21,985] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05616317145526409
[2024-01-22 09:31:24,661] INFO: Iter 3000 Summary: 
[2024-01-22 09:31:24,661] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055999592058360574
[2024-01-22 09:31:27,051] INFO: Iter 3100 Summary: 
[2024-01-22 09:31:27,051] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055527230724692345
[2024-01-22 09:31:29,713] INFO: Iter 3200 Summary: 
[2024-01-22 09:31:29,713] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055334230996668336
[2024-01-22 09:31:32,392] INFO: Iter 3300 Summary: 
[2024-01-22 09:31:32,393] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05524085976183415
[2024-01-22 09:31:34,966] INFO: Iter 3400 Summary: 
[2024-01-22 09:31:34,966] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05536742754280567
[2024-01-22 09:31:37,576] INFO: Iter 3500 Summary: 
[2024-01-22 09:31:37,576] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05535300519317388
[2024-01-22 09:31:40,119] INFO: Iter 3600 Summary: 
[2024-01-22 09:31:40,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05545871268957853
[2024-01-22 09:31:42,752] INFO: Iter 3700 Summary: 
[2024-01-22 09:31:42,752] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05522134453058243
[2024-01-22 09:31:45,419] INFO: Iter 3800 Summary: 
[2024-01-22 09:31:45,419] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05495358947664499
[2024-01-22 09:31:47,975] INFO: Iter 3900 Summary: 
[2024-01-22 09:31:47,975] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054538472704589364
[2024-01-22 09:31:50,634] INFO: Iter 4000 Summary: 
[2024-01-22 09:31:50,634] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05458749920129776
[2024-01-22 09:31:53,316] INFO: Iter 4100 Summary: 
[2024-01-22 09:31:53,316] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054525446929037574
[2024-01-22 09:31:55,991] INFO: Iter 4200 Summary: 
[2024-01-22 09:31:55,991] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054882142841815945
[2024-01-22 09:31:58,564] INFO: Iter 4300 Summary: 
[2024-01-22 09:31:58,565] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05440907523036003
[2024-01-22 09:32:01,053] INFO: Iter 4400 Summary: 
[2024-01-22 09:32:01,053] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054322384409606454
[2024-01-22 09:32:03,634] INFO: Iter 4500 Summary: 
[2024-01-22 09:32:03,634] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05415802184492349
[2024-01-22 09:32:06,272] INFO: Iter 4600 Summary: 
[2024-01-22 09:32:06,272] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05454121269285679
[2024-01-22 09:32:08,936] INFO: Iter 4700 Summary: 
[2024-01-22 09:32:08,936] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05315664891153574
[2024-01-22 09:32:11,444] INFO: Iter 4800 Summary: 
[2024-01-22 09:32:11,444] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05389671057462692
[2024-01-22 09:32:14,047] INFO: Iter 4900 Summary: 
[2024-01-22 09:32:14,047] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05370066542178392
[2024-01-22 09:32:16,858] INFO: Iter 5000 Summary: 
[2024-01-22 09:32:16,858] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053745503947138784
[2024-01-22 09:32:19,326] INFO: Iter 5100 Summary: 
[2024-01-22 09:32:19,326] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05384746540337801
[2024-01-22 09:32:22,012] INFO: Iter 5200 Summary: 
[2024-01-22 09:32:22,012] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05329228848218918
[2024-01-22 09:32:24,588] INFO: Iter 5300 Summary: 
[2024-01-22 09:32:24,588] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05300307709723711
[2024-01-22 09:32:27,157] INFO: Iter 5400 Summary: 
[2024-01-22 09:32:27,157] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05336924072355032
[2024-01-22 09:32:29,851] INFO: Iter 5500 Summary: 
[2024-01-22 09:32:29,851] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053297074735164644
[2024-01-22 09:32:32,532] INFO: Iter 5600 Summary: 
[2024-01-22 09:32:32,532] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05319382280111313
[2024-01-22 09:32:35,148] INFO: Iter 5700 Summary: 
[2024-01-22 09:32:35,148] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05272808615118265
[2024-01-22 09:32:37,671] INFO: Iter 5800 Summary: 
[2024-01-22 09:32:37,671] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052385757565498355
[2024-01-22 09:32:40,453] INFO: Iter 5900 Summary: 
[2024-01-22 09:32:40,453] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052420998811721804
[2024-01-22 09:32:43,089] INFO: Iter 6000 Summary: 
[2024-01-22 09:32:43,089] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052416791953146455
[2024-01-22 09:32:45,670] INFO: Iter 6100 Summary: 
[2024-01-22 09:32:45,670] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05249941978603601
[2024-01-22 09:32:48,410] INFO: Iter 6200 Summary: 
[2024-01-22 09:32:48,410] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05203603569418192
[2024-01-22 09:32:51,011] INFO: Iter 6300 Summary: 
[2024-01-22 09:32:51,011] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05256746016442776
[2024-01-22 09:32:53,697] INFO: Iter 6400 Summary: 
[2024-01-22 09:32:53,697] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05181027941405773
[2024-01-22 09:32:56,359] INFO: Iter 6500 Summary: 
[2024-01-22 09:32:56,359] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05222161553800106
[2024-01-22 09:32:59,042] INFO: Iter 6600 Summary: 
[2024-01-22 09:32:59,042] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051937056481838224
[2024-01-22 09:33:01,798] INFO: Iter 6700 Summary: 
[2024-01-22 09:33:01,798] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05214205823838711
[2024-01-22 09:33:04,269] INFO: Iter 6800 Summary: 
[2024-01-22 09:33:04,269] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05179669953882694
[2024-01-22 09:33:06,918] INFO: Iter 6900 Summary: 
[2024-01-22 09:33:06,918] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05189048957079649
[2024-01-22 09:33:09,641] INFO: Iter 7000 Summary: 
[2024-01-22 09:33:09,641] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05168100520968437
[2024-01-22 09:33:12,228] INFO: Iter 7100 Summary: 
[2024-01-22 09:33:12,228] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051688286364078524
[2024-01-22 09:33:14,844] INFO: Iter 7200 Summary: 
[2024-01-22 09:33:14,844] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0516482587531209
[2024-01-22 09:33:17,347] INFO: Iter 7300 Summary: 
[2024-01-22 09:33:17,347] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051294325962662694
[2024-01-22 09:33:20,005] INFO: Iter 7400 Summary: 
[2024-01-22 09:33:20,005] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051271435990929606
[2024-01-22 09:33:22,693] INFO: Iter 7500 Summary: 
[2024-01-22 09:33:22,693] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05149308450520038
[2024-01-22 09:33:25,305] INFO: Iter 7600 Summary: 
[2024-01-22 09:33:25,306] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051288662366569045
[2024-01-22 09:33:27,895] INFO: Iter 7700 Summary: 
[2024-01-22 09:33:27,895] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05132615584880114
[2024-01-22 09:33:30,403] INFO: Iter 7800 Summary: 
[2024-01-22 09:33:30,403] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05127264615148306
[2024-01-22 09:33:32,999] INFO: Iter 7900 Summary: 
[2024-01-22 09:33:32,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05089722163975239
[2024-01-22 09:33:35,643] INFO: Iter 8000 Summary: 
[2024-01-22 09:33:35,643] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0510490308329463
[2024-01-22 09:33:38,275] INFO: Iter 8100 Summary: 
[2024-01-22 09:33:38,275] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05074500098824501
[2024-01-22 09:33:40,995] INFO: Iter 8200 Summary: 
[2024-01-22 09:33:40,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050944002158939836
[2024-01-22 09:33:43,548] INFO: Iter 8300 Summary: 
[2024-01-22 09:33:43,549] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05073196563869715
[2024-01-22 09:33:46,261] INFO: Iter 8400 Summary: 
[2024-01-22 09:33:46,261] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05042639281600714
[2024-01-22 09:33:48,897] INFO: Iter 8500 Summary: 
[2024-01-22 09:33:48,897] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05078078698366881
[2024-01-22 09:33:51,631] INFO: Iter 8600 Summary: 
[2024-01-22 09:33:51,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0506459628790617
[2024-01-22 09:33:54,269] INFO: Iter 8700 Summary: 
[2024-01-22 09:33:54,269] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050339547209441664
[2024-01-22 09:33:56,958] INFO: Iter 8800 Summary: 
[2024-01-22 09:33:56,959] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05044893391430378
[2024-01-22 09:33:59,625] INFO: Iter 8900 Summary: 
[2024-01-22 09:33:59,625] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050757073760032655
[2024-01-22 09:34:02,347] INFO: Iter 9000 Summary: 
[2024-01-22 09:34:02,347] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05015923582017422
[2024-01-22 09:34:04,913] INFO: Iter 9100 Summary: 
[2024-01-22 09:34:04,914] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05024785097688436
[2024-01-22 09:34:07,517] INFO: Iter 9200 Summary: 
[2024-01-22 09:34:07,517] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05028524521738291
[2024-01-22 09:34:10,304] INFO: Iter 9300 Summary: 
[2024-01-22 09:34:10,305] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050132164172828196
[2024-01-22 09:34:12,888] INFO: Iter 9400 Summary: 
[2024-01-22 09:34:12,888] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05003090996295214
[2024-01-22 09:34:15,366] INFO: Iter 9500 Summary: 
[2024-01-22 09:34:15,366] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05031575903296471
[2024-01-22 09:34:17,992] INFO: Iter 9600 Summary: 
[2024-01-22 09:34:17,992] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04984377797693014
[2024-01-22 09:34:20,631] INFO: Iter 9700 Summary: 
[2024-01-22 09:34:20,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04972668878734112
[2024-01-22 09:34:23,055] INFO: Iter 9800 Summary: 
[2024-01-22 09:34:23,055] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0502498010918498
[2024-01-22 09:34:25,559] INFO: Iter 9900 Summary: 
[2024-01-22 09:34:25,559] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05023289181292057
[2024-01-22 09:34:28,146] INFO: Iter 10000 Summary: 
[2024-01-22 09:34:28,146] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04940365295857191
[2024-01-22 09:34:30,805] INFO: Iter 10100 Summary: 
[2024-01-22 09:34:30,805] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04971793238073587
[2024-01-22 09:34:33,397] INFO: Iter 10200 Summary: 
[2024-01-22 09:34:33,397] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04958397377282381
[2024-01-22 09:34:36,151] INFO: Iter 10300 Summary: 
[2024-01-22 09:34:36,151] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0499732095003128
[2024-01-22 09:34:38,941] INFO: Iter 10400 Summary: 
[2024-01-22 09:34:38,942] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04935356270521879
[2024-01-22 09:34:41,538] INFO: Iter 10500 Summary: 
[2024-01-22 09:34:41,538] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04962785564363003
[2024-01-22 09:34:44,157] INFO: Iter 10600 Summary: 
[2024-01-22 09:34:44,158] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04960039466619492
[2024-01-22 09:34:46,726] INFO: Iter 10700 Summary: 
[2024-01-22 09:34:46,727] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04932211313396692
[2024-01-22 09:34:49,165] INFO: Iter 10800 Summary: 
[2024-01-22 09:34:49,165] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049294187650084496
[2024-01-22 09:34:51,901] INFO: Iter 10900 Summary: 
[2024-01-22 09:34:51,901] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049128708094358445
[2024-01-22 09:34:54,550] INFO: Iter 11000 Summary: 
[2024-01-22 09:34:54,550] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049283920340240005
[2024-01-22 09:34:57,189] INFO: Iter 11100 Summary: 
[2024-01-22 09:34:57,189] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04923346657305956
[2024-01-22 09:34:59,776] INFO: Iter 11200 Summary: 
[2024-01-22 09:34:59,776] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04909020368009806
[2024-01-22 09:35:02,313] INFO: Iter 11300 Summary: 
[2024-01-22 09:35:02,313] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04895477265119553
[2024-01-22 09:35:04,947] INFO: Iter 11400 Summary: 
[2024-01-22 09:35:04,947] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04870882194489241
[2024-01-22 09:35:07,297] INFO: Iter 11500 Summary: 
[2024-01-22 09:35:07,297] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04893469039350748
[2024-01-22 09:35:09,888] INFO: Iter 11600 Summary: 
[2024-01-22 09:35:09,888] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049438013471662996
[2024-01-22 09:35:12,509] INFO: Iter 11700 Summary: 
[2024-01-22 09:35:12,509] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04885612174868584
[2024-01-22 09:35:15,023] INFO: Iter 11800 Summary: 
[2024-01-22 09:35:15,023] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04872216165065765
[2024-01-22 09:35:17,536] INFO: Iter 11900 Summary: 
[2024-01-22 09:35:17,536] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04894314873963594
[2024-01-22 09:35:20,109] INFO: Iter 12000 Summary: 
[2024-01-22 09:35:20,109] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048847691304981705
[2024-01-22 09:35:23,036] INFO: Iter 12100 Summary: 
[2024-01-22 09:35:23,036] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04850454393774271
[2024-01-22 09:35:25,650] INFO: Iter 12200 Summary: 
[2024-01-22 09:35:25,650] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048578016720712186
[2024-01-22 09:35:28,292] INFO: Iter 12300 Summary: 
[2024-01-22 09:35:28,293] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.049010972380638125
[2024-01-22 09:35:31,021] INFO: Iter 12400 Summary: 
[2024-01-22 09:35:31,021] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04804294139146805
[2024-01-22 09:35:33,744] INFO: Iter 12500 Summary: 
[2024-01-22 09:35:33,744] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048570802472531795
[2024-01-22 09:35:36,462] INFO: Iter 12600 Summary: 
[2024-01-22 09:35:36,462] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04848194181919098
[2024-01-22 09:35:39,217] INFO: Iter 12700 Summary: 
[2024-01-22 09:35:39,217] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048214729428291324
[2024-01-22 09:35:41,785] INFO: Iter 12800 Summary: 
[2024-01-22 09:35:41,786] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048867131061851976
[2024-01-22 09:35:44,424] INFO: Iter 12900 Summary: 
[2024-01-22 09:35:44,424] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04855127729475498
[2024-01-22 09:35:47,068] INFO: Iter 13000 Summary: 
[2024-01-22 09:35:47,069] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048624868094921114
[2024-01-22 09:35:49,644] INFO: Iter 13100 Summary: 
[2024-01-22 09:35:49,644] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04819885078817606
[2024-01-22 09:35:52,194] INFO: Iter 13200 Summary: 
[2024-01-22 09:35:52,194] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04826186828315258
[2024-01-22 09:35:54,809] INFO: Iter 13300 Summary: 
[2024-01-22 09:35:54,809] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04800314459949732
[2024-01-22 09:35:57,456] INFO: Iter 13400 Summary: 
[2024-01-22 09:35:57,456] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04818452011793852
[2024-01-22 09:36:00,061] INFO: Iter 13500 Summary: 
[2024-01-22 09:36:00,061] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04821053855121136
[2024-01-22 09:36:02,752] INFO: Iter 13600 Summary: 
[2024-01-22 09:36:02,752] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04818660482764244
[2024-01-22 09:36:05,441] INFO: Iter 13700 Summary: 
[2024-01-22 09:36:05,441] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048280311860144136
[2024-01-22 09:36:07,977] INFO: Iter 13800 Summary: 
[2024-01-22 09:36:07,977] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048112309873104095
[2024-01-22 09:36:10,494] INFO: Iter 13900 Summary: 
[2024-01-22 09:36:10,494] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04780868019908666
[2024-01-22 09:36:13,144] INFO: Iter 14000 Summary: 
[2024-01-22 09:36:13,144] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04775915287435055
[2024-01-22 09:36:15,747] INFO: Iter 14100 Summary: 
[2024-01-22 09:36:15,747] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0482143447548151
[2024-01-22 09:36:18,301] INFO: Iter 14200 Summary: 
[2024-01-22 09:36:18,301] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047946917973458764
[2024-01-22 09:36:21,070] INFO: Iter 14300 Summary: 
[2024-01-22 09:36:21,070] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04755120240151882
[2024-01-22 09:36:23,772] INFO: Iter 14400 Summary: 
[2024-01-22 09:36:23,772] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04762171436101198
[2024-01-22 09:36:26,459] INFO: Iter 14500 Summary: 
[2024-01-22 09:36:26,459] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.048141405694186686
[2024-01-22 09:36:29,175] INFO: Iter 14600 Summary: 
[2024-01-22 09:36:29,176] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04781522188335657
[2024-01-22 09:36:31,819] INFO: Iter 14700 Summary: 
[2024-01-22 09:36:31,819] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047816805429756644
[2024-01-22 09:36:34,493] INFO: Iter 14800 Summary: 
[2024-01-22 09:36:34,493] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04786643236875534
[2024-01-22 09:36:37,114] INFO: Iter 14900 Summary: 
[2024-01-22 09:36:37,114] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04767505541443825
[2024-01-22 09:36:39,795] INFO: Iter 15000 Summary: 
[2024-01-22 09:36:39,795] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04766767282038927
[2024-01-22 09:36:42,445] INFO: Iter 15100 Summary: 
[2024-01-22 09:36:42,446] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04750018306076527
[2024-01-22 09:36:45,035] INFO: Iter 15200 Summary: 
[2024-01-22 09:36:45,035] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04776121210306883
[2024-01-22 09:36:47,789] INFO: Iter 15300 Summary: 
[2024-01-22 09:36:47,789] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047616570219397546
[2024-01-22 09:36:50,440] INFO: Iter 15400 Summary: 
[2024-01-22 09:36:50,440] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04791520193219185
[2024-01-22 09:36:53,055] INFO: Iter 15500 Summary: 
[2024-01-22 09:36:53,055] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04745046995580196
[2024-01-22 09:36:55,520] INFO: Iter 15600 Summary: 
[2024-01-22 09:36:55,520] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04739225514233112
[2024-01-22 09:36:58,190] INFO: Iter 15700 Summary: 
[2024-01-22 09:36:58,190] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04756820637732744
[2024-01-22 09:37:00,866] INFO: Iter 15800 Summary: 
[2024-01-22 09:37:00,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047446914538741114
[2024-01-22 09:37:03,368] INFO: Iter 15900 Summary: 
[2024-01-22 09:37:03,368] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04759765766561031
[2024-01-22 09:37:06,091] INFO: Iter 16000 Summary: 
[2024-01-22 09:37:06,091] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04708791017532349
[2024-01-22 09:37:08,780] INFO: Iter 16100 Summary: 
[2024-01-22 09:37:08,780] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04725845649838448
[2024-01-22 09:37:11,461] INFO: Iter 16200 Summary: 
[2024-01-22 09:37:11,461] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04692946847528219
[2024-01-22 09:37:14,144] INFO: Iter 16300 Summary: 
[2024-01-22 09:37:14,144] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047404594570398334
[2024-01-22 09:37:16,693] INFO: Iter 16400 Summary: 
[2024-01-22 09:37:16,693] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04731134958565235
[2024-01-22 09:37:19,273] INFO: Iter 16500 Summary: 
[2024-01-22 09:37:19,273] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047336722649633886
[2024-01-22 09:37:21,744] INFO: Iter 16600 Summary: 
[2024-01-22 09:37:21,744] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047363260686397554
[2024-01-22 09:37:24,238] INFO: Iter 16700 Summary: 
[2024-01-22 09:37:24,238] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04694462060928345
[2024-01-22 09:37:26,962] INFO: Iter 16800 Summary: 
[2024-01-22 09:37:26,962] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047078523486852646
[2024-01-22 09:37:29,559] INFO: Iter 16900 Summary: 
[2024-01-22 09:37:29,559] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04731229394674301
[2024-01-22 09:37:32,250] INFO: Iter 17000 Summary: 
[2024-01-22 09:37:32,250] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04721838165074587
[2024-01-22 09:37:34,945] INFO: Iter 17100 Summary: 
[2024-01-22 09:37:34,945] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04766858011484146
[2024-01-22 09:37:37,729] INFO: Iter 17200 Summary: 
[2024-01-22 09:37:37,729] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04751962911337614
[2024-01-22 09:37:40,135] INFO: Iter 17300 Summary: 
[2024-01-22 09:37:40,136] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046972572393715384
[2024-01-22 09:37:42,796] INFO: Iter 17400 Summary: 
[2024-01-22 09:37:42,796] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04677107140421868
[2024-01-22 09:37:45,412] INFO: Iter 17500 Summary: 
[2024-01-22 09:37:45,412] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04666114568710327
[2024-01-22 09:37:47,992] INFO: Iter 17600 Summary: 
[2024-01-22 09:37:47,992] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04714560203254223
[2024-01-22 09:37:50,584] INFO: Iter 17700 Summary: 
[2024-01-22 09:37:50,584] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04697451386600733
[2024-01-22 09:37:53,253] INFO: Iter 17800 Summary: 
[2024-01-22 09:37:53,253] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047010385766625404
[2024-01-22 09:37:55,800] INFO: Iter 17900 Summary: 
[2024-01-22 09:37:55,800] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.047043323963880536
[2024-01-22 09:37:58,274] INFO: Iter 18000 Summary: 
[2024-01-22 09:37:58,274] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04669398721307516
[2024-01-22 09:38:00,930] INFO: Iter 18100 Summary: 
[2024-01-22 09:38:00,930] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046872284039855
[2024-01-22 09:38:03,546] INFO: Iter 18200 Summary: 
[2024-01-22 09:38:03,546] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046977019719779495
[2024-01-22 09:38:05,964] INFO: Iter 18300 Summary: 
[2024-01-22 09:38:05,965] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04669479090720415
[2024-01-22 09:38:08,683] INFO: Iter 18400 Summary: 
[2024-01-22 09:38:08,683] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04660618666559458
[2024-01-22 09:38:11,289] INFO: Iter 18500 Summary: 
[2024-01-22 09:38:11,289] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046604659594595435
[2024-01-22 09:38:13,810] INFO: Iter 18600 Summary: 
[2024-01-22 09:38:13,810] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04698157519102097
[2024-01-22 09:38:16,468] INFO: Iter 18700 Summary: 
[2024-01-22 09:38:16,469] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04657864764332771
[2024-01-22 09:38:19,218] INFO: Iter 18800 Summary: 
[2024-01-22 09:38:19,219] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04672066327184439
[2024-01-22 09:38:21,620] INFO: Iter 18900 Summary: 
[2024-01-22 09:38:21,620] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04679304279386997
[2024-01-22 09:38:24,239] INFO: Iter 19000 Summary: 
[2024-01-22 09:38:24,240] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04658994991332292
[2024-01-22 09:38:26,922] INFO: Iter 19100 Summary: 
[2024-01-22 09:38:26,922] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046428651586174964
[2024-01-22 09:38:29,394] INFO: Iter 19200 Summary: 
[2024-01-22 09:38:29,394] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046345857642591
[2024-01-22 09:38:31,954] INFO: Iter 19300 Summary: 
[2024-01-22 09:38:31,954] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04674255929887295
[2024-01-22 09:38:34,435] INFO: Iter 19400 Summary: 
[2024-01-22 09:38:34,435] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04607490912079811
[2024-01-22 09:38:37,084] INFO: Iter 19500 Summary: 
[2024-01-22 09:38:37,084] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04646977845579386
[2024-01-22 09:38:39,747] INFO: Iter 19600 Summary: 
[2024-01-22 09:38:39,747] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04653589315712452
[2024-01-22 09:38:42,371] INFO: Iter 19700 Summary: 
[2024-01-22 09:38:42,371] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046354604996740816
[2024-01-22 09:38:45,063] INFO: Iter 19800 Summary: 
[2024-01-22 09:38:45,063] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04678636278957129
[2024-01-22 09:38:47,765] INFO: Iter 19900 Summary: 
[2024-01-22 09:38:47,765] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04614385858178139
[2024-01-22 09:38:50,270] INFO: Iter 20000 Summary: 
[2024-01-22 09:38:50,270] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04668197851628065
[2024-01-22 09:38:52,822] INFO: Iter 20100 Summary: 
[2024-01-22 09:38:52,822] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046057703234255315
[2024-01-22 09:38:55,439] INFO: Iter 20200 Summary: 
[2024-01-22 09:38:55,439] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0464134531468153
[2024-01-22 09:38:57,935] INFO: Iter 20300 Summary: 
[2024-01-22 09:38:57,935] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04638200633227825
[2024-01-22 09:39:00,433] INFO: Iter 20400 Summary: 
[2024-01-22 09:39:00,434] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04609065841883421
[2024-01-22 09:39:03,008] INFO: Iter 20500 Summary: 
[2024-01-22 09:39:03,008] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04635730244219303
[2024-01-22 09:39:05,552] INFO: Iter 20600 Summary: 
[2024-01-22 09:39:05,552] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046409811601042746
[2024-01-22 09:39:08,310] INFO: Iter 20700 Summary: 
[2024-01-22 09:39:08,310] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046703166775405404
[2024-01-22 09:39:10,960] INFO: Iter 20800 Summary: 
[2024-01-22 09:39:10,961] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046175027117133144
[2024-01-22 09:39:13,672] INFO: Iter 20900 Summary: 
[2024-01-22 09:39:13,673] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04623736076056957
[2024-01-22 09:39:16,323] INFO: Iter 21000 Summary: 
[2024-01-22 09:39:16,323] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04630418557673693
[2024-01-22 09:39:19,073] INFO: Iter 21100 Summary: 
[2024-01-22 09:39:19,073] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04642226785421372
[2024-01-22 09:39:21,797] INFO: Iter 21200 Summary: 
[2024-01-22 09:39:21,797] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04596228566020727
[2024-01-22 09:39:24,343] INFO: Iter 21300 Summary: 
[2024-01-22 09:39:24,343] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04625927120447159
[2024-01-22 09:39:27,074] INFO: Iter 21400 Summary: 
[2024-01-22 09:39:27,075] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04598068207502365
[2024-01-22 09:39:29,691] INFO: Iter 21500 Summary: 
[2024-01-22 09:39:29,691] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04571697879582644
[2024-01-22 09:39:32,111] INFO: Iter 21600 Summary: 
[2024-01-22 09:39:32,111] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045717478208243845
[2024-01-22 09:39:34,713] INFO: Iter 21700 Summary: 
[2024-01-22 09:39:34,713] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04630730319768191
[2024-01-22 09:39:37,391] INFO: Iter 21800 Summary: 
[2024-01-22 09:39:37,392] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045943669825792316
[2024-01-22 09:39:40,171] INFO: Iter 21900 Summary: 
[2024-01-22 09:39:40,171] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04565697532147169
[2024-01-22 09:39:42,853] INFO: Iter 22000 Summary: 
[2024-01-22 09:39:42,853] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04599916432052851
[2024-01-22 09:39:45,545] INFO: Iter 22100 Summary: 
[2024-01-22 09:39:45,545] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046455191783607006
[2024-01-22 09:39:48,253] INFO: Iter 22200 Summary: 
[2024-01-22 09:39:48,253] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04618005182594061
[2024-01-22 09:39:50,809] INFO: Iter 22300 Summary: 
[2024-01-22 09:39:50,809] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04569714240729809
[2024-01-22 09:39:53,393] INFO: Iter 22400 Summary: 
[2024-01-22 09:39:53,393] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04581700555980205
[2024-01-22 09:39:55,994] INFO: Iter 22500 Summary: 
[2024-01-22 09:39:55,995] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04625257901847363
[2024-01-22 09:39:58,711] INFO: Iter 22600 Summary: 
[2024-01-22 09:39:58,711] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04612680494785309
[2024-01-22 09:40:01,234] INFO: Iter 22700 Summary: 
[2024-01-22 09:40:01,234] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045938281901180746
[2024-01-22 09:40:03,793] INFO: Iter 22800 Summary: 
[2024-01-22 09:40:03,793] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04597247593104839
[2024-01-22 09:40:06,447] INFO: Iter 22900 Summary: 
[2024-01-22 09:40:06,447] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0459106532484293
[2024-01-22 09:40:09,013] INFO: Iter 23000 Summary: 
[2024-01-22 09:40:09,013] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046013395600020886
[2024-01-22 09:40:11,779] INFO: Iter 23100 Summary: 
[2024-01-22 09:40:11,779] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04620418965816498
[2024-01-22 09:40:14,412] INFO: Iter 23200 Summary: 
[2024-01-22 09:40:14,412] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04598819125443697
[2024-01-22 09:40:16,962] INFO: Iter 23300 Summary: 
[2024-01-22 09:40:16,962] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04586262840777636
[2024-01-22 09:40:19,683] INFO: Iter 23400 Summary: 
[2024-01-22 09:40:19,683] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04595125216990709
[2024-01-22 09:40:22,260] INFO: Iter 23500 Summary: 
[2024-01-22 09:40:22,260] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04552149340510368
[2024-01-22 09:40:24,834] INFO: Iter 23600 Summary: 
[2024-01-22 09:40:24,834] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045946400985121726
[2024-01-22 09:40:27,544] INFO: Iter 23700 Summary: 
[2024-01-22 09:40:27,544] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04604402121156454
[2024-01-22 09:40:30,141] INFO: Iter 23800 Summary: 
[2024-01-22 09:40:30,141] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0458093785867095
[2024-01-22 09:40:32,758] INFO: Iter 23900 Summary: 
[2024-01-22 09:40:32,759] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045799940377473834
[2024-01-22 09:40:35,296] INFO: Iter 24000 Summary: 
[2024-01-22 09:40:35,296] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.046189581826329235
[2024-01-22 09:40:37,985] INFO: Iter 24100 Summary: 
[2024-01-22 09:40:37,985] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04575140494853258
[2024-01-22 09:40:40,560] INFO: Iter 24200 Summary: 
[2024-01-22 09:40:40,560] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04555966950953007
[2024-01-22 09:40:43,196] INFO: Iter 24300 Summary: 
[2024-01-22 09:40:43,196] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04583658784627914
[2024-01-22 09:40:45,769] INFO: Iter 24400 Summary: 
[2024-01-22 09:40:45,769] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04583479553461075
[2024-01-22 09:40:48,416] INFO: Iter 24500 Summary: 
[2024-01-22 09:40:48,416] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04570646606385708
[2024-01-22 09:40:51,069] INFO: Iter 24600 Summary: 
[2024-01-22 09:40:51,069] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04563846670091152
[2024-01-22 09:40:53,543] INFO: Iter 24700 Summary: 
[2024-01-22 09:40:53,543] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04563422106206417
[2024-01-22 09:40:56,219] INFO: Iter 24800 Summary: 
[2024-01-22 09:40:56,219] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04576201792806387
[2024-01-22 09:40:58,738] INFO: Iter 24900 Summary: 
[2024-01-22 09:40:58,738] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04560079462826252
[2024-01-22 09:41:01,262] INFO: Iter 25000 Summary: 
[2024-01-22 09:41:01,263] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04549538891762495
[2024-01-22 09:41:03,857] INFO: Iter 25100 Summary: 
[2024-01-22 09:41:03,857] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04573417622596025
[2024-01-22 09:41:06,601] INFO: Iter 25200 Summary: 
[2024-01-22 09:41:06,601] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045749893113970756
[2024-01-22 09:41:09,188] INFO: Iter 25300 Summary: 
[2024-01-22 09:41:09,188] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045139895528554914
[2024-01-22 09:41:11,754] INFO: Iter 25400 Summary: 
[2024-01-22 09:41:11,754] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04558750059455633
[2024-01-22 09:41:14,287] INFO: Iter 25500 Summary: 
[2024-01-22 09:41:14,287] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04537296984344721
[2024-01-22 09:41:16,908] INFO: Iter 25600 Summary: 
[2024-01-22 09:41:16,908] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04551208395510912
[2024-01-22 09:41:19,375] INFO: Iter 25700 Summary: 
[2024-01-22 09:41:19,376] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04541583016514778
[2024-01-22 09:41:21,990] INFO: Iter 25800 Summary: 
[2024-01-22 09:41:21,990] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04529512245208025
[2024-01-22 09:41:24,536] INFO: Iter 25900 Summary: 
[2024-01-22 09:41:24,536] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045707711465656754
[2024-01-22 09:41:27,040] INFO: Iter 26000 Summary: 
[2024-01-22 09:41:27,041] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04542345136404038
[2024-01-22 09:41:29,732] INFO: Iter 26100 Summary: 
[2024-01-22 09:41:29,733] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045643352903425694
[2024-01-22 09:41:32,312] INFO: Iter 26200 Summary: 
[2024-01-22 09:41:32,312] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045296002440154554
[2024-01-22 09:41:34,840] INFO: Iter 26300 Summary: 
[2024-01-22 09:41:34,841] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04538277741521597
[2024-01-22 09:41:37,359] INFO: Iter 26400 Summary: 
[2024-01-22 09:41:37,359] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04556284010410309
[2024-01-22 09:41:40,080] INFO: Iter 26500 Summary: 
[2024-01-22 09:41:40,080] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04593249954283238
[2024-01-22 09:41:42,908] INFO: Iter 26600 Summary: 
[2024-01-22 09:41:42,908] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045496191754937174
[2024-01-22 09:41:45,415] INFO: Iter 26700 Summary: 
[2024-01-22 09:41:45,416] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04526908934116364
[2024-01-22 09:41:48,111] INFO: Iter 26800 Summary: 
[2024-01-22 09:41:48,111] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04570112217217684
[2024-01-22 09:41:50,742] INFO: Iter 26900 Summary: 
[2024-01-22 09:41:50,742] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04505881372839213
[2024-01-22 09:41:53,222] INFO: Iter 27000 Summary: 
[2024-01-22 09:41:53,223] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04547623801976442
[2024-01-22 09:41:55,899] INFO: Iter 27100 Summary: 
[2024-01-22 09:41:55,900] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04527592301368713
[2024-01-22 09:41:58,407] INFO: Iter 27200 Summary: 
[2024-01-22 09:41:58,407] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045313973277807236
[2024-01-22 09:42:01,062] INFO: Iter 27300 Summary: 
[2024-01-22 09:42:01,063] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04543252982199192
[2024-01-22 09:42:03,658] INFO: Iter 27400 Summary: 
[2024-01-22 09:42:03,658] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04539033140987158
[2024-01-22 09:42:06,185] INFO: Iter 27500 Summary: 
[2024-01-22 09:42:06,185] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04525410611182451
[2024-01-22 09:42:08,829] INFO: Iter 27600 Summary: 
[2024-01-22 09:42:08,829] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045349414311349395
[2024-01-22 09:42:11,309] INFO: Iter 27700 Summary: 
[2024-01-22 09:42:11,309] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04511484257876873
[2024-01-22 09:42:13,810] INFO: Iter 27800 Summary: 
[2024-01-22 09:42:13,810] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045300803557038305
[2024-01-22 09:42:16,514] INFO: Iter 27900 Summary: 
[2024-01-22 09:42:16,514] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04545472476631403
[2024-01-22 09:42:19,181] INFO: Iter 28000 Summary: 
[2024-01-22 09:42:19,181] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0450011782720685
[2024-01-22 09:42:21,795] INFO: Iter 28100 Summary: 
[2024-01-22 09:42:21,795] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0450972905382514
[2024-01-22 09:42:24,308] INFO: Iter 28200 Summary: 
[2024-01-22 09:42:24,308] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045218861028552056
[2024-01-22 09:42:26,961] INFO: Iter 28300 Summary: 
[2024-01-22 09:42:26,961] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04520492948591709
[2024-01-22 09:42:29,553] INFO: Iter 28400 Summary: 
[2024-01-22 09:42:29,553] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04501065138727427
[2024-01-22 09:42:32,290] INFO: Iter 28500 Summary: 
[2024-01-22 09:42:32,291] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0452198551595211
[2024-01-22 09:42:34,958] INFO: Iter 28600 Summary: 
[2024-01-22 09:42:34,958] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04532491285353899
[2024-01-22 09:42:37,438] INFO: Iter 28700 Summary: 
[2024-01-22 09:42:37,438] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045126370228827
[2024-01-22 09:42:40,132] INFO: Iter 28800 Summary: 
[2024-01-22 09:42:40,133] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04527148723602295
[2024-01-22 09:42:42,755] INFO: Iter 28900 Summary: 
[2024-01-22 09:42:42,755] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04517772547900677
[2024-01-22 09:42:45,487] INFO: Iter 29000 Summary: 
[2024-01-22 09:42:45,487] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04536615733057261
[2024-01-22 09:42:47,958] INFO: Iter 29100 Summary: 
[2024-01-22 09:42:47,958] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044938361719250676
[2024-01-22 09:42:50,581] INFO: Iter 29200 Summary: 
[2024-01-22 09:42:50,581] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04517381470650435
[2024-01-22 09:42:53,098] INFO: Iter 29300 Summary: 
[2024-01-22 09:42:53,098] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045126778483390806
[2024-01-22 09:42:55,597] INFO: Iter 29400 Summary: 
[2024-01-22 09:42:55,597] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045341355316340926
[2024-01-22 09:42:58,098] INFO: Iter 29500 Summary: 
[2024-01-22 09:42:58,098] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0451957368478179
[2024-01-22 09:43:00,750] INFO: Iter 29600 Summary: 
[2024-01-22 09:43:00,750] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04463934600353241
[2024-01-22 09:43:03,254] INFO: Iter 29700 Summary: 
[2024-01-22 09:43:03,254] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04563980836421251
[2024-01-22 09:43:05,806] INFO: Iter 29800 Summary: 
[2024-01-22 09:43:05,807] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04521096546202898
[2024-01-22 09:43:08,381] INFO: Iter 29900 Summary: 
[2024-01-22 09:43:08,381] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04507623434066772
[2024-01-22 09:43:11,067] INFO: Iter 30000 Summary: 
[2024-01-22 09:43:11,067] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045068191066384315
[2024-01-22 09:43:13,807] INFO: Iter 30100 Summary: 
[2024-01-22 09:43:13,807] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0450696212425828
[2024-01-22 09:43:16,416] INFO: Iter 30200 Summary: 
[2024-01-22 09:43:16,416] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044873826541006565
[2024-01-22 09:43:19,030] INFO: Iter 30300 Summary: 
[2024-01-22 09:43:19,030] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04511665478348732
[2024-01-22 09:43:21,611] INFO: Iter 30400 Summary: 
[2024-01-22 09:43:21,611] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045308951437473294
[2024-01-22 09:43:24,166] INFO: Iter 30500 Summary: 
[2024-01-22 09:43:24,166] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045065110363066194
[2024-01-22 09:43:26,809] INFO: Iter 30600 Summary: 
[2024-01-22 09:43:26,809] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04516573954373598
[2024-01-22 09:43:29,367] INFO: Iter 30700 Summary: 
[2024-01-22 09:43:29,367] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045111528486013415
[2024-01-22 09:43:31,768] INFO: Iter 30800 Summary: 
[2024-01-22 09:43:31,768] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04502787031233311
[2024-01-22 09:43:34,544] INFO: Iter 30900 Summary: 
[2024-01-22 09:43:34,544] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0449596107751131
[2024-01-22 09:43:37,317] INFO: Iter 31000 Summary: 
[2024-01-22 09:43:37,317] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04511247329413891
[2024-01-22 09:43:39,889] INFO: Iter 31100 Summary: 
[2024-01-22 09:43:39,889] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045055635944008826
[2024-01-22 09:43:42,409] INFO: Iter 31200 Summary: 
[2024-01-22 09:43:42,409] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0448934043943882
[2024-01-22 09:43:45,035] INFO: Iter 31300 Summary: 
[2024-01-22 09:43:45,035] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04487552896142006
[2024-01-22 09:43:47,597] INFO: Iter 31400 Summary: 
[2024-01-22 09:43:47,597] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04492085993289947
[2024-01-22 09:43:50,123] INFO: Iter 31500 Summary: 
[2024-01-22 09:43:50,123] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04484195441007614
[2024-01-22 09:43:52,839] INFO: Iter 31600 Summary: 
[2024-01-22 09:43:52,839] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04513923358172178
[2024-01-22 09:43:55,316] INFO: Iter 31700 Summary: 
[2024-01-22 09:43:55,316] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04483621552586556
[2024-01-22 09:43:57,791] INFO: Iter 31800 Summary: 
[2024-01-22 09:43:57,792] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04478543110191822
[2024-01-22 09:44:00,412] INFO: Iter 31900 Summary: 
[2024-01-22 09:44:00,412] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044975597113370895
[2024-01-22 09:44:03,019] INFO: Iter 32000 Summary: 
[2024-01-22 09:44:03,019] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04469526182860136
[2024-01-22 09:44:05,504] INFO: Iter 32100 Summary: 
[2024-01-22 09:44:05,504] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045134314820170406
[2024-01-22 09:44:08,053] INFO: Iter 32200 Summary: 
[2024-01-22 09:44:08,054] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04516666255891323
[2024-01-22 09:44:10,707] INFO: Iter 32300 Summary: 
[2024-01-22 09:44:10,707] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04438265077769756
[2024-01-22 09:44:13,172] INFO: Iter 32400 Summary: 
[2024-01-22 09:44:13,173] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04499930705875158
[2024-01-22 09:44:15,929] INFO: Iter 32500 Summary: 
[2024-01-22 09:44:15,929] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04465522926300764
[2024-01-22 09:44:18,535] INFO: Iter 32600 Summary: 
[2024-01-22 09:44:18,535] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04474292162805796
[2024-01-22 09:44:21,089] INFO: Iter 32700 Summary: 
[2024-01-22 09:44:21,089] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04459319289773703
[2024-01-22 09:44:23,425] INFO: Iter 32800 Summary: 
[2024-01-22 09:44:23,425] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044919989071786404
[2024-01-22 09:44:25,942] INFO: Iter 32900 Summary: 
[2024-01-22 09:44:25,942] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0450146209448576
[2024-01-22 09:44:28,520] INFO: Iter 33000 Summary: 
[2024-01-22 09:44:28,520] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04475711986422539
[2024-01-22 09:44:31,036] INFO: Iter 33100 Summary: 
[2024-01-22 09:44:31,036] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0444561729580164
[2024-01-22 09:44:33,781] INFO: Iter 33200 Summary: 
[2024-01-22 09:44:33,781] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04496260739862919
[2024-01-22 09:44:36,444] INFO: Iter 33300 Summary: 
[2024-01-22 09:44:36,444] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04484326526522636
[2024-01-22 09:44:39,081] INFO: Iter 33400 Summary: 
[2024-01-22 09:44:39,082] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044736971519887446
[2024-01-22 09:44:41,510] INFO: Iter 33500 Summary: 
[2024-01-22 09:44:41,511] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04483176596462726
[2024-01-22 09:44:43,999] INFO: Iter 33600 Summary: 
[2024-01-22 09:44:43,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04477162603288889
[2024-01-22 09:44:46,571] INFO: Iter 33700 Summary: 
[2024-01-22 09:44:46,571] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.045257704965770244
[2024-01-22 09:44:49,103] INFO: Iter 33800 Summary: 
[2024-01-22 09:44:49,103] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04515054009854794
[2024-01-22 09:44:51,810] INFO: Iter 33900 Summary: 
[2024-01-22 09:44:51,810] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04472258854657411
[2024-01-22 09:44:54,556] INFO: Iter 34000 Summary: 
[2024-01-22 09:44:54,556] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0446690858900547
[2024-01-22 09:44:57,025] INFO: Iter 34100 Summary: 
[2024-01-22 09:44:57,026] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04494289010763168
[2024-01-22 09:44:59,686] INFO: Iter 34200 Summary: 
[2024-01-22 09:44:59,686] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044749505557119845
[2024-01-22 09:45:02,316] INFO: Iter 34300 Summary: 
[2024-01-22 09:45:02,316] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04490995671600104
[2024-01-22 09:45:04,909] INFO: Iter 34400 Summary: 
[2024-01-22 09:45:04,909] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04492561724036932
[2024-01-22 09:45:07,430] INFO: Iter 34500 Summary: 
[2024-01-22 09:45:07,430] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04461097218096256
[2024-01-22 09:45:10,196] INFO: Iter 34600 Summary: 
[2024-01-22 09:45:10,197] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04472830452024937
[2024-01-22 09:45:12,717] INFO: Iter 34700 Summary: 
[2024-01-22 09:45:12,717] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04482683628797531
[2024-01-22 09:45:15,157] INFO: Iter 34800 Summary: 
[2024-01-22 09:45:15,157] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04467572808265686
[2024-01-22 09:45:17,899] INFO: Iter 34900 Summary: 
[2024-01-22 09:45:17,899] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04497703321278095
[2024-01-22 09:45:20,667] INFO: Iter 35000 Summary: 
[2024-01-22 09:45:20,667] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04499464984983206
[2024-01-22 09:45:23,271] INFO: Iter 35100 Summary: 
[2024-01-22 09:45:23,271] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044872805178165436
[2024-01-22 09:45:26,007] INFO: Iter 35200 Summary: 
[2024-01-22 09:45:26,007] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04469749875366688
[2024-01-22 09:45:28,512] INFO: Iter 35300 Summary: 
[2024-01-22 09:45:28,512] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04479540873318911
[2024-01-22 09:45:31,180] INFO: Iter 35400 Summary: 
[2024-01-22 09:45:31,180] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044667812250554564
[2024-01-22 09:45:33,793] INFO: Iter 35500 Summary: 
[2024-01-22 09:45:33,794] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04481043670326471
[2024-01-22 09:45:36,531] INFO: Iter 35600 Summary: 
[2024-01-22 09:45:36,531] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04483440838754177
[2024-01-22 09:45:39,094] INFO: Iter 35700 Summary: 
[2024-01-22 09:45:39,094] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04502453058958054
[2024-01-22 09:45:41,730] INFO: Iter 35800 Summary: 
[2024-01-22 09:45:41,730] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04465095590800047
[2024-01-22 09:45:44,387] INFO: Iter 35900 Summary: 
[2024-01-22 09:45:44,387] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04428334049880504
[2024-01-22 09:45:47,144] INFO: Iter 36000 Summary: 
[2024-01-22 09:45:47,144] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04446783196181059
[2024-01-22 09:45:49,688] INFO: Iter 36100 Summary: 
[2024-01-22 09:45:49,688] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04480046976357698
[2024-01-22 09:45:52,246] INFO: Iter 36200 Summary: 
[2024-01-22 09:45:52,246] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044666096195578577
[2024-01-22 09:45:54,847] INFO: Iter 36300 Summary: 
[2024-01-22 09:45:54,847] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04472607828676701
[2024-01-22 09:45:57,593] INFO: Iter 36400 Summary: 
[2024-01-22 09:45:57,593] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04479884140193462
[2024-01-22 09:46:00,213] INFO: Iter 36500 Summary: 
[2024-01-22 09:46:00,213] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04433444771915674
[2024-01-22 09:46:02,899] INFO: Iter 36600 Summary: 
[2024-01-22 09:46:02,899] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04466501697897911
[2024-01-22 09:46:05,512] INFO: Iter 36700 Summary: 
[2024-01-22 09:46:05,512] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04500096026808023
[2024-01-22 09:46:07,966] INFO: Iter 36800 Summary: 
[2024-01-22 09:46:07,966] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04454227793961763
[2024-01-22 09:46:10,503] INFO: Iter 36900 Summary: 
[2024-01-22 09:46:10,503] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044492971375584604
[2024-01-22 09:46:13,175] INFO: Iter 37000 Summary: 
[2024-01-22 09:46:13,175] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04426666796207428
[2024-01-22 09:46:15,918] INFO: Iter 37100 Summary: 
[2024-01-22 09:46:15,918] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418948400765657
[2024-01-22 09:46:18,489] INFO: Iter 37200 Summary: 
[2024-01-22 09:46:18,490] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044389937706291675
[2024-01-22 09:46:21,209] INFO: Iter 37300 Summary: 
[2024-01-22 09:46:21,209] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04429722063243389
[2024-01-22 09:46:23,794] INFO: Iter 37400 Summary: 
[2024-01-22 09:46:23,794] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044396082907915115
[2024-01-22 09:46:26,307] INFO: Iter 37500 Summary: 
[2024-01-22 09:46:26,307] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044599950686097144
[2024-01-22 09:46:28,980] INFO: Iter 37600 Summary: 
[2024-01-22 09:46:28,980] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04421442918479443
[2024-01-22 09:46:31,569] INFO: Iter 37700 Summary: 
[2024-01-22 09:46:31,570] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04459664031863213
[2024-01-22 09:46:34,009] INFO: Iter 37800 Summary: 
[2024-01-22 09:46:34,009] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04476152919232845
[2024-01-22 09:46:36,583] INFO: Iter 37900 Summary: 
[2024-01-22 09:46:36,583] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04453292846679688
[2024-01-22 09:46:39,274] INFO: Iter 38000 Summary: 
[2024-01-22 09:46:39,274] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04398870248347521
[2024-01-22 09:46:42,043] INFO: Iter 38100 Summary: 
[2024-01-22 09:46:42,044] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04461600936949253
[2024-01-22 09:46:44,463] INFO: Iter 38200 Summary: 
[2024-01-22 09:46:44,463] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04487854115664959
[2024-01-22 09:46:47,113] INFO: Iter 38300 Summary: 
[2024-01-22 09:46:47,113] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0444058147072792
[2024-01-22 09:46:49,645] INFO: Iter 38400 Summary: 
[2024-01-22 09:46:49,645] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04456448581069708
[2024-01-22 09:46:52,255] INFO: Iter 38500 Summary: 
[2024-01-22 09:46:52,255] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04477925181388855
[2024-01-22 09:46:54,854] INFO: Iter 38600 Summary: 
[2024-01-22 09:46:54,854] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0445923599973321
[2024-01-22 09:46:57,520] INFO: Iter 38700 Summary: 
[2024-01-22 09:46:57,520] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044346744753420354
[2024-01-22 09:47:00,135] INFO: Iter 38800 Summary: 
[2024-01-22 09:47:00,135] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04449617117643356
[2024-01-22 09:47:02,749] INFO: Iter 38900 Summary: 
[2024-01-22 09:47:02,750] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04420080401003361
[2024-01-22 09:47:05,460] INFO: Iter 39000 Summary: 
[2024-01-22 09:47:05,460] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044553517885506155
[2024-01-22 09:47:08,051] INFO: Iter 39100 Summary: 
[2024-01-22 09:47:08,052] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044732554703950884
[2024-01-22 09:47:10,454] INFO: Iter 39200 Summary: 
[2024-01-22 09:47:10,454] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04461104620248079
[2024-01-22 09:47:13,152] INFO: Iter 39300 Summary: 
[2024-01-22 09:47:13,152] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044287254959344866
[2024-01-22 09:47:15,671] INFO: Iter 39400 Summary: 
[2024-01-22 09:47:15,671] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044290792755782606
[2024-01-22 09:47:18,214] INFO: Iter 39500 Summary: 
[2024-01-22 09:47:18,214] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04449369579553604
[2024-01-22 09:47:20,750] INFO: Iter 39600 Summary: 
[2024-01-22 09:47:20,750] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04450795948505402
[2024-01-22 09:47:23,340] INFO: Iter 39700 Summary: 
[2024-01-22 09:47:23,340] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04450284298509359
[2024-01-22 09:47:25,918] INFO: Iter 39800 Summary: 
[2024-01-22 09:47:25,918] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418864235281944
[2024-01-22 09:47:28,449] INFO: Iter 39900 Summary: 
[2024-01-22 09:47:28,450] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04437816109508276
[2024-01-22 09:47:31,032] INFO: Iter 40000 Summary: 
[2024-01-22 09:47:31,033] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04427728582173586
[2024-01-22 09:47:33,589] INFO: Iter 40100 Summary: 
[2024-01-22 09:47:33,589] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044356932528316975
[2024-01-22 09:47:36,197] INFO: Iter 40200 Summary: 
[2024-01-22 09:47:36,197] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044275180511176584
[2024-01-22 09:47:38,850] INFO: Iter 40300 Summary: 
[2024-01-22 09:47:38,850] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04457957718521357
[2024-01-22 09:47:41,439] INFO: Iter 40400 Summary: 
[2024-01-22 09:47:41,439] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04411593295633793
[2024-01-22 09:47:43,943] INFO: Iter 40500 Summary: 
[2024-01-22 09:47:43,943] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044393535405397415
[2024-01-22 09:47:46,505] INFO: Iter 40600 Summary: 
[2024-01-22 09:47:46,506] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04428249586373568
[2024-01-22 09:47:49,153] INFO: Iter 40700 Summary: 
[2024-01-22 09:47:49,154] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418441738933325
[2024-01-22 09:47:51,786] INFO: Iter 40800 Summary: 
[2024-01-22 09:47:51,786] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044501946941018104
[2024-01-22 09:47:54,278] INFO: Iter 40900 Summary: 
[2024-01-22 09:47:54,278] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04444786876440048
[2024-01-22 09:47:56,763] INFO: Iter 41000 Summary: 
[2024-01-22 09:47:56,764] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04470245935022831
[2024-01-22 09:47:59,342] INFO: Iter 41100 Summary: 
[2024-01-22 09:47:59,343] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04415977448225021
[2024-01-22 09:48:01,944] INFO: Iter 41200 Summary: 
[2024-01-22 09:48:01,945] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04431740246713162
[2024-01-22 09:48:04,735] INFO: Iter 41300 Summary: 
[2024-01-22 09:48:04,735] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04408435672521591
[2024-01-22 09:48:07,410] INFO: Iter 41400 Summary: 
[2024-01-22 09:48:07,410] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04455803118646145
[2024-01-22 09:48:10,045] INFO: Iter 41500 Summary: 
[2024-01-22 09:48:10,045] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04427940018475056
[2024-01-22 09:48:12,513] INFO: Iter 41600 Summary: 
[2024-01-22 09:48:12,513] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0443386510387063
[2024-01-22 09:48:15,084] INFO: Iter 41700 Summary: 
[2024-01-22 09:48:15,084] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04452127069234848
[2024-01-22 09:48:17,887] INFO: Iter 41800 Summary: 
[2024-01-22 09:48:17,887] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04437248893082142
[2024-01-22 09:48:20,510] INFO: Iter 41900 Summary: 
[2024-01-22 09:48:20,510] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044299268797039985
[2024-01-22 09:48:23,216] INFO: Iter 42000 Summary: 
[2024-01-22 09:48:23,216] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04441817443817854
[2024-01-22 09:48:25,743] INFO: Iter 42100 Summary: 
[2024-01-22 09:48:25,743] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04448385458439588
[2024-01-22 09:48:28,258] INFO: Iter 42200 Summary: 
[2024-01-22 09:48:28,258] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04423307292163372
[2024-01-22 09:48:30,934] INFO: Iter 42300 Summary: 
[2024-01-22 09:48:30,934] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04425233181566
[2024-01-22 09:48:33,604] INFO: Iter 42400 Summary: 
[2024-01-22 09:48:33,604] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04437621392309666
[2024-01-22 09:48:36,181] INFO: Iter 42500 Summary: 
[2024-01-22 09:48:36,181] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044552898928523064
[2024-01-22 09:48:38,646] INFO: Iter 42600 Summary: 
[2024-01-22 09:48:38,646] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04436825968325138
[2024-01-22 09:48:41,360] INFO: Iter 42700 Summary: 
[2024-01-22 09:48:41,360] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04436651058495045
[2024-01-22 09:48:43,999] INFO: Iter 42800 Summary: 
[2024-01-22 09:48:43,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04426128376275301
[2024-01-22 09:48:46,603] INFO: Iter 42900 Summary: 
[2024-01-22 09:48:46,603] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04431760292500257
[2024-01-22 09:48:49,318] INFO: Iter 43000 Summary: 
[2024-01-22 09:48:49,318] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04409464243799448
[2024-01-22 09:48:51,957] INFO: Iter 43100 Summary: 
[2024-01-22 09:48:51,957] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044467531926929954
[2024-01-22 09:48:54,385] INFO: Iter 43200 Summary: 
[2024-01-22 09:48:54,385] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0443781952559948
[2024-01-22 09:48:56,999] INFO: Iter 43300 Summary: 
[2024-01-22 09:48:56,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04459474679082632
[2024-01-22 09:48:59,735] INFO: Iter 43400 Summary: 
[2024-01-22 09:48:59,735] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044203989766538145
[2024-01-22 09:49:02,351] INFO: Iter 43500 Summary: 
[2024-01-22 09:49:02,351] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043944016322493555
[2024-01-22 09:49:04,965] INFO: Iter 43600 Summary: 
[2024-01-22 09:49:04,965] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04467176407575607
[2024-01-22 09:49:07,618] INFO: Iter 43700 Summary: 
[2024-01-22 09:49:07,618] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043930493257939816
[2024-01-22 09:49:10,271] INFO: Iter 43800 Summary: 
[2024-01-22 09:49:10,271] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04431738749146461
[2024-01-22 09:49:12,826] INFO: Iter 43900 Summary: 
[2024-01-22 09:49:12,826] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044228265881538394
[2024-01-22 09:49:15,613] INFO: Iter 44000 Summary: 
[2024-01-22 09:49:15,614] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04428473591804504
[2024-01-22 09:49:18,407] INFO: Iter 44100 Summary: 
[2024-01-22 09:49:18,408] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04408544674515724
[2024-01-22 09:49:21,170] INFO: Iter 44200 Summary: 
[2024-01-22 09:49:21,170] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04453198198229075
[2024-01-22 09:49:23,752] INFO: Iter 44300 Summary: 
[2024-01-22 09:49:23,752] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044023277796804906
[2024-01-22 09:49:26,475] INFO: Iter 44400 Summary: 
[2024-01-22 09:49:26,475] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04399484921246767
[2024-01-22 09:49:29,193] INFO: Iter 44500 Summary: 
[2024-01-22 09:49:29,194] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04440322518348694
[2024-01-22 09:49:31,715] INFO: Iter 44600 Summary: 
[2024-01-22 09:49:31,715] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044218408316373824
[2024-01-22 09:49:34,507] INFO: Iter 44700 Summary: 
[2024-01-22 09:49:34,507] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366303525865078
[2024-01-22 09:49:37,204] INFO: Iter 44800 Summary: 
[2024-01-22 09:49:37,204] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04439988680183887
[2024-01-22 09:49:39,911] INFO: Iter 44900 Summary: 
[2024-01-22 09:49:39,911] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418771661818027
[2024-01-22 09:49:42,717] INFO: Iter 45000 Summary: 
[2024-01-22 09:49:42,717] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04428821463137865
[2024-01-22 09:49:45,293] INFO: Iter 45100 Summary: 
[2024-01-22 09:49:45,293] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04385430213063955
[2024-01-22 09:49:47,831] INFO: Iter 45200 Summary: 
[2024-01-22 09:49:47,831] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04455798849463463
[2024-01-22 09:49:50,405] INFO: Iter 45300 Summary: 
[2024-01-22 09:49:50,405] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044114292562007905
[2024-01-22 09:49:53,081] INFO: Iter 45400 Summary: 
[2024-01-22 09:49:53,081] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04398076366633177
[2024-01-22 09:49:55,879] INFO: Iter 45500 Summary: 
[2024-01-22 09:49:55,880] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04424060821533203
[2024-01-22 09:49:58,490] INFO: Iter 45600 Summary: 
[2024-01-22 09:49:58,490] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04422922544181347
[2024-01-22 09:50:01,145] INFO: Iter 45700 Summary: 
[2024-01-22 09:50:01,146] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04398425757884979
[2024-01-22 09:50:03,687] INFO: Iter 45800 Summary: 
[2024-01-22 09:50:03,687] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044130173437297346
[2024-01-22 09:50:06,192] INFO: Iter 45900 Summary: 
[2024-01-22 09:50:06,192] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04388433121144772
[2024-01-22 09:50:08,896] INFO: Iter 46000 Summary: 
[2024-01-22 09:50:08,896] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04397213343530893
[2024-01-22 09:50:11,442] INFO: Iter 46100 Summary: 
[2024-01-22 09:50:11,442] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043810296431183816
[2024-01-22 09:50:14,141] INFO: Iter 46200 Summary: 
[2024-01-22 09:50:14,141] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0440921999886632
[2024-01-22 09:50:16,776] INFO: Iter 46300 Summary: 
[2024-01-22 09:50:16,777] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043926538340747355
[2024-01-22 09:50:19,379] INFO: Iter 46400 Summary: 
[2024-01-22 09:50:19,379] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04403003443032503
[2024-01-22 09:50:21,983] INFO: Iter 46500 Summary: 
[2024-01-22 09:50:21,984] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04399214465171099
[2024-01-22 09:50:24,505] INFO: Iter 46600 Summary: 
[2024-01-22 09:50:24,505] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04386811725795269
[2024-01-22 09:50:27,112] INFO: Iter 46700 Summary: 
[2024-01-22 09:50:27,113] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04410086791962385
[2024-01-22 09:50:29,696] INFO: Iter 46800 Summary: 
[2024-01-22 09:50:29,696] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0439137864112854
[2024-01-22 09:50:32,482] INFO: Iter 46900 Summary: 
[2024-01-22 09:50:32,482] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04391390919685364
[2024-01-22 09:50:35,041] INFO: Iter 47000 Summary: 
[2024-01-22 09:50:35,041] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044355414994060996
[2024-01-22 09:50:37,753] INFO: Iter 47100 Summary: 
[2024-01-22 09:50:37,753] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044534758031368256
[2024-01-22 09:50:40,311] INFO: Iter 47200 Summary: 
[2024-01-22 09:50:40,311] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04419305689632893
[2024-01-22 09:50:42,919] INFO: Iter 47300 Summary: 
[2024-01-22 09:50:42,919] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418247666209936
[2024-01-22 09:50:45,493] INFO: Iter 47400 Summary: 
[2024-01-22 09:50:45,493] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04401179060339928
[2024-01-22 09:50:48,249] INFO: Iter 47500 Summary: 
[2024-01-22 09:50:48,249] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043931263908743856
[2024-01-22 09:50:50,838] INFO: Iter 47600 Summary: 
[2024-01-22 09:50:50,839] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04424656767398119
[2024-01-22 09:50:53,594] INFO: Iter 47700 Summary: 
[2024-01-22 09:50:53,595] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044229438602924345
[2024-01-22 09:50:56,210] INFO: Iter 47800 Summary: 
[2024-01-22 09:50:56,210] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043991674706339835
[2024-01-22 09:50:58,970] INFO: Iter 47900 Summary: 
[2024-01-22 09:50:58,971] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04438489217311144
[2024-01-22 09:51:01,431] INFO: Iter 48000 Summary: 
[2024-01-22 09:51:01,431] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044147703908383844
[2024-01-22 09:51:04,079] INFO: Iter 48100 Summary: 
[2024-01-22 09:51:04,079] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044007473550736906
[2024-01-22 09:51:06,699] INFO: Iter 48200 Summary: 
[2024-01-22 09:51:06,700] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04425082098692656
[2024-01-22 09:51:09,328] INFO: Iter 48300 Summary: 
[2024-01-22 09:51:09,328] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04401395436376333
[2024-01-22 09:51:11,866] INFO: Iter 48400 Summary: 
[2024-01-22 09:51:11,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04401220042258501
[2024-01-22 09:51:14,620] INFO: Iter 48500 Summary: 
[2024-01-22 09:51:14,620] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043896263353526595
[2024-01-22 09:51:17,128] INFO: Iter 48600 Summary: 
[2024-01-22 09:51:17,128] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043949699364602565
[2024-01-22 09:51:19,770] INFO: Iter 48700 Summary: 
[2024-01-22 09:51:19,770] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043513078950345514
[2024-01-22 09:51:22,395] INFO: Iter 48800 Summary: 
[2024-01-22 09:51:22,396] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043731918148696425
[2024-01-22 09:51:25,094] INFO: Iter 48900 Summary: 
[2024-01-22 09:51:25,094] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043994887955486776
[2024-01-22 09:51:27,737] INFO: Iter 49000 Summary: 
[2024-01-22 09:51:27,737] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044418214000761506
[2024-01-22 09:51:30,512] INFO: Iter 49100 Summary: 
[2024-01-22 09:51:30,512] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04403471454977989
[2024-01-22 09:51:33,221] INFO: Iter 49200 Summary: 
[2024-01-22 09:51:33,221] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04401041608303785
[2024-01-22 09:51:35,960] INFO: Iter 49300 Summary: 
[2024-01-22 09:51:35,960] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043894456662237645
[2024-01-22 09:51:38,542] INFO: Iter 49400 Summary: 
[2024-01-22 09:51:38,542] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04384636927396059
[2024-01-22 09:51:41,186] INFO: Iter 49500 Summary: 
[2024-01-22 09:51:41,186] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043830458782613275
[2024-01-22 09:51:43,795] INFO: Iter 49600 Summary: 
[2024-01-22 09:51:43,796] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043683288022875785
[2024-01-22 09:51:46,470] INFO: Iter 49700 Summary: 
[2024-01-22 09:51:46,470] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04401068314909935
[2024-01-22 09:51:49,131] INFO: Iter 49800 Summary: 
[2024-01-22 09:51:49,131] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0443716087564826
[2024-01-22 09:51:51,728] INFO: Iter 49900 Summary: 
[2024-01-22 09:51:51,728] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04397403407841921
[2024-01-22 09:51:54,248] INFO: Iter 50000 Summary: 
[2024-01-22 09:51:54,248] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04390831358730793
[2024-01-22 09:51:56,841] INFO: Iter 50100 Summary: 
[2024-01-22 09:51:56,841] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043987278304994104
[2024-01-22 09:51:59,493] INFO: Iter 50200 Summary: 
[2024-01-22 09:51:59,493] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04415125392377377
[2024-01-22 09:52:02,040] INFO: Iter 50300 Summary: 
[2024-01-22 09:52:02,040] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04381270367652178
[2024-01-22 09:52:04,638] INFO: Iter 50400 Summary: 
[2024-01-22 09:52:04,638] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044036615677177905
[2024-01-22 09:52:07,276] INFO: Iter 50500 Summary: 
[2024-01-22 09:52:07,276] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04410934060811997
[2024-01-22 09:52:09,818] INFO: Iter 50600 Summary: 
[2024-01-22 09:52:09,818] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043983296528458594
[2024-01-22 09:52:12,227] INFO: Iter 50700 Summary: 
[2024-01-22 09:52:12,227] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04388830147683621
[2024-01-22 09:52:14,974] INFO: Iter 50800 Summary: 
[2024-01-22 09:52:14,974] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0440101533010602
[2024-01-22 09:52:17,497] INFO: Iter 50900 Summary: 
[2024-01-22 09:52:17,497] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04389371208846569
[2024-01-22 09:52:20,162] INFO: Iter 51000 Summary: 
[2024-01-22 09:52:20,162] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04415514949709177
[2024-01-22 09:52:22,755] INFO: Iter 51100 Summary: 
[2024-01-22 09:52:22,755] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04393018186092377
[2024-01-22 09:52:25,216] INFO: Iter 51200 Summary: 
[2024-01-22 09:52:25,216] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04388497076928616
[2024-01-22 09:52:27,769] INFO: Iter 51300 Summary: 
[2024-01-22 09:52:27,769] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043981735147535804
[2024-01-22 09:52:30,450] INFO: Iter 51400 Summary: 
[2024-01-22 09:52:30,450] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394360896199942
[2024-01-22 09:52:33,069] INFO: Iter 51500 Summary: 
[2024-01-22 09:52:33,070] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370105937123299
[2024-01-22 09:52:35,820] INFO: Iter 51600 Summary: 
[2024-01-22 09:52:35,820] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04412420641630888
[2024-01-22 09:52:38,598] INFO: Iter 51700 Summary: 
[2024-01-22 09:52:38,598] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04425992898643017
[2024-01-22 09:52:41,277] INFO: Iter 51800 Summary: 
[2024-01-22 09:52:41,277] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04377830728888512
[2024-01-22 09:52:44,025] INFO: Iter 51900 Summary: 
[2024-01-22 09:52:44,025] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04392323900014162
[2024-01-22 09:52:46,490] INFO: Iter 52000 Summary: 
[2024-01-22 09:52:46,491] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043895124457776546
[2024-01-22 09:52:49,130] INFO: Iter 52100 Summary: 
[2024-01-22 09:52:49,130] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04399973899126053
[2024-01-22 09:52:51,724] INFO: Iter 52200 Summary: 
[2024-01-22 09:52:51,724] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044142823554575446
[2024-01-22 09:52:54,307] INFO: Iter 52300 Summary: 
[2024-01-22 09:52:54,307] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043659642562270166
[2024-01-22 09:52:56,869] INFO: Iter 52400 Summary: 
[2024-01-22 09:52:56,869] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043902136534452435
[2024-01-22 09:52:59,486] INFO: Iter 52500 Summary: 
[2024-01-22 09:52:59,486] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04406759507954121
[2024-01-22 09:53:02,061] INFO: Iter 52600 Summary: 
[2024-01-22 09:53:02,061] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394161265343428
[2024-01-22 09:53:04,626] INFO: Iter 52700 Summary: 
[2024-01-22 09:53:04,626] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044031138457357885
[2024-01-22 09:53:07,338] INFO: Iter 52800 Summary: 
[2024-01-22 09:53:07,338] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359203908592463
[2024-01-22 09:53:10,070] INFO: Iter 52900 Summary: 
[2024-01-22 09:53:10,070] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0441457162052393
[2024-01-22 09:53:12,579] INFO: Iter 53000 Summary: 
[2024-01-22 09:53:12,579] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043866624534130094
[2024-01-22 09:53:15,411] INFO: Iter 53100 Summary: 
[2024-01-22 09:53:15,411] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04408240057528019
[2024-01-22 09:53:17,968] INFO: Iter 53200 Summary: 
[2024-01-22 09:53:17,968] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0436669696867466
[2024-01-22 09:53:20,613] INFO: Iter 53300 Summary: 
[2024-01-22 09:53:20,614] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04394857604056597
[2024-01-22 09:53:23,041] INFO: Iter 53400 Summary: 
[2024-01-22 09:53:23,042] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370525747537613
[2024-01-22 09:53:25,658] INFO: Iter 53500 Summary: 
[2024-01-22 09:53:25,658] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04417108315974474
[2024-01-22 09:53:28,224] INFO: Iter 53600 Summary: 
[2024-01-22 09:53:28,224] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04400563918054104
[2024-01-22 09:53:30,740] INFO: Iter 53700 Summary: 
[2024-01-22 09:53:30,740] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0441157428920269
[2024-01-22 09:53:33,465] INFO: Iter 53800 Summary: 
[2024-01-22 09:53:33,465] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044017383009195325
[2024-01-22 09:53:36,188] INFO: Iter 53900 Summary: 
[2024-01-22 09:53:36,189] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043813437148928644
[2024-01-22 09:53:38,842] INFO: Iter 54000 Summary: 
[2024-01-22 09:53:38,842] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04391944631934166
[2024-01-22 09:53:41,530] INFO: Iter 54100 Summary: 
[2024-01-22 09:53:41,530] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04364235728979111
[2024-01-22 09:53:44,268] INFO: Iter 54200 Summary: 
[2024-01-22 09:53:44,268] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04395762216299772
[2024-01-22 09:53:46,921] INFO: Iter 54300 Summary: 
[2024-01-22 09:53:46,921] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04412426043301821
[2024-01-22 09:53:49,466] INFO: Iter 54400 Summary: 
[2024-01-22 09:53:49,466] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043813886530697344
[2024-01-22 09:53:52,015] INFO: Iter 54500 Summary: 
[2024-01-22 09:53:52,015] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043742046877741816
[2024-01-22 09:53:54,545] INFO: Iter 54600 Summary: 
[2024-01-22 09:53:54,545] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043748501800000665
[2024-01-22 09:53:56,955] INFO: Iter 54700 Summary: 
[2024-01-22 09:53:56,955] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04381635673344135
[2024-01-22 09:53:59,458] INFO: Iter 54800 Summary: 
[2024-01-22 09:53:59,458] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044233166985213755
[2024-01-22 09:54:02,152] INFO: Iter 54900 Summary: 
[2024-01-22 09:54:02,152] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04378367871046066
[2024-01-22 09:54:04,767] INFO: Iter 55000 Summary: 
[2024-01-22 09:54:04,768] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04402273382991553
[2024-01-22 09:54:07,234] INFO: Iter 55100 Summary: 
[2024-01-22 09:54:07,235] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04369497992098331
[2024-01-22 09:54:09,920] INFO: Iter 55200 Summary: 
[2024-01-22 09:54:09,920] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366604298353195
[2024-01-22 09:54:12,472] INFO: Iter 55300 Summary: 
[2024-01-22 09:54:12,472] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371363628655672
[2024-01-22 09:54:15,189] INFO: Iter 55400 Summary: 
[2024-01-22 09:54:15,189] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04376485917717218
[2024-01-22 09:54:17,877] INFO: Iter 55500 Summary: 
[2024-01-22 09:54:17,878] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04418721344321966
[2024-01-22 09:54:20,474] INFO: Iter 55600 Summary: 
[2024-01-22 09:54:20,474] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043901280090212824
[2024-01-22 09:54:22,899] INFO: Iter 55700 Summary: 
[2024-01-22 09:54:22,899] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043563750833272935
[2024-01-22 09:54:25,674] INFO: Iter 55800 Summary: 
[2024-01-22 09:54:25,675] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04376074496656656
[2024-01-22 09:54:28,395] INFO: Iter 55900 Summary: 
[2024-01-22 09:54:28,396] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04399267867207527
[2024-01-22 09:54:31,093] INFO: Iter 56000 Summary: 
[2024-01-22 09:54:31,093] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043886991962790486
[2024-01-22 09:54:33,622] INFO: Iter 56100 Summary: 
[2024-01-22 09:54:33,622] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043748666048049924
[2024-01-22 09:54:36,284] INFO: Iter 56200 Summary: 
[2024-01-22 09:54:36,284] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0441256994754076
[2024-01-22 09:54:38,841] INFO: Iter 56300 Summary: 
[2024-01-22 09:54:38,841] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04337160680443049
[2024-01-22 09:54:41,517] INFO: Iter 56400 Summary: 
[2024-01-22 09:54:41,517] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043943238519132136
[2024-01-22 09:54:44,235] INFO: Iter 56500 Summary: 
[2024-01-22 09:54:44,235] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.044207854084670546
[2024-01-22 09:54:46,840] INFO: Iter 56600 Summary: 
[2024-01-22 09:54:46,841] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04391303528100252
[2024-01-22 09:54:49,399] INFO: Iter 56700 Summary: 
[2024-01-22 09:54:49,400] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04385758712887764
[2024-01-22 09:54:52,053] INFO: Iter 56800 Summary: 
[2024-01-22 09:54:52,053] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04360218342393637
[2024-01-22 09:54:54,783] INFO: Iter 56900 Summary: 
[2024-01-22 09:54:54,783] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371616590768099
[2024-01-22 09:54:57,330] INFO: Iter 57000 Summary: 
[2024-01-22 09:54:57,330] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04404743146151304
[2024-01-22 09:54:59,998] INFO: Iter 57100 Summary: 
[2024-01-22 09:54:59,999] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04353363517671824
[2024-01-22 09:55:02,719] INFO: Iter 57200 Summary: 
[2024-01-22 09:55:02,719] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04337940745055675
[2024-01-22 09:55:05,496] INFO: Iter 57300 Summary: 
[2024-01-22 09:55:05,496] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04378432225435972
[2024-01-22 09:55:08,092] INFO: Iter 57400 Summary: 
[2024-01-22 09:55:08,092] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04358578409999609
[2024-01-22 09:55:10,818] INFO: Iter 57500 Summary: 
[2024-01-22 09:55:10,818] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043767519891262055
[2024-01-22 09:55:13,567] INFO: Iter 57600 Summary: 
[2024-01-22 09:55:13,568] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043685339353978635
[2024-01-22 09:55:16,124] INFO: Iter 57700 Summary: 
[2024-01-22 09:55:16,125] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043889213390648364
[2024-01-22 09:55:18,685] INFO: Iter 57800 Summary: 
[2024-01-22 09:55:18,685] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04388440933078527
[2024-01-22 09:55:21,219] INFO: Iter 57900 Summary: 
[2024-01-22 09:55:21,220] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043987908735871314
[2024-01-22 09:55:23,857] INFO: Iter 58000 Summary: 
[2024-01-22 09:55:23,857] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043691517300903794
[2024-01-22 09:55:26,501] INFO: Iter 58100 Summary: 
[2024-01-22 09:55:26,501] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04411493808031082
[2024-01-22 09:55:29,238] INFO: Iter 58200 Summary: 
[2024-01-22 09:55:29,238] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04381342969834805
[2024-01-22 09:55:31,845] INFO: Iter 58300 Summary: 
[2024-01-22 09:55:31,845] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04390533067286015
[2024-01-22 09:55:34,300] INFO: Iter 58400 Summary: 
[2024-01-22 09:55:34,300] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04373081583529711
[2024-01-22 09:55:37,030] INFO: Iter 58500 Summary: 
[2024-01-22 09:55:37,030] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04381570793688297
[2024-01-22 09:55:39,758] INFO: Iter 58600 Summary: 
[2024-01-22 09:55:39,758] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043624902702867986
[2024-01-22 09:55:42,399] INFO: Iter 58700 Summary: 
[2024-01-22 09:55:42,399] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043718334138393405
[2024-01-22 09:55:45,008] INFO: Iter 58800 Summary: 
[2024-01-22 09:55:45,008] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04384575977921486
[2024-01-22 09:55:47,721] INFO: Iter 58900 Summary: 
[2024-01-22 09:55:47,721] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04385745711624622
[2024-01-22 09:55:50,456] INFO: Iter 59000 Summary: 
[2024-01-22 09:55:50,456] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366136413067579
[2024-01-22 09:55:52,997] INFO: Iter 59100 Summary: 
[2024-01-22 09:55:52,997] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043905769363045695
[2024-01-22 09:55:55,668] INFO: Iter 59200 Summary: 
[2024-01-22 09:55:55,668] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04400937512516975
[2024-01-22 09:55:58,480] INFO: Iter 59300 Summary: 
[2024-01-22 09:55:58,480] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043903947658836845
[2024-01-22 09:56:00,908] INFO: Iter 59400 Summary: 
[2024-01-22 09:56:00,909] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043676183447241786
[2024-01-22 09:56:03,494] INFO: Iter 59500 Summary: 
[2024-01-22 09:56:03,494] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043748686127364635
[2024-01-22 09:56:06,001] INFO: Iter 59600 Summary: 
[2024-01-22 09:56:06,001] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043921012878417966
[2024-01-22 09:56:08,601] INFO: Iter 59700 Summary: 
[2024-01-22 09:56:08,601] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346241604536772
[2024-01-22 09:56:11,139] INFO: Iter 59800 Summary: 
[2024-01-22 09:56:11,139] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04367611240595579
[2024-01-22 09:56:13,746] INFO: Iter 59900 Summary: 
[2024-01-22 09:56:13,746] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04370215654373169
[2024-01-22 09:56:16,403] INFO: Iter 60000 Summary: 
[2024-01-22 09:56:16,403] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04365439023822546
[2024-01-22 09:56:18,892] INFO: Iter 60100 Summary: 
[2024-01-22 09:56:18,892] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043544784523546695
[2024-01-22 09:56:21,590] INFO: Iter 60200 Summary: 
[2024-01-22 09:56:21,590] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043906766287982464
[2024-01-22 09:56:24,260] INFO: Iter 60300 Summary: 
[2024-01-22 09:56:24,260] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359420385211706
[2024-01-22 09:56:26,785] INFO: Iter 60400 Summary: 
[2024-01-22 09:56:26,785] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04386023808270693
[2024-01-22 09:56:29,402] INFO: Iter 60500 Summary: 
[2024-01-22 09:56:29,402] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043526274114847184
[2024-01-22 09:56:32,221] INFO: Iter 60600 Summary: 
[2024-01-22 09:56:32,221] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366434995085001
[2024-01-22 09:56:34,766] INFO: Iter 60700 Summary: 
[2024-01-22 09:56:34,767] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04373471457511187
[2024-01-22 09:56:37,355] INFO: Iter 60800 Summary: 
[2024-01-22 09:56:37,355] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04387699160724878
[2024-01-22 09:56:40,068] INFO: Iter 60900 Summary: 
[2024-01-22 09:56:40,068] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04374784592539072
[2024-01-22 09:56:42,745] INFO: Iter 61000 Summary: 
[2024-01-22 09:56:42,745] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04339983657002449
[2024-01-22 09:56:45,239] INFO: Iter 61100 Summary: 
[2024-01-22 09:56:45,239] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043773738779127595
[2024-01-22 09:56:47,905] INFO: Iter 61200 Summary: 
[2024-01-22 09:56:47,905] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04387050312012434
[2024-01-22 09:56:50,585] INFO: Iter 61300 Summary: 
[2024-01-22 09:56:50,585] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04362797357141972
[2024-01-22 09:56:53,366] INFO: Iter 61400 Summary: 
[2024-01-22 09:56:53,366] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04348433144390583
[2024-01-22 09:56:56,027] INFO: Iter 61500 Summary: 
[2024-01-22 09:56:56,027] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04334660172462464
[2024-01-22 09:56:58,616] INFO: Iter 61600 Summary: 
[2024-01-22 09:56:58,616] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04372625768184662
[2024-01-22 09:57:01,233] INFO: Iter 61700 Summary: 
[2024-01-22 09:57:01,233] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043910546712577346
[2024-01-22 09:57:03,792] INFO: Iter 61800 Summary: 
[2024-01-22 09:57:03,792] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336675755679607
[2024-01-22 09:57:06,324] INFO: Iter 61900 Summary: 
[2024-01-22 09:57:06,325] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043700095936656
[2024-01-22 09:57:09,004] INFO: Iter 62000 Summary: 
[2024-01-22 09:57:09,004] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043748187683522705
[2024-01-22 09:57:11,638] INFO: Iter 62100 Summary: 
[2024-01-22 09:57:11,638] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04357329860329628
[2024-01-22 09:57:14,186] INFO: Iter 62200 Summary: 
[2024-01-22 09:57:14,186] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043717811219394206
[2024-01-22 09:57:16,825] INFO: Iter 62300 Summary: 
[2024-01-22 09:57:16,825] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043637515231966975
[2024-01-22 09:57:19,391] INFO: Iter 62400 Summary: 
[2024-01-22 09:57:19,391] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04385813634842634
[2024-01-22 09:57:21,955] INFO: Iter 62500 Summary: 
[2024-01-22 09:57:21,955] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04362685699015856
[2024-01-22 09:57:24,519] INFO: Iter 62600 Summary: 
[2024-01-22 09:57:24,519] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336472827941179
[2024-01-22 09:57:27,154] INFO: Iter 62700 Summary: 
[2024-01-22 09:57:27,155] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04374432649463415
[2024-01-22 09:57:29,798] INFO: Iter 62800 Summary: 
[2024-01-22 09:57:29,798] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04342475488781929
[2024-01-22 09:57:32,610] INFO: Iter 62900 Summary: 
[2024-01-22 09:57:32,611] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04365725234150886
[2024-01-22 09:57:35,319] INFO: Iter 63000 Summary: 
[2024-01-22 09:57:35,319] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04388359494507313
[2024-01-22 09:57:37,956] INFO: Iter 63100 Summary: 
[2024-01-22 09:57:37,956] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371958691626787
[2024-01-22 09:57:40,509] INFO: Iter 63200 Summary: 
[2024-01-22 09:57:40,509] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04333436012268067
[2024-01-22 09:57:43,151] INFO: Iter 63300 Summary: 
[2024-01-22 09:57:43,151] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359933704137802
[2024-01-22 09:57:45,697] INFO: Iter 63400 Summary: 
[2024-01-22 09:57:45,697] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043377050347626206
[2024-01-22 09:57:48,135] INFO: Iter 63500 Summary: 
[2024-01-22 09:57:48,135] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04398191370069981
[2024-01-22 09:57:50,786] INFO: Iter 63600 Summary: 
[2024-01-22 09:57:50,786] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04356801927089691
[2024-01-22 09:57:53,418] INFO: Iter 63700 Summary: 
[2024-01-22 09:57:53,418] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04362318832427263
[2024-01-22 09:57:55,970] INFO: Iter 63800 Summary: 
[2024-01-22 09:57:55,970] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366565257310867
[2024-01-22 09:57:58,698] INFO: Iter 63900 Summary: 
[2024-01-22 09:57:58,698] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043242655992507935
[2024-01-22 09:58:01,315] INFO: Iter 64000 Summary: 
[2024-01-22 09:58:01,315] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04389686897397041
[2024-01-22 09:58:04,071] INFO: Iter 64100 Summary: 
[2024-01-22 09:58:04,071] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04367443785071373
[2024-01-22 09:58:06,646] INFO: Iter 64200 Summary: 
[2024-01-22 09:58:06,646] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366343636065721
[2024-01-22 09:58:09,246] INFO: Iter 64300 Summary: 
[2024-01-22 09:58:09,246] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04369466107338667
[2024-01-22 09:58:11,879] INFO: Iter 64400 Summary: 
[2024-01-22 09:58:11,880] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043431646339595315
[2024-01-22 09:58:14,473] INFO: Iter 64500 Summary: 
[2024-01-22 09:58:14,473] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043245042338967324
[2024-01-22 09:58:17,058] INFO: Iter 64600 Summary: 
[2024-01-22 09:58:17,058] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04357242424041033
[2024-01-22 09:58:19,583] INFO: Iter 64700 Summary: 
[2024-01-22 09:58:19,583] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04342757139354944
[2024-01-22 09:58:22,080] INFO: Iter 64800 Summary: 
[2024-01-22 09:58:22,080] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04383873328566551
[2024-01-22 09:58:24,757] INFO: Iter 64900 Summary: 
[2024-01-22 09:58:24,757] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04360345806926489
[2024-01-22 09:58:27,310] INFO: Iter 65000 Summary: 
[2024-01-22 09:58:27,311] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04365910284221172
[2024-01-22 09:58:29,968] INFO: Iter 65100 Summary: 
[2024-01-22 09:58:29,968] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04372021328657866
[2024-01-22 09:58:32,457] INFO: Iter 65200 Summary: 
[2024-01-22 09:58:32,457] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04334201656281948
[2024-01-22 09:58:35,233] INFO: Iter 65300 Summary: 
[2024-01-22 09:58:35,233] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04356864850968122
[2024-01-22 09:58:37,795] INFO: Iter 65400 Summary: 
[2024-01-22 09:58:37,795] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04326974216848612
[2024-01-22 09:58:40,386] INFO: Iter 65500 Summary: 
[2024-01-22 09:58:40,386] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043630530573427674
[2024-01-22 09:58:42,991] INFO: Iter 65600 Summary: 
[2024-01-22 09:58:42,992] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043400947637856006
[2024-01-22 09:58:45,606] INFO: Iter 65700 Summary: 
[2024-01-22 09:58:45,606] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043523500561714175
[2024-01-22 09:58:48,366] INFO: Iter 65800 Summary: 
[2024-01-22 09:58:48,367] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043249068595468995
[2024-01-22 09:58:51,029] INFO: Iter 65900 Summary: 
[2024-01-22 09:58:51,029] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043453260771930215
[2024-01-22 09:58:53,739] INFO: Iter 66000 Summary: 
[2024-01-22 09:58:53,739] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04356954649090767
[2024-01-22 09:58:56,354] INFO: Iter 66100 Summary: 
[2024-01-22 09:58:56,354] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043251116015017035
[2024-01-22 09:58:58,933] INFO: Iter 66200 Summary: 
[2024-01-22 09:58:58,933] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04373754009604454
[2024-01-22 09:59:01,590] INFO: Iter 66300 Summary: 
[2024-01-22 09:59:01,590] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043732039406895636
[2024-01-22 09:59:04,350] INFO: Iter 66400 Summary: 
[2024-01-22 09:59:04,350] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04348613653331995
[2024-01-22 09:59:06,942] INFO: Iter 66500 Summary: 
[2024-01-22 09:59:06,942] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04337978068739176
[2024-01-22 09:59:09,637] INFO: Iter 66600 Summary: 
[2024-01-22 09:59:09,637] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04358363334089518
[2024-01-22 09:59:12,331] INFO: Iter 66700 Summary: 
[2024-01-22 09:59:12,331] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043023599721491335
[2024-01-22 09:59:14,986] INFO: Iter 66800 Summary: 
[2024-01-22 09:59:14,986] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04377674039453268
[2024-01-22 09:59:17,586] INFO: Iter 66900 Summary: 
[2024-01-22 09:59:17,587] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04339855305850506
[2024-01-22 09:59:20,181] INFO: Iter 67000 Summary: 
[2024-01-22 09:59:20,181] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04339350026100874
[2024-01-22 09:59:22,866] INFO: Iter 67100 Summary: 
[2024-01-22 09:59:22,866] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043694288618862626
[2024-01-22 09:59:25,474] INFO: Iter 67200 Summary: 
[2024-01-22 09:59:25,475] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04327679350972176
[2024-01-22 09:59:28,182] INFO: Iter 67300 Summary: 
[2024-01-22 09:59:28,183] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043504638001322746
[2024-01-22 09:59:30,795] INFO: Iter 67400 Summary: 
[2024-01-22 09:59:30,795] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043551277555525306
[2024-01-22 09:59:33,213] INFO: Iter 67500 Summary: 
[2024-01-22 09:59:33,213] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04361282203346491
[2024-01-22 09:59:35,805] INFO: Iter 67600 Summary: 
[2024-01-22 09:59:35,805] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04320266306400299
[2024-01-22 09:59:38,510] INFO: Iter 67700 Summary: 
[2024-01-22 09:59:38,510] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04362518332898617
[2024-01-22 09:59:41,103] INFO: Iter 67800 Summary: 
[2024-01-22 09:59:41,103] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04357237920165062
[2024-01-22 09:59:43,564] INFO: Iter 67900 Summary: 
[2024-01-22 09:59:43,564] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043875254355371
[2024-01-22 09:59:46,153] INFO: Iter 68000 Summary: 
[2024-01-22 09:59:46,154] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04327557984739542
[2024-01-22 09:59:48,697] INFO: Iter 68100 Summary: 
[2024-01-22 09:59:48,697] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043476952649652956
[2024-01-22 09:59:51,224] INFO: Iter 68200 Summary: 
[2024-01-22 09:59:51,224] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043321839794516565
[2024-01-22 09:59:53,868] INFO: Iter 68300 Summary: 
[2024-01-22 09:59:53,868] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371173124760389
[2024-01-22 09:59:56,623] INFO: Iter 68400 Summary: 
[2024-01-22 09:59:56,623] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04381878811866045
[2024-01-22 09:59:59,347] INFO: Iter 68500 Summary: 
[2024-01-22 09:59:59,347] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04329822901636362
[2024-01-22 10:00:01,893] INFO: Iter 68600 Summary: 
[2024-01-22 10:00:01,893] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043382026404142376
[2024-01-22 10:00:04,594] INFO: Iter 68700 Summary: 
[2024-01-22 10:00:04,594] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043293683752417565
[2024-01-22 10:00:07,330] INFO: Iter 68800 Summary: 
[2024-01-22 10:00:07,330] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359382573515177
[2024-01-22 10:00:09,890] INFO: Iter 68900 Summary: 
[2024-01-22 10:00:09,890] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043451478146016596
[2024-01-22 10:00:12,518] INFO: Iter 69000 Summary: 
[2024-01-22 10:00:12,518] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043675329647958276
[2024-01-22 10:00:15,186] INFO: Iter 69100 Summary: 
[2024-01-22 10:00:15,186] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04338876128196716
[2024-01-22 10:00:17,825] INFO: Iter 69200 Summary: 
[2024-01-22 10:00:17,825] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04367097035050392
[2024-01-22 10:00:20,490] INFO: Iter 69300 Summary: 
[2024-01-22 10:00:20,490] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04317248605191708
[2024-01-22 10:00:23,231] INFO: Iter 69400 Summary: 
[2024-01-22 10:00:23,231] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04375517901033163
[2024-01-22 10:00:25,964] INFO: Iter 69500 Summary: 
[2024-01-22 10:00:25,965] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043560165241360665
[2024-01-22 10:00:28,590] INFO: Iter 69600 Summary: 
[2024-01-22 10:00:28,590] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043327520340681075
[2024-01-22 10:00:31,341] INFO: Iter 69700 Summary: 
[2024-01-22 10:00:31,341] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04352218583226204
[2024-01-22 10:00:34,131] INFO: Iter 69800 Summary: 
[2024-01-22 10:00:34,131] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04362292993813753
[2024-01-22 10:00:36,705] INFO: Iter 69900 Summary: 
[2024-01-22 10:00:36,705] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043551966547966
[2024-01-22 10:00:39,294] INFO: Iter 70000 Summary: 
[2024-01-22 10:00:39,294] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04355305802077055
[2024-01-22 10:00:41,840] INFO: Iter 70100 Summary: 
[2024-01-22 10:00:41,841] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043580354377627376
[2024-01-22 10:00:44,513] INFO: Iter 70200 Summary: 
[2024-01-22 10:00:44,513] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04369284007698297
[2024-01-22 10:00:47,249] INFO: Iter 70300 Summary: 
[2024-01-22 10:00:47,249] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043582756519317624
[2024-01-22 10:00:49,798] INFO: Iter 70400 Summary: 
[2024-01-22 10:00:49,798] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04306141529232264
[2024-01-22 10:00:52,422] INFO: Iter 70500 Summary: 
[2024-01-22 10:00:52,422] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043379468247294425
[2024-01-22 10:00:55,146] INFO: Iter 70600 Summary: 
[2024-01-22 10:00:55,146] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04357094634324312
[2024-01-22 10:00:57,732] INFO: Iter 70700 Summary: 
[2024-01-22 10:00:57,732] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043580671660602095
[2024-01-22 10:01:00,446] INFO: Iter 70800 Summary: 
[2024-01-22 10:01:00,446] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0434684032946825
[2024-01-22 10:01:03,013] INFO: Iter 70900 Summary: 
[2024-01-22 10:01:03,013] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043584396466612815
[2024-01-22 10:01:05,672] INFO: Iter 71000 Summary: 
[2024-01-22 10:01:05,672] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043464024886488914
[2024-01-22 10:01:08,268] INFO: Iter 71100 Summary: 
[2024-01-22 10:01:08,268] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04362399961799383
[2024-01-22 10:01:10,825] INFO: Iter 71200 Summary: 
[2024-01-22 10:01:10,825] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04314258679747582
[2024-01-22 10:01:13,282] INFO: Iter 71300 Summary: 
[2024-01-22 10:01:13,282] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04320455960929394
[2024-01-22 10:01:15,950] INFO: Iter 71400 Summary: 
[2024-01-22 10:01:15,951] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04388801433146
[2024-01-22 10:01:18,541] INFO: Iter 71500 Summary: 
[2024-01-22 10:01:18,541] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04329539388418198
[2024-01-22 10:01:21,097] INFO: Iter 71600 Summary: 
[2024-01-22 10:01:21,097] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366590768098831
[2024-01-22 10:01:23,790] INFO: Iter 71700 Summary: 
[2024-01-22 10:01:23,790] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04382726658135652
[2024-01-22 10:01:26,433] INFO: Iter 71800 Summary: 
[2024-01-22 10:01:26,433] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04350121341645718
[2024-01-22 10:01:28,990] INFO: Iter 71900 Summary: 
[2024-01-22 10:01:28,991] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04323743972927332
[2024-01-22 10:01:31,641] INFO: Iter 72000 Summary: 
[2024-01-22 10:01:31,642] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04329150829464197
[2024-01-22 10:01:34,248] INFO: Iter 72100 Summary: 
[2024-01-22 10:01:34,248] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04357466328889131
[2024-01-22 10:01:36,772] INFO: Iter 72200 Summary: 
[2024-01-22 10:01:36,772] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043413741365075115
[2024-01-22 10:01:39,182] INFO: Iter 72300 Summary: 
[2024-01-22 10:01:39,182] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04341551013290882
[2024-01-22 10:01:41,947] INFO: Iter 72400 Summary: 
[2024-01-22 10:01:41,947] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043300964087247845
[2024-01-22 10:01:44,639] INFO: Iter 72500 Summary: 
[2024-01-22 10:01:44,639] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04355649374425411
[2024-01-22 10:01:47,196] INFO: Iter 72600 Summary: 
[2024-01-22 10:01:47,196] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04344595491886139
[2024-01-22 10:01:49,852] INFO: Iter 72700 Summary: 
[2024-01-22 10:01:49,853] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346107445657253
[2024-01-22 10:01:52,481] INFO: Iter 72800 Summary: 
[2024-01-22 10:01:52,481] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04362701524049044
[2024-01-22 10:01:55,003] INFO: Iter 72900 Summary: 
[2024-01-22 10:01:55,003] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043167243376374245
[2024-01-22 10:01:57,500] INFO: Iter 73000 Summary: 
[2024-01-22 10:01:57,500] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04327376287430525
[2024-01-22 10:02:00,022] INFO: Iter 73100 Summary: 
[2024-01-22 10:02:00,022] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04342714969068766
[2024-01-22 10:02:02,736] INFO: Iter 73200 Summary: 
[2024-01-22 10:02:02,736] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043342960588634016
[2024-01-22 10:02:05,276] INFO: Iter 73300 Summary: 
[2024-01-22 10:02:05,276] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04368194829672575
[2024-01-22 10:02:08,021] INFO: Iter 73400 Summary: 
[2024-01-22 10:02:08,022] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043120454363524914
[2024-01-22 10:02:10,605] INFO: Iter 73500 Summary: 
[2024-01-22 10:02:10,605] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043503366485238074
[2024-01-22 10:02:13,034] INFO: Iter 73600 Summary: 
[2024-01-22 10:02:13,034] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04321444824337959
[2024-01-22 10:02:15,595] INFO: Iter 73700 Summary: 
[2024-01-22 10:02:15,596] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04359078574925661
[2024-01-22 10:02:18,251] INFO: Iter 73800 Summary: 
[2024-01-22 10:02:18,251] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043598265871405605
[2024-01-22 10:02:20,850] INFO: Iter 73900 Summary: 
[2024-01-22 10:02:20,850] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04343178674578667
[2024-01-22 10:02:23,437] INFO: Iter 74000 Summary: 
[2024-01-22 10:02:23,437] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340715598315001
[2024-01-22 10:02:26,270] INFO: Iter 74100 Summary: 
[2024-01-22 10:02:26,270] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04366524640470743
[2024-01-22 10:02:28,849] INFO: Iter 74200 Summary: 
[2024-01-22 10:02:28,850] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04355464819818735
[2024-01-22 10:02:31,455] INFO: Iter 74300 Summary: 
[2024-01-22 10:02:31,455] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04357757415622473
[2024-01-22 10:02:34,052] INFO: Iter 74400 Summary: 
[2024-01-22 10:02:34,052] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04371015518903732
[2024-01-22 10:02:36,631] INFO: Iter 74500 Summary: 
[2024-01-22 10:02:36,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04356349643319845
[2024-01-22 10:02:39,293] INFO: Iter 74600 Summary: 
[2024-01-22 10:02:39,293] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04334985811263323
[2024-01-22 10:02:42,120] INFO: Iter 74700 Summary: 
[2024-01-22 10:02:42,120] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04310942072421312
[2024-01-22 10:02:44,657] INFO: Iter 74800 Summary: 
[2024-01-22 10:02:44,657] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04347511999309063
[2024-01-22 10:02:47,291] INFO: Iter 74900 Summary: 
[2024-01-22 10:02:47,291] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043140325620770456
[2024-01-22 10:02:49,726] INFO: Iter 75000 Summary: 
[2024-01-22 10:02:49,726] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04317468550056219
[2024-01-22 10:02:52,291] INFO: Iter 75100 Summary: 
[2024-01-22 10:02:52,291] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04372733421623707
[2024-01-22 10:02:55,001] INFO: Iter 75200 Summary: 
[2024-01-22 10:02:55,001] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04302538823336363
[2024-01-22 10:02:57,626] INFO: Iter 75300 Summary: 
[2024-01-22 10:02:57,627] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04329432141035795
[2024-01-22 10:03:00,187] INFO: Iter 75400 Summary: 
[2024-01-22 10:03:00,187] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316533524543047
[2024-01-22 10:03:02,703] INFO: Iter 75500 Summary: 
[2024-01-22 10:03:02,703] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340327184647322
[2024-01-22 10:03:05,256] INFO: Iter 75600 Summary: 
[2024-01-22 10:03:05,256] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04345812521874905
[2024-01-22 10:03:07,945] INFO: Iter 75700 Summary: 
[2024-01-22 10:03:07,945] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340005494654178
[2024-01-22 10:03:10,635] INFO: Iter 75800 Summary: 
[2024-01-22 10:03:10,635] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336838785558939
[2024-01-22 10:03:13,188] INFO: Iter 75900 Summary: 
[2024-01-22 10:03:13,188] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04348515246063471
[2024-01-22 10:03:15,626] INFO: Iter 76000 Summary: 
[2024-01-22 10:03:15,627] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316186018288135
[2024-01-22 10:03:18,371] INFO: Iter 76100 Summary: 
[2024-01-22 10:03:18,371] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04303290050476789
[2024-01-22 10:03:20,962] INFO: Iter 76200 Summary: 
[2024-01-22 10:03:20,962] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043431446217000486
[2024-01-22 10:03:23,465] INFO: Iter 76300 Summary: 
[2024-01-22 10:03:23,465] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043191165439784525
[2024-01-22 10:03:26,155] INFO: Iter 76400 Summary: 
[2024-01-22 10:03:26,156] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04335507679730654
[2024-01-22 10:03:28,859] INFO: Iter 76500 Summary: 
[2024-01-22 10:03:28,859] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043670067414641384
[2024-01-22 10:03:31,500] INFO: Iter 76600 Summary: 
[2024-01-22 10:03:31,500] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04354274790734053
[2024-01-22 10:03:33,991] INFO: Iter 76700 Summary: 
[2024-01-22 10:03:33,991] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04345875564962626
[2024-01-22 10:03:36,646] INFO: Iter 76800 Summary: 
[2024-01-22 10:03:36,647] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0431703395023942
[2024-01-22 10:03:39,235] INFO: Iter 76900 Summary: 
[2024-01-22 10:03:39,235] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04353749860078096
[2024-01-22 10:03:41,789] INFO: Iter 77000 Summary: 
[2024-01-22 10:03:41,789] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04343251843005419
[2024-01-22 10:03:44,511] INFO: Iter 77100 Summary: 
[2024-01-22 10:03:44,511] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04302667051553726
[2024-01-22 10:03:47,125] INFO: Iter 77200 Summary: 
[2024-01-22 10:03:47,126] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043218338787555696
[2024-01-22 10:03:49,630] INFO: Iter 77300 Summary: 
[2024-01-22 10:03:49,630] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04346145417541265
[2024-01-22 10:03:52,272] INFO: Iter 77400 Summary: 
[2024-01-22 10:03:52,272] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04324615638703108
[2024-01-22 10:03:54,873] INFO: Iter 77500 Summary: 
[2024-01-22 10:03:54,873] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043057645931839944
[2024-01-22 10:03:57,597] INFO: Iter 77600 Summary: 
[2024-01-22 10:03:57,597] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04351327534765005
[2024-01-22 10:04:00,107] INFO: Iter 77700 Summary: 
[2024-01-22 10:04:00,107] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043488207124173645
[2024-01-22 10:04:02,832] INFO: Iter 77800 Summary: 
[2024-01-22 10:04:02,832] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04315011367201805
[2024-01-22 10:04:05,349] INFO: Iter 77900 Summary: 
[2024-01-22 10:04:05,350] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04342246506363154
[2024-01-22 10:04:07,876] INFO: Iter 78000 Summary: 
[2024-01-22 10:04:07,877] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04352758288383484
[2024-01-22 10:04:10,684] INFO: Iter 78100 Summary: 
[2024-01-22 10:04:10,685] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043299603909254074
[2024-01-22 10:04:13,210] INFO: Iter 78200 Summary: 
[2024-01-22 10:04:13,210] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04335673607885838
[2024-01-22 10:04:15,722] INFO: Iter 78300 Summary: 
[2024-01-22 10:04:15,722] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04341382682323456
[2024-01-22 10:04:18,403] INFO: Iter 78400 Summary: 
[2024-01-22 10:04:18,403] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04314053926616907
[2024-01-22 10:04:20,987] INFO: Iter 78500 Summary: 
[2024-01-22 10:04:20,987] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04320460770279169
[2024-01-22 10:04:23,625] INFO: Iter 78600 Summary: 
[2024-01-22 10:04:23,625] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043319809176027776
[2024-01-22 10:04:26,165] INFO: Iter 78700 Summary: 
[2024-01-22 10:04:26,165] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04331248857080936
[2024-01-22 10:04:28,855] INFO: Iter 78800 Summary: 
[2024-01-22 10:04:28,855] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043124649301171304
[2024-01-22 10:04:31,536] INFO: Iter 78900 Summary: 
[2024-01-22 10:04:31,536] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04312242988497019
[2024-01-22 10:04:34,106] INFO: Iter 79000 Summary: 
[2024-01-22 10:04:34,106] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04284788630902767
[2024-01-22 10:04:36,733] INFO: Iter 79100 Summary: 
[2024-01-22 10:04:36,733] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043100795671343804
[2024-01-22 10:04:39,229] INFO: Iter 79200 Summary: 
[2024-01-22 10:04:39,229] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04344983596354723
[2024-01-22 10:04:41,917] INFO: Iter 79300 Summary: 
[2024-01-22 10:04:41,918] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043438025787472724
[2024-01-22 10:04:44,656] INFO: Iter 79400 Summary: 
[2024-01-22 10:04:44,657] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043301941640675065
[2024-01-22 10:04:47,192] INFO: Iter 79500 Summary: 
[2024-01-22 10:04:47,193] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043370608501136304
[2024-01-22 10:04:49,865] INFO: Iter 79600 Summary: 
[2024-01-22 10:04:49,865] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043274954594671726
[2024-01-22 10:04:52,355] INFO: Iter 79700 Summary: 
[2024-01-22 10:04:52,355] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04344586383551359
[2024-01-22 10:04:55,046] INFO: Iter 79800 Summary: 
[2024-01-22 10:04:55,046] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04296326566487551
[2024-01-22 10:04:57,755] INFO: Iter 79900 Summary: 
[2024-01-22 10:04:57,755] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04331802651286125
[2024-01-22 10:05:00,269] INFO: Iter 80000 Summary: 
[2024-01-22 10:05:00,270] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04341778337955475
[2024-01-22 10:05:02,870] INFO: Iter 80100 Summary: 
[2024-01-22 10:05:02,870] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043324856348335744
[2024-01-22 10:05:05,738] INFO: Iter 80200 Summary: 
[2024-01-22 10:05:05,739] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04387903071939945
[2024-01-22 10:05:08,382] INFO: Iter 80300 Summary: 
[2024-01-22 10:05:08,383] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04310453150421381
[2024-01-22 10:05:10,913] INFO: Iter 80400 Summary: 
[2024-01-22 10:05:10,913] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04320194095373154
[2024-01-22 10:05:13,609] INFO: Iter 80500 Summary: 
[2024-01-22 10:05:13,609] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04336755193769932
[2024-01-22 10:05:16,360] INFO: Iter 80600 Summary: 
[2024-01-22 10:05:16,360] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043691471219062805
[2024-01-22 10:05:18,878] INFO: Iter 80700 Summary: 
[2024-01-22 10:05:18,878] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04322824645787478
[2024-01-22 10:05:21,509] INFO: Iter 80800 Summary: 
[2024-01-22 10:05:21,509] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04307346262037754
[2024-01-22 10:05:24,171] INFO: Iter 80900 Summary: 
[2024-01-22 10:05:24,172] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04287023037672043
[2024-01-22 10:05:26,710] INFO: Iter 81000 Summary: 
[2024-01-22 10:05:26,710] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04352017804980278
[2024-01-22 10:05:29,559] INFO: Iter 81100 Summary: 
[2024-01-22 10:05:29,559] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043477116264402864
[2024-01-22 10:05:32,209] INFO: Iter 81200 Summary: 
[2024-01-22 10:05:32,209] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043386077918112276
[2024-01-22 10:05:34,749] INFO: Iter 81300 Summary: 
[2024-01-22 10:05:34,749] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04332069937139749
[2024-01-22 10:05:37,440] INFO: Iter 81400 Summary: 
[2024-01-22 10:05:37,441] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04306336987763643
[2024-01-22 10:05:40,049] INFO: Iter 81500 Summary: 
[2024-01-22 10:05:40,049] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043173496313393116
[2024-01-22 10:05:42,748] INFO: Iter 81600 Summary: 
[2024-01-22 10:05:42,748] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04299908302724362
[2024-01-22 10:05:45,392] INFO: Iter 81700 Summary: 
[2024-01-22 10:05:45,392] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04318819522857666
[2024-01-22 10:05:48,066] INFO: Iter 81800 Summary: 
[2024-01-22 10:05:48,066] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04338387787342071
[2024-01-22 10:05:50,724] INFO: Iter 81900 Summary: 
[2024-01-22 10:05:50,724] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04313376534730196
[2024-01-22 10:05:53,375] INFO: Iter 82000 Summary: 
[2024-01-22 10:05:53,375] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04333710122853517
[2024-01-22 10:05:55,886] INFO: Iter 82100 Summary: 
[2024-01-22 10:05:55,886] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043238696567714216
[2024-01-22 10:05:58,555] INFO: Iter 82200 Summary: 
[2024-01-22 10:05:58,555] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043076746240258215
[2024-01-22 10:06:01,090] INFO: Iter 82300 Summary: 
[2024-01-22 10:06:01,090] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043341299928724764
[2024-01-22 10:06:03,501] INFO: Iter 82400 Summary: 
[2024-01-22 10:06:03,501] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043276488855481146
[2024-01-22 10:06:06,201] INFO: Iter 82500 Summary: 
[2024-01-22 10:06:06,201] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0435031084343791
[2024-01-22 10:06:08,769] INFO: Iter 82600 Summary: 
[2024-01-22 10:06:08,769] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04304181817919016
[2024-01-22 10:06:11,354] INFO: Iter 82700 Summary: 
[2024-01-22 10:06:11,355] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043363542705774305
[2024-01-22 10:06:13,885] INFO: Iter 82800 Summary: 
[2024-01-22 10:06:13,885] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043156731426715854
[2024-01-22 10:06:16,668] INFO: Iter 82900 Summary: 
[2024-01-22 10:06:16,669] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316739670932293
[2024-01-22 10:06:19,183] INFO: Iter 83000 Summary: 
[2024-01-22 10:06:19,183] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0429914378747344
[2024-01-22 10:06:21,737] INFO: Iter 83100 Summary: 
[2024-01-22 10:06:21,737] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04319163527339697
[2024-01-22 10:06:24,586] INFO: Iter 83200 Summary: 
[2024-01-22 10:06:24,587] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043265821635723116
[2024-01-22 10:06:27,194] INFO: Iter 83300 Summary: 
[2024-01-22 10:06:27,194] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043258129768073555
[2024-01-22 10:06:29,713] INFO: Iter 83400 Summary: 
[2024-01-22 10:06:29,714] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04320436548441649
[2024-01-22 10:06:32,433] INFO: Iter 83500 Summary: 
[2024-01-22 10:06:32,433] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043263179138302806
[2024-01-22 10:06:35,074] INFO: Iter 83600 Summary: 
[2024-01-22 10:06:35,075] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04308877244591713
[2024-01-22 10:06:37,738] INFO: Iter 83700 Summary: 
[2024-01-22 10:06:37,738] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04335165403783321
[2024-01-22 10:06:40,391] INFO: Iter 83800 Summary: 
[2024-01-22 10:06:40,392] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043619922883808615
[2024-01-22 10:06:43,070] INFO: Iter 83900 Summary: 
[2024-01-22 10:06:43,070] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042966976165771484
[2024-01-22 10:06:45,612] INFO: Iter 84000 Summary: 
[2024-01-22 10:06:45,613] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04340187929570675
[2024-01-22 10:06:48,316] INFO: Iter 84100 Summary: 
[2024-01-22 10:06:48,316] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04296196147799492
[2024-01-22 10:06:50,950] INFO: Iter 84200 Summary: 
[2024-01-22 10:06:50,951] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043644308596849445
[2024-01-22 10:06:53,568] INFO: Iter 84300 Summary: 
[2024-01-22 10:06:53,568] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04310782108455896
[2024-01-22 10:06:56,108] INFO: Iter 84400 Summary: 
[2024-01-22 10:06:56,108] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042918666079640386
[2024-01-22 10:06:58,833] INFO: Iter 84500 Summary: 
[2024-01-22 10:06:58,833] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0431386411935091
[2024-01-22 10:07:01,301] INFO: Iter 84600 Summary: 
[2024-01-22 10:07:01,301] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04329924661666155
[2024-01-22 10:07:03,814] INFO: Iter 84700 Summary: 
[2024-01-22 10:07:03,814] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04325734861195087
[2024-01-22 10:07:06,361] INFO: Iter 84800 Summary: 
[2024-01-22 10:07:06,361] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04315462492406368
[2024-01-22 10:07:09,142] INFO: Iter 84900 Summary: 
[2024-01-22 10:07:09,143] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043586105071008206
[2024-01-22 10:07:11,797] INFO: Iter 85000 Summary: 
[2024-01-22 10:07:11,798] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04326200883835554
[2024-01-22 10:07:14,358] INFO: Iter 85100 Summary: 
[2024-01-22 10:07:14,358] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04331555165350437
[2024-01-22 10:07:17,107] INFO: Iter 85200 Summary: 
[2024-01-22 10:07:17,107] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04345186166465283
[2024-01-22 10:07:19,718] INFO: Iter 85300 Summary: 
[2024-01-22 10:07:19,718] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04304649155586958
[2024-01-22 10:07:22,150] INFO: Iter 85400 Summary: 
[2024-01-22 10:07:22,150] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043273971378803254
[2024-01-22 10:07:24,868] INFO: Iter 85500 Summary: 
[2024-01-22 10:07:24,868] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043018336668610574
[2024-01-22 10:07:27,643] INFO: Iter 85600 Summary: 
[2024-01-22 10:07:27,643] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0432618398219347
[2024-01-22 10:07:30,355] INFO: Iter 85700 Summary: 
[2024-01-22 10:07:30,355] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043540544100105764
[2024-01-22 10:07:33,008] INFO: Iter 85800 Summary: 
[2024-01-22 10:07:33,008] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042922158762812614
[2024-01-22 10:07:35,603] INFO: Iter 85900 Summary: 
[2024-01-22 10:07:35,603] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04356747444719076
[2024-01-22 10:07:38,208] INFO: Iter 86000 Summary: 
[2024-01-22 10:07:38,208] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043226278573274615
[2024-01-22 10:07:40,646] INFO: Iter 86100 Summary: 
[2024-01-22 10:07:40,646] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043047804087400436
[2024-01-22 10:07:43,332] INFO: Iter 86200 Summary: 
[2024-01-22 10:07:43,333] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043218520991504195
[2024-01-22 10:07:45,922] INFO: Iter 86300 Summary: 
[2024-01-22 10:07:45,922] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04350208781659603
[2024-01-22 10:07:48,393] INFO: Iter 86400 Summary: 
[2024-01-22 10:07:48,393] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0431664227694273
[2024-01-22 10:07:50,970] INFO: Iter 86500 Summary: 
[2024-01-22 10:07:50,970] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043682551570236684
[2024-01-22 10:07:53,610] INFO: Iter 86600 Summary: 
[2024-01-22 10:07:53,610] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0433344654366374
[2024-01-22 10:07:56,262] INFO: Iter 86700 Summary: 
[2024-01-22 10:07:56,262] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04333771932870149
[2024-01-22 10:07:58,798] INFO: Iter 86800 Summary: 
[2024-01-22 10:07:58,798] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04315823379904032
[2024-01-22 10:08:01,393] INFO: Iter 86900 Summary: 
[2024-01-22 10:08:01,393] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04313732668757439
[2024-01-22 10:08:03,996] INFO: Iter 87000 Summary: 
[2024-01-22 10:08:03,996] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04317031849175692
[2024-01-22 10:08:06,498] INFO: Iter 87100 Summary: 
[2024-01-22 10:08:06,499] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04277921169996261
[2024-01-22 10:08:09,030] INFO: Iter 87200 Summary: 
[2024-01-22 10:08:09,030] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04317280106246471
[2024-01-22 10:08:11,617] INFO: Iter 87300 Summary: 
[2024-01-22 10:08:11,617] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043119161166250704
[2024-01-22 10:08:14,417] INFO: Iter 87400 Summary: 
[2024-01-22 10:08:14,417] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04319918282330036
[2024-01-22 10:08:17,096] INFO: Iter 87500 Summary: 
[2024-01-22 10:08:17,096] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04315058883279562
[2024-01-22 10:08:19,656] INFO: Iter 87600 Summary: 
[2024-01-22 10:08:19,656] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043313830457627774
[2024-01-22 10:08:22,169] INFO: Iter 87700 Summary: 
[2024-01-22 10:08:22,169] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04302073687314987
[2024-01-22 10:08:24,637] INFO: Iter 87800 Summary: 
[2024-01-22 10:08:24,637] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04325413201004267
[2024-01-22 10:08:27,405] INFO: Iter 87900 Summary: 
[2024-01-22 10:08:27,405] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04309475600719452
[2024-01-22 10:08:30,029] INFO: Iter 88000 Summary: 
[2024-01-22 10:08:30,029] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04310368798673153
[2024-01-22 10:08:32,709] INFO: Iter 88100 Summary: 
[2024-01-22 10:08:32,709] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04324850734323263
[2024-01-22 10:08:35,348] INFO: Iter 88200 Summary: 
[2024-01-22 10:08:35,348] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043384199626743794
[2024-01-22 10:08:37,815] INFO: Iter 88300 Summary: 
[2024-01-22 10:08:37,816] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04322453964501619
[2024-01-22 10:08:40,342] INFO: Iter 88400 Summary: 
[2024-01-22 10:08:40,342] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04333636246621609
[2024-01-22 10:08:42,837] INFO: Iter 88500 Summary: 
[2024-01-22 10:08:42,837] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04330156359821558
[2024-01-22 10:08:45,477] INFO: Iter 88600 Summary: 
[2024-01-22 10:08:45,477] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04286964569240809
[2024-01-22 10:08:48,028] INFO: Iter 88700 Summary: 
[2024-01-22 10:08:48,028] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04335272416472435
[2024-01-22 10:08:50,522] INFO: Iter 88800 Summary: 
[2024-01-22 10:08:50,522] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04311080291867256
[2024-01-22 10:08:53,162] INFO: Iter 88900 Summary: 
[2024-01-22 10:08:53,162] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04317074034363031
[2024-01-22 10:08:55,761] INFO: Iter 89000 Summary: 
[2024-01-22 10:08:55,761] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04325152799487114
[2024-01-22 10:08:58,358] INFO: Iter 89100 Summary: 
[2024-01-22 10:08:58,358] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04304012443870306
[2024-01-22 10:09:01,123] INFO: Iter 89200 Summary: 
[2024-01-22 10:09:01,124] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04321116678416729
[2024-01-22 10:09:03,681] INFO: Iter 89300 Summary: 
[2024-01-22 10:09:03,682] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04305423453450203
[2024-01-22 10:09:06,401] INFO: Iter 89400 Summary: 
[2024-01-22 10:09:06,401] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04288470521569252
[2024-01-22 10:09:09,046] INFO: Iter 89500 Summary: 
[2024-01-22 10:09:09,046] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043387329168617726
[2024-01-22 10:09:11,622] INFO: Iter 89600 Summary: 
[2024-01-22 10:09:11,622] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04343601144850254
[2024-01-22 10:09:14,176] INFO: Iter 89700 Summary: 
[2024-01-22 10:09:14,177] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04319944560527802
[2024-01-22 10:09:16,557] INFO: Iter 89800 Summary: 
[2024-01-22 10:09:16,557] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04308092746883631
[2024-01-22 10:09:19,246] INFO: Iter 89900 Summary: 
[2024-01-22 10:09:19,246] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04275986358523369
[2024-01-22 10:09:22,082] INFO: Iter 90000 Summary: 
[2024-01-22 10:09:22,082] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04355361577123404
[2024-01-22 10:09:24,648] INFO: Iter 90100 Summary: 
[2024-01-22 10:09:24,648] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04339659307152033
[2024-01-22 10:09:27,032] INFO: Iter 90200 Summary: 
[2024-01-22 10:09:27,032] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043297288082540034
[2024-01-22 10:09:29,809] INFO: Iter 90300 Summary: 
[2024-01-22 10:09:29,809] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043398142904043195
[2024-01-22 10:09:32,498] INFO: Iter 90400 Summary: 
[2024-01-22 10:09:32,498] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04343963135033846
[2024-01-22 10:09:35,046] INFO: Iter 90500 Summary: 
[2024-01-22 10:09:35,046] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043254110254347326
[2024-01-22 10:09:37,638] INFO: Iter 90600 Summary: 
[2024-01-22 10:09:37,638] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04297286033630371
[2024-01-22 10:09:40,281] INFO: Iter 90700 Summary: 
[2024-01-22 10:09:40,281] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04322823740541935
[2024-01-22 10:09:42,791] INFO: Iter 90800 Summary: 
[2024-01-22 10:09:42,791] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043336690068244935
[2024-01-22 10:09:45,432] INFO: Iter 90900 Summary: 
[2024-01-22 10:09:45,432] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04310478530824184
[2024-01-22 10:09:48,080] INFO: Iter 91000 Summary: 
[2024-01-22 10:09:48,080] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04323583390563726
[2024-01-22 10:09:50,809] INFO: Iter 91100 Summary: 
[2024-01-22 10:09:50,810] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043125937320292
[2024-01-22 10:09:53,565] INFO: Iter 91200 Summary: 
[2024-01-22 10:09:53,565] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042877942137420175
[2024-01-22 10:09:56,158] INFO: Iter 91300 Summary: 
[2024-01-22 10:09:56,159] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043088845275342465
[2024-01-22 10:09:58,856] INFO: Iter 91400 Summary: 
[2024-01-22 10:09:58,856] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043168390803039074
[2024-01-22 10:10:01,460] INFO: Iter 91500 Summary: 
[2024-01-22 10:10:01,461] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043309728465974334
[2024-01-22 10:10:04,174] INFO: Iter 91600 Summary: 
[2024-01-22 10:10:04,174] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04290829461067915
[2024-01-22 10:10:06,876] INFO: Iter 91700 Summary: 
[2024-01-22 10:10:06,876] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04320563051849604
[2024-01-22 10:10:09,360] INFO: Iter 91800 Summary: 
[2024-01-22 10:10:09,360] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0431768436357379
[2024-01-22 10:10:12,188] INFO: Iter 91900 Summary: 
[2024-01-22 10:10:12,188] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04292271785438061
[2024-01-22 10:10:14,872] INFO: Iter 92000 Summary: 
[2024-01-22 10:10:14,872] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04301983218640089
[2024-01-22 10:10:17,572] INFO: Iter 92100 Summary: 
[2024-01-22 10:10:17,572] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042979754097759726
[2024-01-22 10:10:20,124] INFO: Iter 92200 Summary: 
[2024-01-22 10:10:20,125] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043001458197832104
[2024-01-22 10:10:22,673] INFO: Iter 92300 Summary: 
[2024-01-22 10:10:22,673] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04309539910405874
[2024-01-22 10:10:25,306] INFO: Iter 92400 Summary: 
[2024-01-22 10:10:25,306] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042555616423487666
[2024-01-22 10:10:27,903] INFO: Iter 92500 Summary: 
[2024-01-22 10:10:27,904] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04291746698319912
[2024-01-22 10:10:30,511] INFO: Iter 92600 Summary: 
[2024-01-22 10:10:30,512] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042955327592790124
[2024-01-22 10:10:33,106] INFO: Iter 92700 Summary: 
[2024-01-22 10:10:33,106] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043268035911023614
[2024-01-22 10:10:35,815] INFO: Iter 92800 Summary: 
[2024-01-22 10:10:35,815] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043220859318971634
[2024-01-22 10:10:38,523] INFO: Iter 92900 Summary: 
[2024-01-22 10:10:38,524] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316974252462387
[2024-01-22 10:10:41,183] INFO: Iter 93000 Summary: 
[2024-01-22 10:10:41,183] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04315080694854259
[2024-01-22 10:10:43,755] INFO: Iter 93100 Summary: 
[2024-01-22 10:10:43,755] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043085405826568605
[2024-01-22 10:10:46,313] INFO: Iter 93200 Summary: 
[2024-01-22 10:10:46,313] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04313212338835001
[2024-01-22 10:10:48,947] INFO: Iter 93300 Summary: 
[2024-01-22 10:10:48,948] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04292000837624073
[2024-01-22 10:10:51,610] INFO: Iter 93400 Summary: 
[2024-01-22 10:10:51,610] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043217647820711136
[2024-01-22 10:10:54,279] INFO: Iter 93500 Summary: 
[2024-01-22 10:10:54,280] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043055121786892414
[2024-01-22 10:10:56,883] INFO: Iter 93600 Summary: 
[2024-01-22 10:10:56,883] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04292981516569853
[2024-01-22 10:10:59,556] INFO: Iter 93700 Summary: 
[2024-01-22 10:10:59,556] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043116305842995646
[2024-01-22 10:11:02,082] INFO: Iter 93800 Summary: 
[2024-01-22 10:11:02,082] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042978203073143956
[2024-01-22 10:11:04,597] INFO: Iter 93900 Summary: 
[2024-01-22 10:11:04,597] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04285787481814623
[2024-01-22 10:11:07,197] INFO: Iter 94000 Summary: 
[2024-01-22 10:11:07,197] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043031905479729175
[2024-01-22 10:11:09,745] INFO: Iter 94100 Summary: 
[2024-01-22 10:11:09,746] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04307676371186972
[2024-01-22 10:11:12,402] INFO: Iter 94200 Summary: 
[2024-01-22 10:11:12,402] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04312122482806444
[2024-01-22 10:11:15,076] INFO: Iter 94300 Summary: 
[2024-01-22 10:11:15,076] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042885134443640706
[2024-01-22 10:11:17,797] INFO: Iter 94400 Summary: 
[2024-01-22 10:11:17,798] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04292520698159933
[2024-01-22 10:11:20,181] INFO: Iter 94500 Summary: 
[2024-01-22 10:11:20,181] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04323049284517765
[2024-01-22 10:11:22,688] INFO: Iter 94600 Summary: 
[2024-01-22 10:11:22,688] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04299304258078337
[2024-01-22 10:11:25,285] INFO: Iter 94700 Summary: 
[2024-01-22 10:11:25,285] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04335673943161964
[2024-01-22 10:11:27,794] INFO: Iter 94800 Summary: 
[2024-01-22 10:11:27,794] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042882799617946145
[2024-01-22 10:11:30,315] INFO: Iter 94900 Summary: 
[2024-01-22 10:11:30,315] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04345646802335978
[2024-01-22 10:11:33,054] INFO: Iter 95000 Summary: 
[2024-01-22 10:11:33,054] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043283935748040674
[2024-01-22 10:11:35,654] INFO: Iter 95100 Summary: 
[2024-01-22 10:11:35,654] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04293009933084249
[2024-01-22 10:11:38,087] INFO: Iter 95200 Summary: 
[2024-01-22 10:11:38,088] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043103596828877926
[2024-01-22 10:11:40,742] INFO: Iter 95300 Summary: 
[2024-01-22 10:11:40,742] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042899102121591565
[2024-01-22 10:11:43,403] INFO: Iter 95400 Summary: 
[2024-01-22 10:11:43,403] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04287375964224339
[2024-01-22 10:11:46,193] INFO: Iter 95500 Summary: 
[2024-01-22 10:11:46,193] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04291703823953867
[2024-01-22 10:11:48,704] INFO: Iter 95600 Summary: 
[2024-01-22 10:11:48,704] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043271424509584905
[2024-01-22 10:11:51,336] INFO: Iter 95700 Summary: 
[2024-01-22 10:11:51,336] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04311136823147535
[2024-01-22 10:11:53,996] INFO: Iter 95800 Summary: 
[2024-01-22 10:11:53,997] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0429536659270525
[2024-01-22 10:11:56,532] INFO: Iter 95900 Summary: 
[2024-01-22 10:11:56,532] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04259028803557158
[2024-01-22 10:11:59,293] INFO: Iter 96000 Summary: 
[2024-01-22 10:11:59,293] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0431079064309597
[2024-01-22 10:12:01,887] INFO: Iter 96100 Summary: 
[2024-01-22 10:12:01,887] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0429584676772356
[2024-01-22 10:12:04,406] INFO: Iter 96200 Summary: 
[2024-01-22 10:12:04,407] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04322375386953354
[2024-01-22 10:12:07,028] INFO: Iter 96300 Summary: 
[2024-01-22 10:12:07,028] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04310949623584747
[2024-01-22 10:12:09,550] INFO: Iter 96400 Summary: 
[2024-01-22 10:12:09,550] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042975839674472806
[2024-01-22 10:12:12,071] INFO: Iter 96500 Summary: 
[2024-01-22 10:12:12,072] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04295468334108591
[2024-01-22 10:12:14,782] INFO: Iter 96600 Summary: 
[2024-01-22 10:12:14,782] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043284780792891976
[2024-01-22 10:12:17,524] INFO: Iter 96700 Summary: 
[2024-01-22 10:12:17,524] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04303605068475008
[2024-01-22 10:12:20,186] INFO: Iter 96800 Summary: 
[2024-01-22 10:12:20,186] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0430104573071003
[2024-01-22 10:12:22,544] INFO: Iter 96900 Summary: 
[2024-01-22 10:12:22,544] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042865448147058484
[2024-01-22 10:12:25,334] INFO: Iter 97000 Summary: 
[2024-01-22 10:12:25,334] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043067288026213645
[2024-01-22 10:12:27,845] INFO: Iter 97100 Summary: 
[2024-01-22 10:12:27,846] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043060202412307264
[2024-01-22 10:12:30,319] INFO: Iter 97200 Summary: 
[2024-01-22 10:12:30,319] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04277464333921671
[2024-01-22 10:12:32,928] INFO: Iter 97300 Summary: 
[2024-01-22 10:12:32,928] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043070344068109986
[2024-01-22 10:12:35,715] INFO: Iter 97400 Summary: 
[2024-01-22 10:12:35,715] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04316566705703735
[2024-01-22 10:12:38,356] INFO: Iter 97500 Summary: 
[2024-01-22 10:12:38,356] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042782875038683416
[2024-01-22 10:12:40,835] INFO: Iter 97600 Summary: 
[2024-01-22 10:12:40,835] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04281378485262394
[2024-01-22 10:12:43,504] INFO: Iter 97700 Summary: 
[2024-01-22 10:12:43,505] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043427679464221
[2024-01-22 10:12:46,093] INFO: Iter 97800 Summary: 
[2024-01-22 10:12:46,093] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04292865857481956
[2024-01-22 10:12:48,591] INFO: Iter 97900 Summary: 
[2024-01-22 10:12:48,591] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04296314276754856
[2024-01-22 10:12:51,300] INFO: Iter 98000 Summary: 
[2024-01-22 10:12:51,301] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04303392853587866
[2024-01-22 10:12:54,070] INFO: Iter 98100 Summary: 
[2024-01-22 10:12:54,070] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04301890756934881
[2024-01-22 10:12:56,771] INFO: Iter 98200 Summary: 
[2024-01-22 10:12:56,771] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04330902609974146
[2024-01-22 10:12:59,299] INFO: Iter 98300 Summary: 
[2024-01-22 10:12:59,299] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042956078946590426
[2024-01-22 10:13:01,783] INFO: Iter 98400 Summary: 
[2024-01-22 10:13:01,783] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04275156866759062
[2024-01-22 10:13:04,506] INFO: Iter 98500 Summary: 
[2024-01-22 10:13:04,506] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042989667989313606
[2024-01-22 10:13:07,049] INFO: Iter 98600 Summary: 
[2024-01-22 10:13:07,049] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04304056666791439
[2024-01-22 10:13:09,654] INFO: Iter 98700 Summary: 
[2024-01-22 10:13:09,654] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.042823185846209526
[2024-01-22 10:13:12,197] INFO: Iter 98800 Summary: 
[2024-01-22 10:13:12,197] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04296360779553652
[2024-01-22 10:13:14,753] INFO: Iter 98900 Summary: 
[2024-01-22 10:13:14,753] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04268833126872778
[2024-01-22 10:13:17,400] INFO: Iter 99000 Summary: 
[2024-01-22 10:13:17,400] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04300332766026258
[2024-01-22 10:13:20,137] INFO: Iter 99100 Summary: 
[2024-01-22 10:13:20,138] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04257625479251146
[2024-01-22 10:13:22,793] INFO: Iter 99200 Summary: 
[2024-01-22 10:13:22,793] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04301933154463768
[2024-01-22 10:13:25,263] INFO: Iter 99300 Summary: 
[2024-01-22 10:13:25,263] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.043261042386293413
[2024-01-22 10:13:27,805] INFO: Iter 99400 Summary: 
[2024-01-22 10:13:27,805] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04324970062822103
[2024-01-22 10:13:30,346] INFO: Iter 99500 Summary: 
[2024-01-22 10:13:30,346] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04272452000528574
[2024-01-22 10:13:32,843] INFO: Iter 99600 Summary: 
[2024-01-22 10:13:32,843] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04277571704238653
[2024-01-22 10:13:35,637] INFO: Iter 99700 Summary: 
[2024-01-22 10:13:35,637] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0428946328908205
[2024-01-22 10:13:38,203] INFO: Iter 99800 Summary: 
[2024-01-22 10:13:38,203] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04290564429014921
[2024-01-22 10:13:40,738] INFO: Iter 99900 Summary: 
[2024-01-22 10:13:40,738] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04306137319654226
[2024-01-22 10:13:43,429] INFO: Iter 100000 Summary: 
[2024-01-22 10:13:43,429] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.04294578358530998
[2024-01-22 10:13:46,112] INFO: Iter 100100 Summary: 
[2024-01-22 10:13:46,112] INFO: 	 lr: 1.2900000000000026e-05 	 Training loss: 0.04227311726659536
[2024-01-22 10:13:48,798] INFO: Iter 100200 Summary: 
[2024-01-22 10:13:48,798] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04157548647373915
[2024-01-22 10:13:51,322] INFO: Iter 100300 Summary: 
[2024-01-22 10:13:51,322] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.041165013164281845
[2024-01-22 10:13:54,106] INFO: Iter 100400 Summary: 
[2024-01-22 10:13:54,107] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.041101014651358125
[2024-01-22 10:13:56,743] INFO: Iter 100500 Summary: 
[2024-01-22 10:13:56,743] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04072362016886473
[2024-01-22 10:13:59,257] INFO: Iter 100600 Summary: 
[2024-01-22 10:13:59,257] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04099282037466764
[2024-01-22 10:14:01,806] INFO: Iter 100700 Summary: 
[2024-01-22 10:14:01,806] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04089278191328049
[2024-01-22 10:14:04,549] INFO: Iter 100800 Summary: 
[2024-01-22 10:14:04,549] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04073826640844345
[2024-01-22 10:14:07,184] INFO: Iter 100900 Summary: 
[2024-01-22 10:14:07,184] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040702383182942864
[2024-01-22 10:14:09,835] INFO: Iter 101000 Summary: 
[2024-01-22 10:14:09,835] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040941313430666926
[2024-01-22 10:14:12,515] INFO: Iter 101100 Summary: 
[2024-01-22 10:14:12,515] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040753800086677075
[2024-01-22 10:14:15,088] INFO: Iter 101200 Summary: 
[2024-01-22 10:14:15,088] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0407068681716919
[2024-01-22 10:14:17,864] INFO: Iter 101300 Summary: 
[2024-01-22 10:14:17,864] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040564885064959526
[2024-01-22 10:14:20,552] INFO: Iter 101400 Summary: 
[2024-01-22 10:14:20,552] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04077101893723011
[2024-01-22 10:14:23,223] INFO: Iter 101500 Summary: 
[2024-01-22 10:14:23,223] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040523022636771204
[2024-01-22 10:14:25,880] INFO: Iter 101600 Summary: 
[2024-01-22 10:14:25,880] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04035461995750666
[2024-01-22 10:14:28,527] INFO: Iter 101700 Summary: 
[2024-01-22 10:14:28,527] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040829578675329686
[2024-01-22 10:14:31,082] INFO: Iter 101800 Summary: 
[2024-01-22 10:14:31,082] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040553195625543596
[2024-01-22 10:14:33,766] INFO: Iter 101900 Summary: 
[2024-01-22 10:14:33,767] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040794193744659424
[2024-01-22 10:14:36,443] INFO: Iter 102000 Summary: 
[2024-01-22 10:14:36,443] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04063266590237617
[2024-01-22 10:14:39,006] INFO: Iter 102100 Summary: 
[2024-01-22 10:14:39,006] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04041300591081381
[2024-01-22 10:14:41,719] INFO: Iter 102200 Summary: 
[2024-01-22 10:14:41,719] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04071965999901295
[2024-01-22 10:14:44,346] INFO: Iter 102300 Summary: 
[2024-01-22 10:14:44,347] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040508773736655714
[2024-01-22 10:14:46,923] INFO: Iter 102400 Summary: 
[2024-01-22 10:14:46,923] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040250834561884406
[2024-01-22 10:14:49,539] INFO: Iter 102500 Summary: 
[2024-01-22 10:14:49,539] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040643370151519774
[2024-01-22 10:14:52,022] INFO: Iter 102600 Summary: 
[2024-01-22 10:14:52,022] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04039568096399307
[2024-01-22 10:14:54,592] INFO: Iter 102700 Summary: 
[2024-01-22 10:14:54,592] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04044818002730608
[2024-01-22 10:14:57,204] INFO: Iter 102800 Summary: 
[2024-01-22 10:14:57,204] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04047536760568619
[2024-01-22 10:14:59,859] INFO: Iter 102900 Summary: 
[2024-01-22 10:14:59,859] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04053180828690529
[2024-01-22 10:15:02,361] INFO: Iter 103000 Summary: 
[2024-01-22 10:15:02,361] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040494356639683246
[2024-01-22 10:15:05,028] INFO: Iter 103100 Summary: 
[2024-01-22 10:15:05,028] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04044917479157448
[2024-01-22 10:15:07,864] INFO: Iter 103200 Summary: 
[2024-01-22 10:15:07,865] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04030680689960718
[2024-01-22 10:15:10,367] INFO: Iter 103300 Summary: 
[2024-01-22 10:15:10,367] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04022579275071621
[2024-01-22 10:15:12,952] INFO: Iter 103400 Summary: 
[2024-01-22 10:15:12,952] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040266768373548985
[2024-01-22 10:15:15,651] INFO: Iter 103500 Summary: 
[2024-01-22 10:15:15,651] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04019772440195084
[2024-01-22 10:15:18,206] INFO: Iter 103600 Summary: 
[2024-01-22 10:15:18,206] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04007277436554432
[2024-01-22 10:15:20,908] INFO: Iter 103700 Summary: 
[2024-01-22 10:15:20,908] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04024839237332344
[2024-01-22 10:15:23,540] INFO: Iter 103800 Summary: 
[2024-01-22 10:15:23,540] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04047501094639301
[2024-01-22 10:15:26,030] INFO: Iter 103900 Summary: 
[2024-01-22 10:15:26,030] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040105661414563655
[2024-01-22 10:15:28,545] INFO: Iter 104000 Summary: 
[2024-01-22 10:15:28,545] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04050005353987217
[2024-01-22 10:15:31,238] INFO: Iter 104100 Summary: 
[2024-01-22 10:15:31,238] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04028706055134535
[2024-01-22 10:15:33,798] INFO: Iter 104200 Summary: 
[2024-01-22 10:15:33,798] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040267769135534766
[2024-01-22 10:15:36,329] INFO: Iter 104300 Summary: 
[2024-01-22 10:15:36,329] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040175254307687286
[2024-01-22 10:15:38,952] INFO: Iter 104400 Summary: 
[2024-01-22 10:15:38,953] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04038047727197409
[2024-01-22 10:15:41,597] INFO: Iter 104500 Summary: 
[2024-01-22 10:15:41,597] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04042597852647305
[2024-01-22 10:15:44,179] INFO: Iter 104600 Summary: 
[2024-01-22 10:15:44,179] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040156995989382265
[2024-01-22 10:15:46,662] INFO: Iter 104700 Summary: 
[2024-01-22 10:15:46,662] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04017147731035948
[2024-01-22 10:15:49,302] INFO: Iter 104800 Summary: 
[2024-01-22 10:15:49,302] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040232486724853515
[2024-01-22 10:15:52,053] INFO: Iter 104900 Summary: 
[2024-01-22 10:15:52,053] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04036292292177677
[2024-01-22 10:15:54,627] INFO: Iter 105000 Summary: 
[2024-01-22 10:15:54,627] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04012374147772789
[2024-01-22 10:15:57,267] INFO: Iter 105100 Summary: 
[2024-01-22 10:15:57,267] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03996197383850813
[2024-01-22 10:15:59,835] INFO: Iter 105200 Summary: 
[2024-01-22 10:15:59,835] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039784264750778675
[2024-01-22 10:16:02,309] INFO: Iter 105300 Summary: 
[2024-01-22 10:16:02,309] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040155358873307706
[2024-01-22 10:16:05,116] INFO: Iter 105400 Summary: 
[2024-01-22 10:16:05,116] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039931536987423896
[2024-01-22 10:16:07,882] INFO: Iter 105500 Summary: 
[2024-01-22 10:16:07,882] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03976633671671152
[2024-01-22 10:16:10,474] INFO: Iter 105600 Summary: 
[2024-01-22 10:16:10,474] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04015302963554859
[2024-01-22 10:16:12,965] INFO: Iter 105700 Summary: 
[2024-01-22 10:16:12,965] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04012962710112333
[2024-01-22 10:16:15,489] INFO: Iter 105800 Summary: 
[2024-01-22 10:16:15,490] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039974552877247334
[2024-01-22 10:16:18,094] INFO: Iter 105900 Summary: 
[2024-01-22 10:16:18,095] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04013821318745613
[2024-01-22 10:16:20,686] INFO: Iter 106000 Summary: 
[2024-01-22 10:16:20,686] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040051052868366244
[2024-01-22 10:16:23,221] INFO: Iter 106100 Summary: 
[2024-01-22 10:16:23,221] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04043584737926722
[2024-01-22 10:16:25,832] INFO: Iter 106200 Summary: 
[2024-01-22 10:16:25,832] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040309925638139246
[2024-01-22 10:16:28,281] INFO: Iter 106300 Summary: 
[2024-01-22 10:16:28,281] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04009773880243302
[2024-01-22 10:16:30,721] INFO: Iter 106400 Summary: 
[2024-01-22 10:16:30,722] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0401270530372858
[2024-01-22 10:16:33,405] INFO: Iter 106500 Summary: 
[2024-01-22 10:16:33,405] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040074494034051895
[2024-01-22 10:16:36,001] INFO: Iter 106600 Summary: 
[2024-01-22 10:16:36,001] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040223937779664994
[2024-01-22 10:16:38,578] INFO: Iter 106700 Summary: 
[2024-01-22 10:16:38,578] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04002296406775713
[2024-01-22 10:16:41,322] INFO: Iter 106800 Summary: 
[2024-01-22 10:16:41,322] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040056047700345515
[2024-01-22 10:16:43,826] INFO: Iter 106900 Summary: 
[2024-01-22 10:16:43,826] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04018251739442349
[2024-01-22 10:16:46,343] INFO: Iter 107000 Summary: 
[2024-01-22 10:16:46,344] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040063557699322704
[2024-01-22 10:16:48,892] INFO: Iter 107100 Summary: 
[2024-01-22 10:16:48,892] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04021052673459053
[2024-01-22 10:16:51,502] INFO: Iter 107200 Summary: 
[2024-01-22 10:16:51,502] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040275937728583816
[2024-01-22 10:16:54,174] INFO: Iter 107300 Summary: 
[2024-01-22 10:16:54,174] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03991939172148704
[2024-01-22 10:16:56,777] INFO: Iter 107400 Summary: 
[2024-01-22 10:16:56,777] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04002468086779118
[2024-01-22 10:16:59,473] INFO: Iter 107500 Summary: 
[2024-01-22 10:16:59,473] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04004152212291956
[2024-01-22 10:17:02,094] INFO: Iter 107600 Summary: 
[2024-01-22 10:17:02,094] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04000437773764134
[2024-01-22 10:17:04,639] INFO: Iter 107700 Summary: 
[2024-01-22 10:17:04,639] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040287632308900354
[2024-01-22 10:17:07,223] INFO: Iter 107800 Summary: 
[2024-01-22 10:17:07,224] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040018900223076345
[2024-01-22 10:17:09,877] INFO: Iter 107900 Summary: 
[2024-01-22 10:17:09,877] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04020594537258148
[2024-01-22 10:17:12,429] INFO: Iter 108000 Summary: 
[2024-01-22 10:17:12,429] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04013325307518244
[2024-01-22 10:17:15,116] INFO: Iter 108100 Summary: 
[2024-01-22 10:17:15,116] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0399148790538311
[2024-01-22 10:17:17,927] INFO: Iter 108200 Summary: 
[2024-01-22 10:17:17,927] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040074815489351746
[2024-01-22 10:17:20,481] INFO: Iter 108300 Summary: 
[2024-01-22 10:17:20,481] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040384174510836604
[2024-01-22 10:17:22,985] INFO: Iter 108400 Summary: 
[2024-01-22 10:17:22,986] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03984367709606886
[2024-01-22 10:17:25,581] INFO: Iter 108500 Summary: 
[2024-01-22 10:17:25,581] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039974839873611924
[2024-01-22 10:17:28,184] INFO: Iter 108600 Summary: 
[2024-01-22 10:17:28,184] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039883755184710025
[2024-01-22 10:17:30,819] INFO: Iter 108700 Summary: 
[2024-01-22 10:17:30,819] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0399785091355443
[2024-01-22 10:17:33,599] INFO: Iter 108800 Summary: 
[2024-01-22 10:17:33,599] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04020016845315695
[2024-01-22 10:17:36,153] INFO: Iter 108900 Summary: 
[2024-01-22 10:17:36,153] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04005346059799195
[2024-01-22 10:17:38,886] INFO: Iter 109000 Summary: 
[2024-01-22 10:17:38,886] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03999957337975502
[2024-01-22 10:17:41,396] INFO: Iter 109100 Summary: 
[2024-01-22 10:17:41,397] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04005138326436281
[2024-01-22 10:17:44,091] INFO: Iter 109200 Summary: 
[2024-01-22 10:17:44,091] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040292243734002114
[2024-01-22 10:17:46,617] INFO: Iter 109300 Summary: 
[2024-01-22 10:17:46,617] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0400603761151433
[2024-01-22 10:17:49,193] INFO: Iter 109400 Summary: 
[2024-01-22 10:17:49,193] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039972497411072255
[2024-01-22 10:17:51,775] INFO: Iter 109500 Summary: 
[2024-01-22 10:17:51,775] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0400383347645402
[2024-01-22 10:17:54,398] INFO: Iter 109600 Summary: 
[2024-01-22 10:17:54,399] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039996765591204166
[2024-01-22 10:17:56,757] INFO: Iter 109700 Summary: 
[2024-01-22 10:17:56,757] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03994880005717277
[2024-01-22 10:17:59,388] INFO: Iter 109800 Summary: 
[2024-01-22 10:17:59,388] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03979857437312603
[2024-01-22 10:18:02,102] INFO: Iter 109900 Summary: 
[2024-01-22 10:18:02,102] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03985854960978031
[2024-01-22 10:18:04,724] INFO: Iter 110000 Summary: 
[2024-01-22 10:18:04,724] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03975712060928345
[2024-01-22 10:18:07,334] INFO: Iter 110100 Summary: 
[2024-01-22 10:18:07,334] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03988467399030924
[2024-01-22 10:18:10,044] INFO: Iter 110200 Summary: 
[2024-01-22 10:18:10,045] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03999328568577767
[2024-01-22 10:18:12,612] INFO: Iter 110300 Summary: 
[2024-01-22 10:18:12,612] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04000758942216635
[2024-01-22 10:18:15,184] INFO: Iter 110400 Summary: 
[2024-01-22 10:18:15,184] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039921127147972586
[2024-01-22 10:18:17,912] INFO: Iter 110500 Summary: 
[2024-01-22 10:18:17,912] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039724213778972624
[2024-01-22 10:18:20,525] INFO: Iter 110600 Summary: 
[2024-01-22 10:18:20,526] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04005661830306053
[2024-01-22 10:18:23,042] INFO: Iter 110700 Summary: 
[2024-01-22 10:18:23,043] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039673284627497196
[2024-01-22 10:18:25,692] INFO: Iter 110800 Summary: 
[2024-01-22 10:18:25,692] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039702047891914845
[2024-01-22 10:18:28,306] INFO: Iter 110900 Summary: 
[2024-01-22 10:18:28,306] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03969644404947758
[2024-01-22 10:18:30,857] INFO: Iter 111000 Summary: 
[2024-01-22 10:18:30,857] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04001311205327511
[2024-01-22 10:18:33,310] INFO: Iter 111100 Summary: 
[2024-01-22 10:18:33,310] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0399482948333025
[2024-01-22 10:18:35,941] INFO: Iter 111200 Summary: 
[2024-01-22 10:18:35,941] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039993746913969515
[2024-01-22 10:18:38,701] INFO: Iter 111300 Summary: 
[2024-01-22 10:18:38,701] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040116167291998864
[2024-01-22 10:18:41,264] INFO: Iter 111400 Summary: 
[2024-01-22 10:18:41,265] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03989323142915964
[2024-01-22 10:18:43,871] INFO: Iter 111500 Summary: 
[2024-01-22 10:18:43,871] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03994831338524819
[2024-01-22 10:18:46,548] INFO: Iter 111600 Summary: 
[2024-01-22 10:18:46,548] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04008302867412567
[2024-01-22 10:18:49,114] INFO: Iter 111700 Summary: 
[2024-01-22 10:18:49,114] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03974271059036255
[2024-01-22 10:18:51,676] INFO: Iter 111800 Summary: 
[2024-01-22 10:18:51,677] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03970606680959463
[2024-01-22 10:18:54,529] INFO: Iter 111900 Summary: 
[2024-01-22 10:18:54,529] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03963945060968399
[2024-01-22 10:18:57,054] INFO: Iter 112000 Summary: 
[2024-01-22 10:18:57,054] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040008683875203135
[2024-01-22 10:18:59,511] INFO: Iter 112100 Summary: 
[2024-01-22 10:18:59,511] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04008214376866817
[2024-01-22 10:19:02,175] INFO: Iter 112200 Summary: 
[2024-01-22 10:19:02,175] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03981588292866945
[2024-01-22 10:19:04,756] INFO: Iter 112300 Summary: 
[2024-01-22 10:19:04,757] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03986079640686512
[2024-01-22 10:19:07,372] INFO: Iter 112400 Summary: 
[2024-01-22 10:19:07,372] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.040100014247000215
[2024-01-22 10:19:09,997] INFO: Iter 112500 Summary: 
[2024-01-22 10:19:09,997] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04029418382793665
[2024-01-22 10:19:12,609] INFO: Iter 112600 Summary: 
[2024-01-22 10:19:12,609] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03982728563249111
[2024-01-22 10:19:15,202] INFO: Iter 112700 Summary: 
[2024-01-22 10:19:15,202] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039764541126787664
[2024-01-22 10:19:17,878] INFO: Iter 112800 Summary: 
[2024-01-22 10:19:17,878] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04009889606386423
[2024-01-22 10:19:20,622] INFO: Iter 112900 Summary: 
[2024-01-22 10:19:20,623] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03980415090918541
[2024-01-22 10:19:23,356] INFO: Iter 113000 Summary: 
[2024-01-22 10:19:23,356] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03965019136667251
[2024-01-22 10:19:25,933] INFO: Iter 113100 Summary: 
[2024-01-22 10:19:25,933] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03996534157544374
[2024-01-22 10:19:28,607] INFO: Iter 113200 Summary: 
[2024-01-22 10:19:28,608] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039863692820072176
[2024-01-22 10:19:31,264] INFO: Iter 113300 Summary: 
[2024-01-22 10:19:31,264] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03992135666310787
[2024-01-22 10:19:33,865] INFO: Iter 113400 Summary: 
[2024-01-22 10:19:33,865] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039793025590479376
[2024-01-22 10:19:36,573] INFO: Iter 113500 Summary: 
[2024-01-22 10:19:36,574] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03992627732455731
[2024-01-22 10:19:39,117] INFO: Iter 113600 Summary: 
[2024-01-22 10:19:39,118] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039997478239238264
[2024-01-22 10:19:41,670] INFO: Iter 113700 Summary: 
[2024-01-22 10:19:41,670] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03973979875445366
[2024-01-22 10:19:44,094] INFO: Iter 113800 Summary: 
[2024-01-22 10:19:44,094] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039934299290180206
[2024-01-22 10:19:46,814] INFO: Iter 113900 Summary: 
[2024-01-22 10:19:46,815] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039982785135507584
[2024-01-22 10:19:49,531] INFO: Iter 114000 Summary: 
[2024-01-22 10:19:49,531] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03986601609736681
[2024-01-22 10:19:52,170] INFO: Iter 114100 Summary: 
[2024-01-22 10:19:52,170] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03985286340117455
[2024-01-22 10:19:54,802] INFO: Iter 114200 Summary: 
[2024-01-22 10:19:54,802] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04001198336482048
[2024-01-22 10:19:57,499] INFO: Iter 114300 Summary: 
[2024-01-22 10:19:57,499] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04003469403833151
[2024-01-22 10:20:00,289] INFO: Iter 114400 Summary: 
[2024-01-22 10:20:00,289] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.039729645773768425
[2024-01-22 10:20:02,898] INFO: Iter 114500 Summary: 
[2024-01-22 10:20:02,898] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.04008794769644737
[2024-01-22 10:20:05,677] INFO: Iter 114600 Summary: 
[2024-01-22 10:20:05,677] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03984731771051884
[2024-01-22 10:20:08,470] INFO: Iter 114700 Summary: 
[2024-01-22 10:20:08,470] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0397001139074564
[2024-01-22 10:20:11,155] INFO: Iter 114800 Summary: 
[2024-01-22 10:20:11,155] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.0398940247669816
[2024-01-22 10:20:13,845] INFO: Iter 114900 Summary: 
[2024-01-22 10:20:13,845] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03995447658002377
[2024-01-22 10:20:16,617] INFO: Iter 115000 Summary: 
[2024-01-22 10:20:16,617] INFO: 	 lr: 1.000000000000002e-05 	 Training loss: 0.03985172377899289
