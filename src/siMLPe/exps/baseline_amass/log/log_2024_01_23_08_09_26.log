[2024-01-23 08:10:05,936] INFO: {
    "abs_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass",
    "amass_anno_dir": "/home/aiRobots/src/siMLPe/data/amass/",
    "batch_size": 256,
    "cos_lr_max": 0.0003,
    "cos_lr_min": 5e-08,
    "cos_lr_total_iters": 115000,
    "data_aug": true,
    "deriv_input": true,
    "deriv_output": true,
    "link_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/log_last.log",
    "link_val_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/val_last.log",
    "log_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log",
    "log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/log_2024_01_23_08_09_26.log",
    "model_pth": null,
    "motion": {
        "amass_input_length": 50,
        "amass_input_length_dct": 50,
        "amass_target_length": 25,
        "amass_target_length_eval": 25,
        "amass_target_length_train": 25,
        "dim": 54,
        "pw3d_input_length": 50,
        "pw3d_target_length_eval": 25,
        "pw3d_target_length_train": 25
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": false,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": true,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 54,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 50,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "num_workers": 8,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "pw3d_anno_dir": "/home/aiRobots/src/siMLPe/data/3dpw/sequenceFiles/",
    "repo_name": "siMLPe",
    "root_dir": "/home/aiRobots/src/siMLPe",
    "save_every": 5000,
    "seed": 888,
    "shift_step": 5,
    "snapshot_dir": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/snapshot",
    "this_dir": "baseline_amass",
    "use_relative_loss": true,
    "val_log_file": "/home/aiRobots/src/siMLPe/exps/baseline_amass/log/val_2024_01_23_08_09_26.log",
    "weight_decay": 0.0001
}
[2024-01-23 08:10:09,025] INFO: Iter 100 Summary: 
[2024-01-23 08:10:09,025] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.09866055086255074
[2024-01-23 08:10:11,523] INFO: Iter 200 Summary: 
[2024-01-23 08:10:11,523] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07970086477696896
[2024-01-23 08:10:14,143] INFO: Iter 300 Summary: 
[2024-01-23 08:10:14,143] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.07207509778439998
[2024-01-23 08:10:16,559] INFO: Iter 400 Summary: 
[2024-01-23 08:10:16,559] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06888341203331948
[2024-01-23 08:10:19,201] INFO: Iter 500 Summary: 
[2024-01-23 08:10:19,201] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06719265434890985
[2024-01-23 08:10:21,912] INFO: Iter 600 Summary: 
[2024-01-23 08:10:21,912] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06574191194027662
[2024-01-23 08:10:24,521] INFO: Iter 700 Summary: 
[2024-01-23 08:10:24,521] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06505372971296311
[2024-01-23 08:10:27,341] INFO: Iter 800 Summary: 
[2024-01-23 08:10:27,342] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06387996215373277
[2024-01-23 08:10:29,912] INFO: Iter 900 Summary: 
[2024-01-23 08:10:29,912] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0637226140126586
[2024-01-23 08:10:32,709] INFO: Iter 1000 Summary: 
[2024-01-23 08:10:32,709] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06281336572021246
[2024-01-23 08:10:35,387] INFO: Iter 1100 Summary: 
[2024-01-23 08:10:35,387] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.062083161622285846
[2024-01-23 08:10:38,103] INFO: Iter 1200 Summary: 
[2024-01-23 08:10:38,103] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0616617776080966
[2024-01-23 08:10:40,859] INFO: Iter 1300 Summary: 
[2024-01-23 08:10:40,859] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06138411946594715
[2024-01-23 08:10:43,481] INFO: Iter 1400 Summary: 
[2024-01-23 08:10:43,481] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06102396670728922
[2024-01-23 08:10:46,179] INFO: Iter 1500 Summary: 
[2024-01-23 08:10:46,179] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.06052622187882662
[2024-01-23 08:10:48,929] INFO: Iter 1600 Summary: 
[2024-01-23 08:10:48,929] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.060147434398531915
[2024-01-23 08:10:51,631] INFO: Iter 1700 Summary: 
[2024-01-23 08:10:51,631] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05966938678175211
[2024-01-23 08:10:54,432] INFO: Iter 1800 Summary: 
[2024-01-23 08:10:54,432] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05860090240836144
[2024-01-23 08:10:57,139] INFO: Iter 1900 Summary: 
[2024-01-23 08:10:57,139] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05886689271777868
[2024-01-23 08:10:59,797] INFO: Iter 2000 Summary: 
[2024-01-23 08:10:59,797] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.058561157025396826
[2024-01-23 08:11:02,196] INFO: Iter 2100 Summary: 
[2024-01-23 08:11:02,196] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.058301390409469606
[2024-01-23 08:11:04,905] INFO: Iter 2200 Summary: 
[2024-01-23 08:11:04,905] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05770072065293789
[2024-01-23 08:11:07,627] INFO: Iter 2300 Summary: 
[2024-01-23 08:11:07,627] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05735681269317865
[2024-01-23 08:11:10,269] INFO: Iter 2400 Summary: 
[2024-01-23 08:11:10,269] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.057302918545901776
[2024-01-23 08:11:12,864] INFO: Iter 2500 Summary: 
[2024-01-23 08:11:12,864] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05703567836433649
[2024-01-23 08:11:15,428] INFO: Iter 2600 Summary: 
[2024-01-23 08:11:15,428] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05690142896026373
[2024-01-23 08:11:17,885] INFO: Iter 2700 Summary: 
[2024-01-23 08:11:17,885] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.056595995798707006
[2024-01-23 08:11:20,618] INFO: Iter 2800 Summary: 
[2024-01-23 08:11:20,618] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.056499514020979406
[2024-01-23 08:11:23,417] INFO: Iter 2900 Summary: 
[2024-01-23 08:11:23,417] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05616317145526409
[2024-01-23 08:11:26,104] INFO: Iter 3000 Summary: 
[2024-01-23 08:11:26,104] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055999592058360574
[2024-01-23 08:11:28,665] INFO: Iter 3100 Summary: 
[2024-01-23 08:11:28,665] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055527230724692345
[2024-01-23 08:11:31,359] INFO: Iter 3200 Summary: 
[2024-01-23 08:11:31,360] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.055334230996668336
[2024-01-23 08:11:34,119] INFO: Iter 3300 Summary: 
[2024-01-23 08:11:34,119] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05524085976183415
[2024-01-23 08:11:36,804] INFO: Iter 3400 Summary: 
[2024-01-23 08:11:36,804] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05536742754280567
[2024-01-23 08:11:39,603] INFO: Iter 3500 Summary: 
[2024-01-23 08:11:39,603] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05535300519317388
[2024-01-23 08:11:42,365] INFO: Iter 3600 Summary: 
[2024-01-23 08:11:42,365] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05545871268957853
[2024-01-23 08:11:45,062] INFO: Iter 3700 Summary: 
[2024-01-23 08:11:45,062] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05522134453058243
[2024-01-23 08:11:47,779] INFO: Iter 3800 Summary: 
[2024-01-23 08:11:47,780] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05495358947664499
[2024-01-23 08:11:50,622] INFO: Iter 3900 Summary: 
[2024-01-23 08:11:50,622] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054538472704589364
[2024-01-23 08:11:53,394] INFO: Iter 4000 Summary: 
[2024-01-23 08:11:53,394] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05458749920129776
[2024-01-23 08:11:56,053] INFO: Iter 4100 Summary: 
[2024-01-23 08:11:56,053] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054525446929037574
[2024-01-23 08:11:58,817] INFO: Iter 4200 Summary: 
[2024-01-23 08:11:58,817] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054882142841815945
[2024-01-23 08:12:01,608] INFO: Iter 4300 Summary: 
[2024-01-23 08:12:01,608] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05440907523036003
[2024-01-23 08:12:04,116] INFO: Iter 4400 Summary: 
[2024-01-23 08:12:04,116] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.054322384409606454
[2024-01-23 08:12:06,731] INFO: Iter 4500 Summary: 
[2024-01-23 08:12:06,731] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05415802184492349
[2024-01-23 08:12:09,461] INFO: Iter 4600 Summary: 
[2024-01-23 08:12:09,461] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05454121269285679
[2024-01-23 08:12:12,178] INFO: Iter 4700 Summary: 
[2024-01-23 08:12:12,178] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05315664891153574
[2024-01-23 08:12:14,811] INFO: Iter 4800 Summary: 
[2024-01-23 08:12:14,811] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05389671057462692
[2024-01-23 08:12:17,651] INFO: Iter 4900 Summary: 
[2024-01-23 08:12:17,651] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05370066542178392
[2024-01-23 08:12:20,407] INFO: Iter 5000 Summary: 
[2024-01-23 08:12:20,407] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053745503947138784
[2024-01-23 08:12:23,063] INFO: Iter 5100 Summary: 
[2024-01-23 08:12:23,063] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05384746540337801
[2024-01-23 08:12:25,726] INFO: Iter 5200 Summary: 
[2024-01-23 08:12:25,726] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05329228848218918
[2024-01-23 08:12:28,532] INFO: Iter 5300 Summary: 
[2024-01-23 08:12:28,532] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05300307709723711
[2024-01-23 08:12:31,105] INFO: Iter 5400 Summary: 
[2024-01-23 08:12:31,105] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05336924072355032
[2024-01-23 08:12:33,909] INFO: Iter 5500 Summary: 
[2024-01-23 08:12:33,910] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.053297074735164644
[2024-01-23 08:12:36,604] INFO: Iter 5600 Summary: 
[2024-01-23 08:12:36,604] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05319382280111313
[2024-01-23 08:12:39,157] INFO: Iter 5700 Summary: 
[2024-01-23 08:12:39,157] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05272808615118265
[2024-01-23 08:12:41,802] INFO: Iter 5800 Summary: 
[2024-01-23 08:12:41,802] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052385757565498355
[2024-01-23 08:12:44,663] INFO: Iter 5900 Summary: 
[2024-01-23 08:12:44,663] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052420998811721804
[2024-01-23 08:12:47,455] INFO: Iter 6000 Summary: 
[2024-01-23 08:12:47,456] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.052416791953146455
[2024-01-23 08:12:50,225] INFO: Iter 6100 Summary: 
[2024-01-23 08:12:50,225] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05249941978603601
[2024-01-23 08:12:52,922] INFO: Iter 6200 Summary: 
[2024-01-23 08:12:52,922] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05203603569418192
[2024-01-23 08:12:55,716] INFO: Iter 6300 Summary: 
[2024-01-23 08:12:55,717] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05256746016442776
[2024-01-23 08:12:58,507] INFO: Iter 6400 Summary: 
[2024-01-23 08:12:58,508] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05181027941405773
[2024-01-23 08:13:01,213] INFO: Iter 6500 Summary: 
[2024-01-23 08:13:01,213] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05222161553800106
[2024-01-23 08:13:03,870] INFO: Iter 6600 Summary: 
[2024-01-23 08:13:03,870] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051937056481838224
[2024-01-23 08:13:06,581] INFO: Iter 6700 Summary: 
[2024-01-23 08:13:06,581] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05214205823838711
[2024-01-23 08:13:09,269] INFO: Iter 6800 Summary: 
[2024-01-23 08:13:09,269] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05179669953882694
[2024-01-23 08:13:12,007] INFO: Iter 6900 Summary: 
[2024-01-23 08:13:12,007] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05189048957079649
[2024-01-23 08:13:14,635] INFO: Iter 7000 Summary: 
[2024-01-23 08:13:14,635] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05168100520968437
[2024-01-23 08:13:17,128] INFO: Iter 7100 Summary: 
[2024-01-23 08:13:17,128] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051688286364078524
[2024-01-23 08:13:19,876] INFO: Iter 7200 Summary: 
[2024-01-23 08:13:19,876] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0516482587531209
[2024-01-23 08:13:22,564] INFO: Iter 7300 Summary: 
[2024-01-23 08:13:22,564] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051294325962662694
[2024-01-23 08:13:25,231] INFO: Iter 7400 Summary: 
[2024-01-23 08:13:25,231] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051271435990929606
[2024-01-23 08:13:27,743] INFO: Iter 7500 Summary: 
[2024-01-23 08:13:27,743] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05149308450520038
[2024-01-23 08:13:30,449] INFO: Iter 7600 Summary: 
[2024-01-23 08:13:30,449] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.051288662366569045
[2024-01-23 08:13:33,027] INFO: Iter 7700 Summary: 
[2024-01-23 08:13:33,027] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05132615584880114
[2024-01-23 08:13:35,752] INFO: Iter 7800 Summary: 
[2024-01-23 08:13:35,752] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05127264615148306
[2024-01-23 08:13:38,412] INFO: Iter 7900 Summary: 
[2024-01-23 08:13:38,413] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05089722163975239
[2024-01-23 08:13:41,173] INFO: Iter 8000 Summary: 
[2024-01-23 08:13:41,173] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.0510490308329463
[2024-01-23 08:13:43,968] INFO: Iter 8100 Summary: 
[2024-01-23 08:13:43,969] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05074500098824501
[2024-01-23 08:13:46,682] INFO: Iter 8200 Summary: 
[2024-01-23 08:13:46,683] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.050944002158939836
[2024-01-23 08:13:49,303] INFO: Iter 8300 Summary: 
[2024-01-23 08:13:49,303] INFO: 	 lr: 0.00030000000000000073 	 Training loss: 0.05073196563869715
